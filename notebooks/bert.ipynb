{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n",
      "c:\\Users\\bruno\\.conda\\envs\\my_env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of filenames to load\n",
    "filenames = [\"../data/labeled_data/generic_test_0.json\"]\n",
    "\n",
    "# Load all JSON data and concatenate into one DataFrame\n",
    "dfs = []\n",
    "for filename in filenames:\n",
    "    with open(filename) as f:\n",
    "        data = json.load(f)\n",
    "    df_train = pd.DataFrame(data[\"train\"])\n",
    "    df_validation = pd.DataFrame(data[\"valid\"])\n",
    "    df_test = pd.DataFrame(data[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>campaign_name</th>\n",
       "      <th>text</th>\n",
       "      <th>annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>891103871484870657</td>\n",
       "      <td>VENEZUELA_201901_2</td>\n",
       "      <td>How life may find a way on Saturn's moon https...</td>\n",
       "      <td>[Science/Technology]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>898650367067664384</td>\n",
       "      <td>VENEZUELA_201901_2</td>\n",
       "      <td>RightWing Millennial Retweet RT RT_America to ...</td>\n",
       "      <td>[Others]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1012051694136512512</td>\n",
       "      <td>IRA_202012</td>\n",
       "      <td>The tobacco industry then peddles their produc...</td>\n",
       "      <td>[Health, Justice/Crime, Macroeconomics/Economi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1296389394883063810</td>\n",
       "      <td>UGANDA_0621</td>\n",
       "      <td>RT @brianmixologist: Today, i want to salute a...</td>\n",
       "      <td>[Others]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1286189347973279746</td>\n",
       "      <td>UGANDA_0621</td>\n",
       "      <td>RT @ArthurMirama: You can only under estimate ...</td>\n",
       "      <td>[Government/Public]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>1090997040258187266</td>\n",
       "      <td>REA_0621</td>\n",
       "      <td>SABC will not carry out a staff reduction\\nhtt...</td>\n",
       "      <td>[Labor/Employment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3196</th>\n",
       "      <td>818131935117443074</td>\n",
       "      <td>VENEZUELA_201901_2</td>\n",
       "      <td>Wikileaks To Hold Major Press Conf To Address ...</td>\n",
       "      <td>[Others]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>827160940944646145</td>\n",
       "      <td>VENEZUELA_201901_2</td>\n",
       "      <td>Death of veteran DRC opposition leader jeopard...</td>\n",
       "      <td>[Government/Public]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>896689467863511040</td>\n",
       "      <td>VENEZUELA_201901_2</td>\n",
       "      <td>#lost RT realDonaldTrump: Condolences to the f...</td>\n",
       "      <td>[Government/Public, Justice/Crime]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>1054823373006692352</td>\n",
       "      <td>GRU_202012</td>\n",
       "      <td>According to a source in the mukhabarate, the ...</td>\n",
       "      <td>[Government/Public, War/Terror]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id       campaign_name  \\\n",
       "0      891103871484870657  VENEZUELA_201901_2   \n",
       "1      898650367067664384  VENEZUELA_201901_2   \n",
       "2     1012051694136512512          IRA_202012   \n",
       "3     1296389394883063810         UGANDA_0621   \n",
       "4     1286189347973279746         UGANDA_0621   \n",
       "...                   ...                 ...   \n",
       "3195  1090997040258187266            REA_0621   \n",
       "3196   818131935117443074  VENEZUELA_201901_2   \n",
       "3197   827160940944646145  VENEZUELA_201901_2   \n",
       "3198   896689467863511040  VENEZUELA_201901_2   \n",
       "3199  1054823373006692352          GRU_202012   \n",
       "\n",
       "                                                   text  \\\n",
       "0     How life may find a way on Saturn's moon https...   \n",
       "1     RightWing Millennial Retweet RT RT_America to ...   \n",
       "2     The tobacco industry then peddles their produc...   \n",
       "3     RT @brianmixologist: Today, i want to salute a...   \n",
       "4     RT @ArthurMirama: You can only under estimate ...   \n",
       "...                                                 ...   \n",
       "3195  SABC will not carry out a staff reduction\\nhtt...   \n",
       "3196  Wikileaks To Hold Major Press Conf To Address ...   \n",
       "3197  Death of veteran DRC opposition leader jeopard...   \n",
       "3198  #lost RT realDonaldTrump: Condolences to the f...   \n",
       "3199  According to a source in the mukhabarate, the ...   \n",
       "\n",
       "                                            annotations  \n",
       "0                                  [Science/Technology]  \n",
       "1                                              [Others]  \n",
       "2     [Health, Justice/Crime, Macroeconomics/Economi...  \n",
       "3                                              [Others]  \n",
       "4                                   [Government/Public]  \n",
       "...                                                 ...  \n",
       "3195                                 [Labor/Employment]  \n",
       "3196                                           [Others]  \n",
       "3197                                [Government/Public]  \n",
       "3198                 [Government/Public, Justice/Crime]  \n",
       "3199                    [Government/Public, War/Terror]  \n",
       "\n",
       "[3200 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>campaign_name</th>\n",
       "      <th>text</th>\n",
       "      <th>annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>962251540617670661</td>\n",
       "      <td>GRU_202012</td>\n",
       "      <td>RT @Kasman62: After his injury for fifth time ...</td>\n",
       "      <td>[War/Terror]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>898277432922263552</td>\n",
       "      <td>VENEZUELA_201901_2</td>\n",
       "      <td>RT BGEEZ: dncchuckschumernancypelosimsnbccnn h...</td>\n",
       "      <td>[Others]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>966214616601841664</td>\n",
       "      <td>GRU_202012</td>\n",
       "      <td>RT @LinaArabii: Russian presence in the Black ...</td>\n",
       "      <td>[Media/Journalism]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1303856635668987905</td>\n",
       "      <td>UGANDA_0621</td>\n",
       "      <td>RT @brianmixologist: Am disappointed in the an...</td>\n",
       "      <td>[Election Campaign]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>462250037972840449</td>\n",
       "      <td>IRA_202012</td>\n",
       "      <td>RT @CMCL1979: Why do those those ghastly nativ...</td>\n",
       "      <td>[Others]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>847754640972070912</td>\n",
       "      <td>VENEZUELA_201901_2</td>\n",
       "      <td>Stocks: 5 things to know before the bell https...</td>\n",
       "      <td>[Others]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>1038043813267296256</td>\n",
       "      <td>GRU_202012</td>\n",
       "      <td>September 4, 2018 #Syrian air defense units re...</td>\n",
       "      <td>[War/Terror]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>1030464264748728320</td>\n",
       "      <td>UGANDA_0621</td>\n",
       "      <td>RT @xJ57jjSHWvX9mAMmhv7fVaVzxe13bBfCZuGZaBNucL...</td>\n",
       "      <td>[Others]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>1020843977669586945</td>\n",
       "      <td>UGANDA_0621</td>\n",
       "      <td>RT @HowweEnt: Dj Shiru To Thrill His Fans http...</td>\n",
       "      <td>[Others]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>883376409502199808</td>\n",
       "      <td>VENEZUELA_201901_2</td>\n",
       "      <td>BREAKING VIDEO : 70 Injured in Chaotic G20 Rio...</td>\n",
       "      <td>[Government/Public, Justice/Crime]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id       campaign_name  \\\n",
       "0     962251540617670661          GRU_202012   \n",
       "1     898277432922263552  VENEZUELA_201901_2   \n",
       "2     966214616601841664          GRU_202012   \n",
       "3    1303856635668987905         UGANDA_0621   \n",
       "4     462250037972840449          IRA_202012   \n",
       "..                   ...                 ...   \n",
       "795   847754640972070912  VENEZUELA_201901_2   \n",
       "796  1038043813267296256          GRU_202012   \n",
       "797  1030464264748728320         UGANDA_0621   \n",
       "798  1020843977669586945         UGANDA_0621   \n",
       "799   883376409502199808  VENEZUELA_201901_2   \n",
       "\n",
       "                                                  text  \\\n",
       "0    RT @Kasman62: After his injury for fifth time ...   \n",
       "1    RT BGEEZ: dncchuckschumernancypelosimsnbccnn h...   \n",
       "2    RT @LinaArabii: Russian presence in the Black ...   \n",
       "3    RT @brianmixologist: Am disappointed in the an...   \n",
       "4    RT @CMCL1979: Why do those those ghastly nativ...   \n",
       "..                                                 ...   \n",
       "795  Stocks: 5 things to know before the bell https...   \n",
       "796  September 4, 2018 #Syrian air defense units re...   \n",
       "797  RT @xJ57jjSHWvX9mAMmhv7fVaVzxe13bBfCZuGZaBNucL...   \n",
       "798  RT @HowweEnt: Dj Shiru To Thrill His Fans http...   \n",
       "799  BREAKING VIDEO : 70 Injured in Chaotic G20 Rio...   \n",
       "\n",
       "                            annotations  \n",
       "0                          [War/Terror]  \n",
       "1                              [Others]  \n",
       "2                    [Media/Journalism]  \n",
       "3                   [Election Campaign]  \n",
       "4                              [Others]  \n",
       "..                                  ...  \n",
       "795                            [Others]  \n",
       "796                        [War/Terror]  \n",
       "797                            [Others]  \n",
       "798                            [Others]  \n",
       "799  [Government/Public, Justice/Crime]  \n",
       "\n",
       "[800 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>campaign_name</th>\n",
       "      <th>text</th>\n",
       "      <th>annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1144169368227635200</td>\n",
       "      <td>REA_0621</td>\n",
       "      <td>The Automobile Association said it is expectin...</td>\n",
       "      <td>[Macroeconomics/Economic Regulation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1187637103850610688</td>\n",
       "      <td>REA_0621</td>\n",
       "      <td>A severe flooding, triggered by heavy rains an...</td>\n",
       "      <td>[Environment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1062269743749566464</td>\n",
       "      <td>GRU_202012</td>\n",
       "      <td>Militants of the Hayat Tahrir ash-Sham group p...</td>\n",
       "      <td>[War/Terror]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1100425181946880000</td>\n",
       "      <td>REA_0621</td>\n",
       "      <td>PIC Amendment Bill for transparency adopted by...</td>\n",
       "      <td>[Government/Public]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1129575563029561344</td>\n",
       "      <td>UGANDA_0621</td>\n",
       "      <td>RT @CQvMyyB0YfvwrUrsaZ6KI7yqaJfSUDTrAI0joQhgMA...</td>\n",
       "      <td>[Government/Public]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1166999406190780417</td>\n",
       "      <td>REA_0621</td>\n",
       "      <td>Rwanda and Zimbabwe entering deal: A Memorandu...</td>\n",
       "      <td>[Macroeconomics/Economic Regulation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>887818127295361024</td>\n",
       "      <td>VENEZUELA_201901_2</td>\n",
       "      <td>Trump Supporters React to Unhinged Rosie THREA...</td>\n",
       "      <td>[Government/Public]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>495089817923756032</td>\n",
       "      <td>IRA_202012</td>\n",
       "      <td>The American Aggression Enablement Act and the...</td>\n",
       "      <td>[Others]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1154775632871477248</td>\n",
       "      <td>REA_0621</td>\n",
       "      <td>Body of a missing 4-year-old girl has been fou...</td>\n",
       "      <td>[Justice/Crime]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>823680762167771136</td>\n",
       "      <td>VENEZUELA_201901_2</td>\n",
       "      <td>VIDEO : #Women’sMarch Linda Sarsour Told Maddo...</td>\n",
       "      <td>[Conspiracy Theory, Immigration/Integration, J...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id       campaign_name  \\\n",
       "0    1144169368227635200            REA_0621   \n",
       "1    1187637103850610688            REA_0621   \n",
       "2    1062269743749566464          GRU_202012   \n",
       "3    1100425181946880000            REA_0621   \n",
       "4    1129575563029561344         UGANDA_0621   \n",
       "..                   ...                 ...   \n",
       "995  1166999406190780417            REA_0621   \n",
       "996   887818127295361024  VENEZUELA_201901_2   \n",
       "997   495089817923756032          IRA_202012   \n",
       "998  1154775632871477248            REA_0621   \n",
       "999   823680762167771136  VENEZUELA_201901_2   \n",
       "\n",
       "                                                  text  \\\n",
       "0    The Automobile Association said it is expectin...   \n",
       "1    A severe flooding, triggered by heavy rains an...   \n",
       "2    Militants of the Hayat Tahrir ash-Sham group p...   \n",
       "3    PIC Amendment Bill for transparency adopted by...   \n",
       "4    RT @CQvMyyB0YfvwrUrsaZ6KI7yqaJfSUDTrAI0joQhgMA...   \n",
       "..                                                 ...   \n",
       "995  Rwanda and Zimbabwe entering deal: A Memorandu...   \n",
       "996  Trump Supporters React to Unhinged Rosie THREA...   \n",
       "997  The American Aggression Enablement Act and the...   \n",
       "998  Body of a missing 4-year-old girl has been fou...   \n",
       "999  VIDEO : #Women’sMarch Linda Sarsour Told Maddo...   \n",
       "\n",
       "                                           annotations  \n",
       "0                 [Macroeconomics/Economic Regulation]  \n",
       "1                                        [Environment]  \n",
       "2                                         [War/Terror]  \n",
       "3                                  [Government/Public]  \n",
       "4                                  [Government/Public]  \n",
       "..                                                 ...  \n",
       "995               [Macroeconomics/Economic Regulation]  \n",
       "996                                [Government/Public]  \n",
       "997                                           [Others]  \n",
       "998                                    [Justice/Crime]  \n",
       "999  [Conspiracy Theory, Immigration/Integration, J...  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/labeled_data/generic_fold_0.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(k):\n\u001b[0;32m     12\u001b[0m     \u001b[39m# Load the data for this fold\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     filename \u001b[39m=\u001b[39m filenames[i]\n\u001b[1;32m---> 14\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(filename) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     15\u001b[0m         data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n\u001b[0;32m     16\u001b[0m     train_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(data[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\bruno\\.conda\\envs\\my_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/labeled_data/generic_fold_0.json'"
     ]
    }
   ],
   "source": [
    "tasks=['generic', 'GRU_202012', 'IRA_202012', 'REA_0621', 'UGANDA_0621', 'VENEZUELA_201901']\n",
    "\n",
    "# Train and evaluate your model using k-fold cross-validation\n",
    "k = 5\n",
    "results = []\n",
    "for i in range(k):\n",
    "    # Load the data for this fold\n",
    "    filename = f\"../data/labeled_data/generic_test_{i}.json\"\n",
    "    with open(filename) as f:\n",
    "        data = json.load(f)\n",
    "    train_df = pd.DataFrame(data[\"train\"])\n",
    "    val_df = pd.DataFrame(data[\"validation\"])\n",
    "    test_df = pd.DataFrame(data[\"test\"])\n",
    "\n",
    "    # Train and evaluate your model on this fold\n",
    "    # ...\n",
    "\n",
    "    # Store the results for this fold\n",
    "    # ...\n",
    "    results.append(fold_result)\n",
    "\n",
    "# Average the results across all k folds\n",
    "avg_result = np.mean(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, df, y):\n",
    "        self.input_ids = df['input_ids'].tolist()\n",
    "        self.attention_mask = df['attention_mask']\n",
    "        self.labels = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attention_mask[idx], self.labels[idx]\n",
    "    \n",
    "class TweetClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(TweetClassifier, self).__init__()\n",
    "        self.bertweet = AutoModel.from_pretrained(\"vinai/bertweet-large\")\n",
    "        self.linear = nn.Linear(self.bertweet.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.bertweet(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = output.pooler_output\n",
    "        logits = self.linear(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3200"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-large were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Convert annotations column to a list of labels\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train = mlb.fit_transform(df_train['annotations'])\n",
    "y_validation = mlb.fit_transform(df_validation['annotations'])\n",
    "y_test = mlb.fit_transform(df_test['annotations'])\n",
    "\n",
    "print(y)\n",
    "\n",
    "# Load BERTweet model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-large\", normalization=True)\n",
    "model = AutoModel.from_pretrained(\"vinai/bertweet-large\")\n",
    "\n",
    "# Preprocess the text column by encoding the tweets and adding special tokens\n",
    "def preprocess_text(text):\n",
    "    return tokenizer.encode_plus(text, max_length=128, add_special_tokens=True, truncation=True, padding='max_length', return_attention_mask=True, return_tensors='pt')\n",
    "\n",
    "encoded_tweets = df_train['text'].apply(preprocess_text)\n",
    "input_ids = encoded_tweets.apply(lambda x: x['input_ids']).to_list()\n",
    "attention_mask = encoded_tweets.apply(lambda x: x['attention_mask']).to_list()\n",
    "df_encoded_tweets_train = pd.DataFrame({'input_ids': input_ids, 'attention_mask': attention_mask})\n",
    "\n",
    "encoded_tweets = df_validation['text'].apply(preprocess_text)\n",
    "input_ids = encoded_tweets.apply(lambda x: x['input_ids']).to_list()\n",
    "attention_mask = encoded_tweets.apply(lambda x: x['attention_mask']).to_list()\n",
    "df_encoded_tweets_validation = pd.DataFrame({'input_ids': input_ids, 'attention_mask': attention_mask})\n",
    "\n",
    "encoded_tweets = df_validation['text'].apply(preprocess_text)\n",
    "input_ids = encoded_tweets.apply(lambda x: x['input_ids']).to_list()\n",
    "attention_mask = encoded_tweets.apply(lambda x: x['attention_mask']).to_list()\n",
    "df_encoded_tweets_test = pd.DataFrame({'input_ids': input_ids, 'attention_mask': attention_mask})\n",
    "\n",
    "train_dataset = TweetDataset(df_encoded_tweets_train.reset_index(), torch.tensor(y_train))\n",
    "validation_dataset = TweetDataset(df_encoded_tweets_validation.reset_index(), torch.tensor(y_validation))\n",
    "test_dataset = TweetDataset(df_encoded_tweets_test.reset_index(), torch.tensor(y_test))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bertweet-large-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader) # len(daatset) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-large were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\bruno\\.conda\\envs\\my_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_31328\\3797979436.py:75: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics = metrics.append({'epoch': epoch+1, 'train_loss': train_loss, 'test_loss': test_loss, 'precision_macro': precision_macro, 'precision_micro': precision_micro, 'recall_macro': recall_macro, 'recall_micro': recall_micro, 'f1_macro': f1_macro, 'f1_micro': f1_micro, 'accuracy': accuracy, 'time': time_str}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Step 2, Train Loss: 0.6515279710292816, Test Loss: 0.6006966147571802, Precision Macro: 0.06019807314462351, Precision Micro: 0.15666666666666668, Recall Macro: 0.2516214888973418, Recall Micro: 0.4297142857142857, F1 Macro: 0.08104107512248815, F1 Micro: 0.22961832061068702, Accuracy: 0.0109375, Time: 00:04\n",
      "Best Current Model Found, Saving Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bruno\\.conda\\envs\\my_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_31328\\3797979436.py:75: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics = metrics.append({'epoch': epoch+1, 'train_loss': train_loss, 'test_loss': test_loss, 'precision_macro': precision_macro, 'precision_micro': precision_micro, 'recall_macro': recall_macro, 'recall_micro': recall_micro, 'f1_macro': f1_macro, 'f1_micro': f1_micro, 'accuracy': accuracy, 'time': time_str}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Step 4, Train Loss: 0.46678126603364944, Test Loss: 0.5553391557186842, Precision Macro: 0.056026310045576885, Precision Micro: 0.18873239436619718, Recall Macro: 0.09681316518661137, Recall Micro: 0.2297142857142857, F1 Macro: 0.04881403535759021, F1 Micro: 0.2072164948453608, Accuracy: 0.165625, Time: 00:10\n",
      "Best Current Model Found, Saving Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bruno\\.conda\\envs\\my_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_31328\\3797979436.py:75: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics = metrics.append({'epoch': epoch+1, 'train_loss': train_loss, 'test_loss': test_loss, 'precision_macro': precision_macro, 'precision_micro': precision_micro, 'recall_macro': recall_macro, 'recall_micro': recall_micro, 'f1_macro': f1_macro, 'f1_micro': f1_micro, 'accuracy': accuracy, 'time': time_str}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Step 6, Train Loss: 0.260478721310695, Test Loss: 0.49160642083734274, Precision Macro: 0.05243694412596378, Precision Micro: 0.20550458715596331, Recall Macro: 0.06049075240294495, Recall Micro: 0.128, F1 Macro: 0.04854325782030892, F1 Micro: 0.15774647887323945, Accuracy: 0.0546875, Time: 00:15\n",
      "Best Current Model Found, Saving Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bruno\\.conda\\envs\\my_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_31328\\3797979436.py:75: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics = metrics.append({'epoch': epoch+1, 'train_loss': train_loss, 'test_loss': test_loss, 'precision_macro': precision_macro, 'precision_micro': precision_micro, 'recall_macro': recall_macro, 'recall_micro': recall_micro, 'f1_macro': f1_macro, 'f1_micro': f1_micro, 'accuracy': accuracy, 'time': time_str}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Step 8, Train Loss: 0.17050797321523228, Test Loss: 0.5011031497269869, Precision Macro: 0.06022927054540866, Precision Micro: 0.227994227994228, Recall Macro: 0.07341413237208, Recall Micro: 0.18057142857142858, F1 Macro: 0.055515621137384855, F1 Micro: 0.201530612244898, Accuracy: 0.0578125, Time: 00:20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bruno\\.conda\\envs\\my_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_31328\\3797979436.py:75: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics = metrics.append({'epoch': epoch+1, 'train_loss': train_loss, 'test_loss': test_loss, 'precision_macro': precision_macro, 'precision_micro': precision_micro, 'recall_macro': recall_macro, 'recall_micro': recall_micro, 'f1_macro': f1_macro, 'f1_micro': f1_micro, 'accuracy': accuracy, 'time': time_str}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Step 10, Train Loss: 0.1260110324403892, Test Loss: 0.4967038072645664, Precision Macro: 0.06168120499776289, Precision Micro: 0.2446236559139785, Recall Macro: 0.07490325439545223, Recall Micro: 0.208, F1 Macro: 0.05615962130526504, F1 Micro: 0.22483014206300186, Accuracy: 0.0671875, Time: 00:25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bruno\\.conda\\envs\\my_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_31328\\3797979436.py:75: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics = metrics.append({'epoch': epoch+1, 'train_loss': train_loss, 'test_loss': test_loss, 'precision_macro': precision_macro, 'precision_micro': precision_micro, 'recall_macro': recall_macro, 'recall_micro': recall_micro, 'f1_macro': f1_macro, 'f1_micro': f1_micro, 'accuracy': accuracy, 'time': time_str}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Step 12, Train Loss: 0.10445753887114632, Test Loss: 0.4642455283552408, Precision Macro: 0.05622788885137875, Precision Micro: 0.19506172839506172, Recall Macro: 0.04005899945069477, Recall Micro: 0.09028571428571429, F1 Macro: 0.039957725639399706, F1 Micro: 0.1234375, Accuracy: 0.0109375, Time: 00:30\n",
      "Best Current Model Found, Saving Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bruno\\.conda\\envs\\my_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_31328\\3797979436.py:75: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics = metrics.append({'epoch': epoch+1, 'train_loss': train_loss, 'test_loss': test_loss, 'precision_macro': precision_macro, 'precision_micro': precision_micro, 'recall_macro': recall_macro, 'recall_micro': recall_micro, 'f1_macro': f1_macro, 'f1_micro': f1_micro, 'accuracy': accuracy, 'time': time_str}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Step 14, Train Loss: 0.0836641434791668, Test Loss: 0.4166960960254073, Precision Macro: 0.0538527397260274, Precision Micro: 0.22939068100358423, Recall Macro: 0.023799308433003222, Recall Micro: 0.07314285714285715, F1 Macro: 0.032988793183291655, F1 Micro: 0.11091854419410747, Accuracy: 0.0046875, Time: 00:35\n",
      "Best Current Model Found, Saving Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bruno\\.conda\\envs\\my_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_31328\\3797979436.py:75: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics = metrics.append({'epoch': epoch+1, 'train_loss': train_loss, 'test_loss': test_loss, 'precision_macro': precision_macro, 'precision_micro': precision_micro, 'recall_macro': recall_macro, 'recall_micro': recall_micro, 'f1_macro': f1_macro, 'f1_micro': f1_micro, 'accuracy': accuracy, 'time': time_str}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Step 16, Train Loss: 0.05600154551343982, Test Loss: 0.3998026795685291, Precision Macro: 0.06048847609219126, Precision Micro: 0.3010752688172043, Recall Macro: 0.020680400050936947, Recall Micro: 0.064, F1 Macro: 0.030544830544830544, F1 Micro: 0.1055607917059378, Accuracy: 0.009375, Time: 00:40\n",
      "Best Current Model Found, Saving Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bruno\\.conda\\envs\\my_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_31328\\3797979436.py:75: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics = metrics.append({'epoch': epoch+1, 'train_loss': train_loss, 'test_loss': test_loss, 'precision_macro': precision_macro, 'precision_micro': precision_micro, 'recall_macro': recall_macro, 'recall_micro': recall_micro, 'f1_macro': f1_macro, 'f1_micro': f1_micro, 'accuracy': accuracy, 'time': time_str}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Step 18, Train Loss: 0.05962629459107191, Test Loss: 0.3947414806112647, Precision Macro: 0.059893617021276595, Precision Micro: 0.32, Recall Macro: 0.01756149166887067, Recall Micro: 0.054857142857142854, F1 Macro: 0.026617925367581643, F1 Micro: 0.09365853658536584, Accuracy: 0.0109375, Time: 00:45\n",
      "Best Current Model Found, Saving Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bruno\\.conda\\envs\\my_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_31328\\3797979436.py:75: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics = metrics.append({'epoch': epoch+1, 'train_loss': train_loss, 'test_loss': test_loss, 'precision_macro': precision_macro, 'precision_micro': precision_micro, 'recall_macro': recall_macro, 'recall_micro': recall_micro, 'f1_macro': f1_macro, 'f1_micro': f1_micro, 'accuracy': accuracy, 'time': time_str}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Step 20, Train Loss: 0.045753603350025554, Test Loss: 0.37810385040938854, Precision Macro: 0.05607008760951189, Precision Micro: 0.3088235294117647, Recall Macro: 0.015222310382320962, Recall Micro: 0.048, F1 Macro: 0.023214850578539913, F1 Micro: 0.08308605341246292, Accuracy: 0.0109375, Time: 00:51\n",
      "Best Current Model Found, Saving Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bruno\\.conda\\envs\\my_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_31328\\3797979436.py:75: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics = metrics.append({'epoch': epoch+1, 'train_loss': train_loss, 'test_loss': test_loss, 'precision_macro': precision_macro, 'precision_micro': precision_micro, 'recall_macro': recall_macro, 'recall_micro': recall_micro, 'f1_macro': f1_macro, 'f1_micro': f1_micro, 'accuracy': accuracy, 'time': time_str}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Step 22, Train Loss: 0.03684980377875052, Test Loss: 0.3664341842755675, Precision Macro: 0.05933806146572104, Precision Micro: 0.2966101694915254, Recall Macro: 0.01249326554801297, Recall Micro: 0.04, F1 Macro: 0.019278536215396283, F1 Micro: 0.07049345417925479, Accuracy: 0.0109375, Time: 00:56\n",
      "Best Current Model Found, Saving Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bruno\\.conda\\envs\\my_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_31328\\3797979436.py:75: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics = metrics.append({'epoch': epoch+1, 'train_loss': train_loss, 'test_loss': test_loss, 'precision_macro': precision_macro, 'precision_micro': precision_micro, 'recall_macro': recall_macro, 'recall_micro': recall_micro, 'f1_macro': f1_macro, 'f1_micro': f1_micro, 'accuracy': accuracy, 'time': time_str}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Step 24, Train Loss: 0.03485536977041193, Test Loss: 0.360365048982203, Precision Macro: 0.05933806146572104, Precision Micro: 0.27927927927927926, Recall Macro: 0.010933811356979831, Recall Micro: 0.03542857142857143, F1 Macro: 0.016724772774536066, F1 Micro: 0.06288032454361055, Accuracy: 0.0125, Time: 01:01\n",
      "Best Current Model Found, Saving Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bruno\\.conda\\envs\\my_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_31328\\3797979436.py:75: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics = metrics.append({'epoch': epoch+1, 'train_loss': train_loss, 'test_loss': test_loss, 'precision_macro': precision_macro, 'precision_micro': precision_micro, 'recall_macro': recall_macro, 'recall_micro': recall_micro, 'f1_macro': f1_macro, 'f1_micro': f1_micro, 'accuracy': accuracy, 'time': time_str}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Step 26, Train Loss: 0.025951006246067172, Test Loss: 0.35531490873545407, Precision Macro: 0.05933806146572104, Precision Micro: 0.24752475247524752, Recall Macro: 0.008594630070430124, Recall Micro: 0.02857142857142857, F1 Macro: 0.012569496667373685, F1 Micro: 0.051229508196721306, Accuracy: 0.0125, Time: 01:06\n",
      "Best Current Model Found, Saving Model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m attention_mask \u001b[39m=\u001b[39m batch[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m     51\u001b[0m labels \u001b[39m=\u001b[39m batch[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 52\u001b[0m outputs \u001b[39m=\u001b[39m model(input_ids, attention_mask)\n\u001b[0;32m     53\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels\u001b[39m.\u001b[39mfloat())\n\u001b[0;32m     54\u001b[0m test_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\bruno\\.conda\\envs\\my_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[6], line 20\u001b[0m, in \u001b[0;36mTweetClassifier.forward\u001b[1;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_ids, attention_mask):\n\u001b[1;32m---> 20\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbertweet(input_ids\u001b[39m=\u001b[39;49minput_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask)\n\u001b[0;32m     21\u001b[0m     pooled_output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mpooler_output\n\u001b[0;32m     22\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(pooled_output)\n",
      "File \u001b[1;32mc:\\Users\\bruno\\.conda\\envs\\my_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\bruno\\.conda\\envs\\my_env\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:846\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    837\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    839\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m    840\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m    841\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    844\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    845\u001b[0m )\n\u001b[1;32m--> 846\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m    847\u001b[0m     embedding_output,\n\u001b[0;32m    848\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m    849\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    850\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    851\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m    852\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    853\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    854\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    855\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    856\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    857\u001b[0m )\n\u001b[0;32m    858\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    859\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bruno\\.conda\\envs\\my_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\bruno\\.conda\\envs\\my_env\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:520\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    511\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    512\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    513\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    517\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    518\u001b[0m     )\n\u001b[0;32m    519\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 520\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    521\u001b[0m         hidden_states,\n\u001b[0;32m    522\u001b[0m         attention_mask,\n\u001b[0;32m    523\u001b[0m         layer_head_mask,\n\u001b[0;32m    524\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    525\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    526\u001b[0m         past_key_value,\n\u001b[0;32m    527\u001b[0m         output_attentions,\n\u001b[0;32m    528\u001b[0m     )\n\u001b[0;32m    530\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    531\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\bruno\\.conda\\envs\\my_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\bruno\\.conda\\envs\\my_env\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:405\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    394\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    395\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    402\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    403\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    404\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 405\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    406\u001b[0m         hidden_states,\n\u001b[0;32m    407\u001b[0m         attention_mask,\n\u001b[0;32m    408\u001b[0m         head_mask,\n\u001b[0;32m    409\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    410\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    411\u001b[0m     )\n\u001b[0;32m    412\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    414\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bruno\\.conda\\envs\\my_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\bruno\\.conda\\envs\\my_env\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:341\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    323\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    324\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    330\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    332\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself(\n\u001b[0;32m    333\u001b[0m         hidden_states,\n\u001b[0;32m    334\u001b[0m         attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    339\u001b[0m         output_attentions,\n\u001b[0;32m    340\u001b[0m     )\n\u001b[1;32m--> 341\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(self_outputs[\u001b[39m0\u001b[39;49m], hidden_states)\n\u001b[0;32m    342\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n\u001b[0;32m    343\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\bruno\\.conda\\envs\\my_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\bruno\\.conda\\envs\\my_env\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:290\u001b[0m, in \u001b[0;36mRobertaSelfOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor, input_tensor: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m--> 290\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[0;32m    291\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    292\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(hidden_states \u001b[39m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32mc:\\Users\\bruno\\.conda\\envs\\my_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\bruno\\.conda\\envs\\my_env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the classifier model using binary cross-entropy loss and the AdamW optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TweetClassifier(num_labels=len(mlb.classes_)).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=0.001)\n",
    "\n",
    "# Initialize dataframes for logging the losses and metrics\n",
    "metrics = pd.DataFrame(columns=['epoch', 'train_loss', 'test_loss', 'precision_macro', 'precision_micro', 'recall_macro', 'recall_micro', 'f1_macro', 'f1_micro', 'accuracy', 'time'])\n",
    "\n",
    "epochs = 200\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "best_model = None\n",
    "\n",
    "#TODO: implement early stopping with patience = 1200 steps = 1.5 epochs\n",
    "patience = 1200\n",
    "early_stopping_counter = 0\n",
    "\n",
    "eval_steps = 400\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    global_step = 0\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[0].to(device).squeeze(1)\n",
    "        attention_mask = batch[1].to(device).squeeze(1)\n",
    "        labels = batch[2].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Casts operations to mixed precision\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels.float())\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        global_step += 1\n",
    "        \n",
    "        # Evaluate the model every eval_steps\n",
    "        if global_step % eval_steps == 0:\n",
    "            model.eval()\n",
    "            test_loss = 0\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in test_loader:\n",
    "                    input_ids = batch[0].to(device).squeeze(1)\n",
    "                    attention_mask = batch[1].to(device).squeeze(1)\n",
    "                    labels = batch[2].to(device)\n",
    "                    outputs = model(input_ids, attention_mask)\n",
    "                    loss = criterion(outputs, labels.float())\n",
    "                    test_loss += loss.item()\n",
    "\n",
    "                    batch_pred = torch.sigmoid(outputs).cpu().detach().numpy()\n",
    "                    labels = labels.cpu().detach().numpy()\n",
    "                    y_true.append(labels)\n",
    "                    y_pred.append(batch_pred)\n",
    "\n",
    "            test_loss /= len(test_loader)\n",
    "            \n",
    "            y_true = np.vstack(y_true)\n",
    "            y_pred = np.vstack(y_pred)\n",
    "            precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(y_true, y_pred > 0.5, average='macro')\n",
    "            precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(y_true, y_pred > 0.5, average='micro')\n",
    "            accuracy = accuracy_score(y_true, y_pred > 0.5)\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            minutes, seconds = divmod(elapsed_time, 60)\n",
    "            time_str = f\"{int(minutes):02d}:{int(seconds):02d}\"\n",
    "            \n",
    "            train_loss /= global_step\n",
    "\n",
    "            new_row = pd.DataFrame({'epoch': [epoch+1], 'train_loss': [train_loss], 'test_loss': [test_loss], 'precision_macro': [precision_macro], 'precision_micro': [precision_micro], 'recall_macro': [recall_macro], 'recall_micro': [recall_micro], 'f1_macro': [f1_macro], 'f1_micro': [f1_micro], 'accuracy': [accuracy], 'time': [time_str]})\n",
    "            metrics = pd.concat([metrics, new_row], ignore_index=True)  \n",
    "            print(f'Epoch {epoch+1}/{epochs}, Step {global_step}, Train Loss: {train_loss}, Test Loss: {test_loss}, Precision Macro: {precision_macro}, Precision Micro: {precision_micro}, Recall Macro: {recall_macro}, Recall Micro: {recall_micro}, F1 Macro: {f1_macro}, F1 Micro: {f1_micro}, Accuracy: {accuracy}, Time: {time_str}')\n",
    "\n",
    "            # Save the best model based on the evaluation loss\n",
    "            #TODO: change to f1-micro\n",
    "            if test_loss < best_loss:\n",
    "                print(\"Best Current Model Found, Saving Model...\")\n",
    "                best_loss = test_loss\n",
    "                best_model = model.state_dict()\n",
    "                early_stopping_counter = 0\n",
    "\n",
    "            model.train()\n",
    "            \n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load(\"../models/{model_name}.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>precision_micro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>recall_micro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.216469</td>\n",
       "      <td>0.152939</td>\n",
       "      <td>0.487925</td>\n",
       "      <td>0.840989</td>\n",
       "      <td>0.266169</td>\n",
       "      <td>0.544000</td>\n",
       "      <td>0.321336</td>\n",
       "      <td>0.660652</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.127269</td>\n",
       "      <td>0.121936</td>\n",
       "      <td>0.810334</td>\n",
       "      <td>0.849138</td>\n",
       "      <td>0.467086</td>\n",
       "      <td>0.675429</td>\n",
       "      <td>0.550905</td>\n",
       "      <td>0.752387</td>\n",
       "      <td>0.584375</td>\n",
       "      <td>02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.089811</td>\n",
       "      <td>0.109567</td>\n",
       "      <td>0.768305</td>\n",
       "      <td>0.860497</td>\n",
       "      <td>0.578644</td>\n",
       "      <td>0.712000</td>\n",
       "      <td>0.637684</td>\n",
       "      <td>0.779237</td>\n",
       "      <td>0.615625</td>\n",
       "      <td>02:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.065169</td>\n",
       "      <td>0.109837</td>\n",
       "      <td>0.779510</td>\n",
       "      <td>0.828283</td>\n",
       "      <td>0.620807</td>\n",
       "      <td>0.749714</td>\n",
       "      <td>0.674415</td>\n",
       "      <td>0.787043</td>\n",
       "      <td>0.631250</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.048520</td>\n",
       "      <td>0.106363</td>\n",
       "      <td>0.766207</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>0.635792</td>\n",
       "      <td>0.752000</td>\n",
       "      <td>0.675435</td>\n",
       "      <td>0.786611</td>\n",
       "      <td>0.631250</td>\n",
       "      <td>02:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.034025</td>\n",
       "      <td>0.106338</td>\n",
       "      <td>0.759219</td>\n",
       "      <td>0.811138</td>\n",
       "      <td>0.659796</td>\n",
       "      <td>0.765714</td>\n",
       "      <td>0.694594</td>\n",
       "      <td>0.787772</td>\n",
       "      <td>0.631250</td>\n",
       "      <td>02:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.026678</td>\n",
       "      <td>0.114554</td>\n",
       "      <td>0.769001</td>\n",
       "      <td>0.801190</td>\n",
       "      <td>0.640851</td>\n",
       "      <td>0.769143</td>\n",
       "      <td>0.674094</td>\n",
       "      <td>0.784840</td>\n",
       "      <td>0.620313</td>\n",
       "      <td>02:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.020222</td>\n",
       "      <td>0.125085</td>\n",
       "      <td>0.757733</td>\n",
       "      <td>0.811414</td>\n",
       "      <td>0.633249</td>\n",
       "      <td>0.747429</td>\n",
       "      <td>0.667265</td>\n",
       "      <td>0.778108</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>02:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.019148</td>\n",
       "      <td>0.122573</td>\n",
       "      <td>0.738763</td>\n",
       "      <td>0.799770</td>\n",
       "      <td>0.672417</td>\n",
       "      <td>0.794286</td>\n",
       "      <td>0.692426</td>\n",
       "      <td>0.797018</td>\n",
       "      <td>0.639062</td>\n",
       "      <td>02:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.019440</td>\n",
       "      <td>0.122291</td>\n",
       "      <td>0.750067</td>\n",
       "      <td>0.803738</td>\n",
       "      <td>0.676510</td>\n",
       "      <td>0.786286</td>\n",
       "      <td>0.699300</td>\n",
       "      <td>0.794916</td>\n",
       "      <td>0.639062</td>\n",
       "      <td>02:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  epoch  train_loss  test_loss  precision_macro  precision_micro  \\\n",
       "0     1    0.216469   0.152939         0.487925         0.840989   \n",
       "1     2    0.127269   0.121936         0.810334         0.849138   \n",
       "2     3    0.089811   0.109567         0.768305         0.860497   \n",
       "3     4    0.065169   0.109837         0.779510         0.828283   \n",
       "4     5    0.048520   0.106363         0.766207         0.824561   \n",
       "5     6    0.034025   0.106338         0.759219         0.811138   \n",
       "6     7    0.026678   0.114554         0.769001         0.801190   \n",
       "7     8    0.020222   0.125085         0.757733         0.811414   \n",
       "8     9    0.019148   0.122573         0.738763         0.799770   \n",
       "9    10    0.019440   0.122291         0.750067         0.803738   \n",
       "\n",
       "   recall_macro  recall_micro  f1_macro  f1_micro  accuracy   time  \n",
       "0      0.266169      0.544000  0.321336  0.660652  0.475000  02:12  \n",
       "1      0.467086      0.675429  0.550905  0.752387  0.584375  02:10  \n",
       "2      0.578644      0.712000  0.637684  0.779237  0.615625  02:09  \n",
       "3      0.620807      0.749714  0.674415  0.787043  0.631250  02:12  \n",
       "4      0.635792      0.752000  0.675435  0.786611  0.631250  02:09  \n",
       "5      0.659796      0.765714  0.694594  0.787772  0.631250  02:07  \n",
       "6      0.640851      0.769143  0.674094  0.784840  0.620313  02:07  \n",
       "7      0.633249      0.747429  0.667265  0.778108  0.625000  02:09  \n",
       "8      0.672417      0.794286  0.692426  0.797018  0.639062  02:09  \n",
       "9      0.676510      0.786286  0.699300  0.794916  0.639062  02:08  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"../models\"\n",
    "model_name = \"bertweet_large_mlb.pt\"\n",
    "\n",
    "import os\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "torch.save(model.state_dict(), os.path.join(folder_path, model_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
