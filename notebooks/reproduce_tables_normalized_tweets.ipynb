{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bruno\\.conda\\envs\\my_env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       loss  accuracy  micro_precision  micro_recall  micro_f1  \\\n",
      "0  0.121153       0.0         0.819905      0.800926  0.810304   \n",
      "1  0.138046       0.0         0.787618      0.772483  0.779977   \n",
      "2  0.141095       0.0         0.798335      0.784545  0.791380   \n",
      "3  0.140656       0.0         0.793049      0.771714  0.782236   \n",
      "4  0.109987       0.0         0.808470      0.786517  0.797342   \n",
      "5  0.116768       0.0         0.796516      0.773251  0.784711   \n",
      "6  0.125396       0.0         0.819923      0.794063  0.806786   \n",
      "7  0.132581       0.0         0.795075      0.769408  0.782031   \n",
      "8  0.111217       0.0         0.816387      0.773620  0.794428   \n",
      "9  0.118092       0.0         0.790660      0.754804  0.772316   \n",
      "\n",
      "   macro_precision  macro_recall  macro_f1  runtime  samples_per_second  \\\n",
      "0         0.785532      0.732025  0.750269    6.767             118.221   \n",
      "1         0.706886      0.657421  0.674854    8.425             118.694   \n",
      "2         0.782169      0.704428  0.732035    7.043             113.588   \n",
      "3         0.714138      0.672311  0.687991    8.301             120.467   \n",
      "4         0.778960      0.636299  0.681311    6.625             120.755   \n",
      "5         0.760337      0.651862  0.685468    8.277             120.817   \n",
      "6         0.797461      0.705027  0.738831    6.962             114.910   \n",
      "7         0.716366      0.663523  0.685407    8.477             117.966   \n",
      "8         0.741366      0.679100  0.693930    7.301             109.574   \n",
      "9         0.750471      0.657315  0.688811    9.509             105.164   \n",
      "\n",
      "   steps_per_second  num_epochs dataset  fold  \n",
      "0            29.555         7.5   valid     1  \n",
      "1            29.674         7.5    test     1  \n",
      "2            28.397         8.0   valid     2  \n",
      "3            30.117         8.0    test     2  \n",
      "4            30.189         5.0   valid     3  \n",
      "5            30.204         5.0    test     3  \n",
      "6            28.727         7.5   valid     4  \n",
      "7            29.492         7.5    test     4  \n",
      "8            27.394         5.0   valid     5  \n",
      "9            26.291         5.0    test     5  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import confusion_matrix, classification_report, multilabel_confusion_matrix\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from typing import List, Dict\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the dataset class\n",
    "\n",
    "# Load data from json file\n",
    "with open('../reports/generic_epochs_200_train_size_full.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "dfs = []\n",
    "for k, v in data.items():\n",
    "    valid_metrics = v['valid']\n",
    "    valid_metrics['dataset'] = 'valid'\n",
    "    valid_metrics['fold'] = int(k) + 1\n",
    "    dfs.append(pd.DataFrame([valid_metrics]))\n",
    "    \n",
    "    test_metrics = v['test']\n",
    "    test_metrics['dataset'] = 'test'\n",
    "    test_metrics['fold'] = int(k) + 1\n",
    "    dfs.append(pd.DataFrame([test_metrics]))\n",
    "\n",
    "# Concatenate all dataframes together\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Rename columns\n",
    "df.columns = df.columns.str.replace('eval_', '')\n",
    "df = df.rename(columns={'epoch': 'num_epochs'})\n",
    "\n",
    "# Print the final dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>micro_precision</th>\n",
       "      <th>micro_recall</th>\n",
       "      <th>micro_f1</th>\n",
       "      <th>macro_precision</th>\n",
       "      <th>macro_recall</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>runtime</th>\n",
       "      <th>samples_per_second</th>\n",
       "      <th>steps_per_second</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>dataset</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.121153</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.819905</td>\n",
       "      <td>0.800926</td>\n",
       "      <td>0.810304</td>\n",
       "      <td>0.785532</td>\n",
       "      <td>0.732025</td>\n",
       "      <td>0.750269</td>\n",
       "      <td>6.767</td>\n",
       "      <td>118.221</td>\n",
       "      <td>29.555</td>\n",
       "      <td>7.5</td>\n",
       "      <td>valid</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.138046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.787618</td>\n",
       "      <td>0.772483</td>\n",
       "      <td>0.779977</td>\n",
       "      <td>0.706886</td>\n",
       "      <td>0.657421</td>\n",
       "      <td>0.674854</td>\n",
       "      <td>8.425</td>\n",
       "      <td>118.694</td>\n",
       "      <td>29.674</td>\n",
       "      <td>7.5</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.141095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.798335</td>\n",
       "      <td>0.784545</td>\n",
       "      <td>0.791380</td>\n",
       "      <td>0.782169</td>\n",
       "      <td>0.704428</td>\n",
       "      <td>0.732035</td>\n",
       "      <td>7.043</td>\n",
       "      <td>113.588</td>\n",
       "      <td>28.397</td>\n",
       "      <td>8.0</td>\n",
       "      <td>valid</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.140656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.793049</td>\n",
       "      <td>0.771714</td>\n",
       "      <td>0.782236</td>\n",
       "      <td>0.714138</td>\n",
       "      <td>0.672311</td>\n",
       "      <td>0.687991</td>\n",
       "      <td>8.301</td>\n",
       "      <td>120.467</td>\n",
       "      <td>30.117</td>\n",
       "      <td>8.0</td>\n",
       "      <td>test</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.109987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.808470</td>\n",
       "      <td>0.786517</td>\n",
       "      <td>0.797342</td>\n",
       "      <td>0.778960</td>\n",
       "      <td>0.636299</td>\n",
       "      <td>0.681311</td>\n",
       "      <td>6.625</td>\n",
       "      <td>120.755</td>\n",
       "      <td>30.189</td>\n",
       "      <td>5.0</td>\n",
       "      <td>valid</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.116768</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.796516</td>\n",
       "      <td>0.773251</td>\n",
       "      <td>0.784711</td>\n",
       "      <td>0.760337</td>\n",
       "      <td>0.651862</td>\n",
       "      <td>0.685468</td>\n",
       "      <td>8.277</td>\n",
       "      <td>120.817</td>\n",
       "      <td>30.204</td>\n",
       "      <td>5.0</td>\n",
       "      <td>test</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.125396</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.819923</td>\n",
       "      <td>0.794063</td>\n",
       "      <td>0.806786</td>\n",
       "      <td>0.797461</td>\n",
       "      <td>0.705027</td>\n",
       "      <td>0.738831</td>\n",
       "      <td>6.962</td>\n",
       "      <td>114.910</td>\n",
       "      <td>28.727</td>\n",
       "      <td>7.5</td>\n",
       "      <td>valid</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.132581</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.795075</td>\n",
       "      <td>0.769408</td>\n",
       "      <td>0.782031</td>\n",
       "      <td>0.716366</td>\n",
       "      <td>0.663523</td>\n",
       "      <td>0.685407</td>\n",
       "      <td>8.477</td>\n",
       "      <td>117.966</td>\n",
       "      <td>29.492</td>\n",
       "      <td>7.5</td>\n",
       "      <td>test</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.111217</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.816387</td>\n",
       "      <td>0.773620</td>\n",
       "      <td>0.794428</td>\n",
       "      <td>0.741366</td>\n",
       "      <td>0.679100</td>\n",
       "      <td>0.693930</td>\n",
       "      <td>7.301</td>\n",
       "      <td>109.574</td>\n",
       "      <td>27.394</td>\n",
       "      <td>5.0</td>\n",
       "      <td>valid</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.118092</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.790660</td>\n",
       "      <td>0.754804</td>\n",
       "      <td>0.772316</td>\n",
       "      <td>0.750471</td>\n",
       "      <td>0.657315</td>\n",
       "      <td>0.688811</td>\n",
       "      <td>9.509</td>\n",
       "      <td>105.164</td>\n",
       "      <td>26.291</td>\n",
       "      <td>5.0</td>\n",
       "      <td>test</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy  micro_precision  micro_recall  micro_f1  \\\n",
       "0  0.121153       0.0         0.819905      0.800926  0.810304   \n",
       "1  0.138046       0.0         0.787618      0.772483  0.779977   \n",
       "2  0.141095       0.0         0.798335      0.784545  0.791380   \n",
       "3  0.140656       0.0         0.793049      0.771714  0.782236   \n",
       "4  0.109987       0.0         0.808470      0.786517  0.797342   \n",
       "5  0.116768       0.0         0.796516      0.773251  0.784711   \n",
       "6  0.125396       0.0         0.819923      0.794063  0.806786   \n",
       "7  0.132581       0.0         0.795075      0.769408  0.782031   \n",
       "8  0.111217       0.0         0.816387      0.773620  0.794428   \n",
       "9  0.118092       0.0         0.790660      0.754804  0.772316   \n",
       "\n",
       "   macro_precision  macro_recall  macro_f1  runtime  samples_per_second  \\\n",
       "0         0.785532      0.732025  0.750269    6.767             118.221   \n",
       "1         0.706886      0.657421  0.674854    8.425             118.694   \n",
       "2         0.782169      0.704428  0.732035    7.043             113.588   \n",
       "3         0.714138      0.672311  0.687991    8.301             120.467   \n",
       "4         0.778960      0.636299  0.681311    6.625             120.755   \n",
       "5         0.760337      0.651862  0.685468    8.277             120.817   \n",
       "6         0.797461      0.705027  0.738831    6.962             114.910   \n",
       "7         0.716366      0.663523  0.685407    8.477             117.966   \n",
       "8         0.741366      0.679100  0.693930    7.301             109.574   \n",
       "9         0.750471      0.657315  0.688811    9.509             105.164   \n",
       "\n",
       "   steps_per_second  num_epochs dataset  fold  \n",
       "0            29.555         7.5   valid     1  \n",
       "1            29.674         7.5    test     1  \n",
       "2            28.397         8.0   valid     2  \n",
       "3            30.117         8.0    test     2  \n",
       "4            30.189         5.0   valid     3  \n",
       "5            30.204         5.0    test     3  \n",
       "6            28.727         7.5   valid     4  \n",
       "7            29.492         7.5    test     4  \n",
       "8            27.394         5.0   valid     5  \n",
       "9            26.291         5.0    test     5  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/generic_epochs_200_train_size_full_fold_0\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/generic_epochs_200_train_size_full_fold_1\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/generic_epochs_200_train_size_full_fold_2\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/generic_epochs_200_train_size_full_fold_3\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/generic_epochs_200_train_size_full_fold_4\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Validation Classification Report In DataFrame Format:\n",
      "                                    precision    recall  f1-score  support\n",
      "Conspiracy Theory                    0.684166  0.460611  0.544436     50.8\n",
      "Education                            0.831414  0.601677  0.691619     12.6\n",
      "Election Campaign                    0.773443  0.827550  0.793036     26.6\n",
      "Environment                          0.784127  0.554221  0.638319     11.6\n",
      "Government/Public                    0.774086  0.818285  0.795384    249.6\n",
      "Health                               0.849340  0.764804  0.802604     42.8\n",
      "Immigration/Integration              0.794401  0.746866  0.760890     40.2\n",
      "Justice/Crime                        0.778646  0.815865  0.796420    114.4\n",
      "Labor/Employment                     0.813095  0.577675  0.649563     19.4\n",
      "Macroeconomics/Economic Regulation   0.760334  0.699159  0.726307     50.0\n",
      "Media/Journalism                     0.774926  0.606182  0.671627     36.8\n",
      "Others                               0.853551  0.779605  0.814337    211.4\n",
      "Religion                             0.727302  0.594563  0.625162     14.2\n",
      "Science/Technology                   0.633333  0.251212  0.352381     10.8\n",
      "War/Terror                           0.923960  0.912720  0.918036    187.8\n",
      "micro avg                            0.815419  0.774951  0.794610   1079.0\n",
      "macro avg                            0.783742  0.667400  0.705341   1079.0\n",
      "weighted avg                         0.816758  0.774951  0.789495   1079.0\n",
      "samples avg                          0.820546  0.800621  0.794991   1079.0\n",
      "\n",
      "Average Test Classification Report In DataFrame Format:\n",
      "                                    precision    recall  f1-score  support\n",
      "Conspiracy Theory                    0.537811  0.417778  0.457861     45.0\n",
      "Education                            0.493333  0.461538  0.474545     13.0\n",
      "Election Campaign                    0.797357  0.818182  0.805885     33.0\n",
      "Environment                          0.801984  0.485714  0.583712     14.0\n",
      "Government/Public                    0.742254  0.756014  0.748731    291.0\n",
      "Health                               0.739958  0.604348  0.665101     46.0\n",
      "Immigration/Integration              0.819355  0.711111  0.757157     36.0\n",
      "Justice/Crime                        0.807785  0.824818  0.815902    137.0\n",
      "Labor/Employment                     0.694124  0.550000  0.605355     28.0\n",
      "Macroeconomics/Economic Regulation   0.812322  0.667742  0.732051     62.0\n",
      "Media/Journalism                     0.811264  0.558333  0.655741     48.0\n",
      "Others                               0.829258  0.797048  0.811338    271.0\n",
      "Religion                             0.629524  0.563636  0.587175     11.0\n",
      "Science/Technology                   0.698889  0.327273  0.416777     11.0\n",
      "War/Terror                           0.932032  0.882353  0.905949    255.0\n",
      "micro avg                            0.800973  0.752806  0.776106   1301.0\n",
      "macro avg                            0.743150  0.628393  0.668219   1301.0\n",
      "weighted avg                         0.802538  0.752806  0.772326   1301.0\n",
      "samples avg                          0.806867  0.787100  0.782287   1301.0\n",
      "../models/GRU_202012_epochs_200_train_size_full_fold_0\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/GRU_202012_epochs_200_train_size_full_fold_1\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/GRU_202012_epochs_200_train_size_full_fold_2\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/GRU_202012_epochs_200_train_size_full_fold_3\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/GRU_202012_epochs_200_train_size_full_fold_4\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Validation Classification Report In DataFrame Format:\n",
      "                                    precision    recall  f1-score  support\n",
      "Conspiracy Theory                    0.581429  0.222214  0.304458     27.6\n",
      "Education                            0.754524  0.458741  0.558935     13.4\n",
      "Election Campaign                    0.831053  0.782267  0.802301     32.4\n",
      "Environment                          0.719987  0.581026  0.634218     13.4\n",
      "Government/Public                    0.793737  0.805297  0.797972    285.0\n",
      "Health                               0.818029  0.741223  0.773197     41.0\n",
      "Immigration/Integration              0.769521  0.695402  0.725647     30.4\n",
      "Justice/Crime                        0.801344  0.860016  0.828719    133.8\n",
      "Labor/Employment                     0.729171  0.559524  0.612891     22.0\n",
      "Macroeconomics/Economic Regulation   0.746056  0.736954  0.739461     58.4\n",
      "Media/Journalism                     0.808618  0.724841  0.761295     40.4\n",
      "Others                               0.864576  0.787744  0.823241    256.4\n",
      "Religion                             0.683292  0.501429  0.573736     14.8\n",
      "Science/Technology                   0.626190  0.300476  0.404329     12.0\n",
      "War/Terror                           0.771321  0.779587  0.770144     60.2\n",
      "micro avg                            0.800105  0.754477  0.776579   1041.2\n",
      "macro avg                            0.753257  0.635783  0.674036   1041.2\n",
      "weighted avg                         0.800357  0.754477  0.769313   1041.2\n",
      "samples avg                          0.801812  0.780171  0.776400   1041.2\n",
      "\n",
      "Average Test Classification Report In DataFrame Format:\n",
      "                                    precision    recall  f1-score  support\n",
      "Conspiracy Theory                    0.777778  0.023602  0.045136    161.0\n",
      "Education                            0.733333  0.422222  0.519048      9.0\n",
      "Election Campaign                    0.000000  0.000000  0.000000      4.0\n",
      "Environment                          0.000000  0.000000  0.000000      5.0\n",
      "Government/Public                    0.485481  0.535088  0.497223    114.0\n",
      "Health                               0.824664  0.458182  0.579591     55.0\n",
      "Immigration/Integration              0.857165  0.524706  0.642758     85.0\n",
      "Justice/Crime                        0.480564  0.460000  0.463299     40.0\n",
      "Labor/Employment                     0.600000  0.080000  0.138725     15.0\n",
      "Macroeconomics/Economic Regulation   0.731439  0.650000  0.676806     20.0\n",
      "Media/Journalism                     0.855152  0.400000  0.539601     30.0\n",
      "Others                               0.767471  0.721739  0.740502     46.0\n",
      "Religion                             0.793333  0.375000  0.504895      8.0\n",
      "Science/Technology                   0.666667  0.320000  0.378788      5.0\n",
      "War/Terror                           0.972764  0.911310  0.940664    893.0\n",
      "micro avg                            0.874308  0.694362  0.773844   1490.0\n",
      "macro avg                            0.636387  0.392123  0.444469   1490.0\n",
      "weighted avg                         0.864128  0.694362  0.728510   1490.0\n",
      "samples avg                          0.901433  0.785980  0.815453   1490.0\n",
      "../models/IRA_202012_epochs_200_train_size_full_fold_0\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/IRA_202012_epochs_200_train_size_full_fold_1\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/IRA_202012_epochs_200_train_size_full_fold_2\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/IRA_202012_epochs_200_train_size_full_fold_3\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/IRA_202012_epochs_200_train_size_full_fold_4\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Validation Classification Report In DataFrame Format:\n",
      "                                    precision    recall  f1-score  support\n",
      "Conspiracy Theory                    0.711209  0.588366  0.630602     53.0\n",
      "Education                            0.762143  0.399145  0.506403     12.6\n",
      "Election Campaign                    0.820857  0.759654  0.785141     24.6\n",
      "Environment                          0.695000  0.479929  0.550039     11.2\n",
      "Government/Public                    0.786692  0.816428  0.799408    219.8\n",
      "Health                               0.764709  0.722180  0.738587     41.2\n",
      "Immigration/Integration              0.805935  0.779931  0.792491     39.8\n",
      "Justice/Crime                        0.824889  0.838655  0.829200    126.8\n",
      "Labor/Employment                     0.697509  0.362805  0.468212     15.2\n",
      "Macroeconomics/Economic Regulation   0.723294  0.528434  0.609977     25.0\n",
      "Media/Journalism                     0.826389  0.662084  0.712248     26.2\n",
      "Others                               0.882451  0.823879  0.851401    229.6\n",
      "Religion                             0.732222  0.565128  0.622891     13.4\n",
      "Science/Technology                   0.660606  0.342222  0.429217     11.2\n",
      "War/Terror                           0.946116  0.961706  0.953798    195.4\n",
      "micro avg                            0.835245  0.794825  0.814075   1045.0\n",
      "macro avg                            0.776001  0.642036  0.685308   1045.0\n",
      "weighted avg                         0.835574  0.794825  0.808023   1045.0\n",
      "samples avg                          0.842229  0.822517  0.818045   1045.0\n",
      "\n",
      "Average Test Classification Report In DataFrame Format:\n",
      "                                    precision    recall  f1-score  support\n",
      "Conspiracy Theory                    0.181828  0.470588  0.255747     34.0\n",
      "Education                            0.879762  0.369231  0.492431     13.0\n",
      "Election Campaign                    0.742313  0.767442  0.753324     43.0\n",
      "Environment                          0.677143  0.562500  0.613272     16.0\n",
      "Government/Public                    0.718731  0.735909  0.724618    440.0\n",
      "Health                               0.914853  0.655556  0.759696     54.0\n",
      "Immigration/Integration              0.660634  0.615789  0.633859     38.0\n",
      "Justice/Crime                        0.523520  0.538667  0.516940     75.0\n",
      "Labor/Employment                     0.763484  0.632653  0.682739     49.0\n",
      "Macroeconomics/Economic Regulation   0.891831  0.414973  0.560327    187.0\n",
      "Media/Journalism                     0.699271  0.306931  0.415784    101.0\n",
      "Others                               0.668541  0.544444  0.597525    180.0\n",
      "Religion                             0.754286  0.373333  0.482694     15.0\n",
      "Science/Technology                   0.126753  0.155556  0.129378      9.0\n",
      "War/Terror                           0.694400  0.688479  0.690685    217.0\n",
      "micro avg                            0.666849  0.598097  0.629581   1471.0\n",
      "macro avg                            0.659823  0.522137  0.553935   1471.0\n",
      "weighted avg                         0.712897  0.598097  0.629560   1471.0\n",
      "samples avg                          0.658533  0.626053  0.617047   1471.0\n",
      "../models/REA_0621_epochs_200_train_size_full_fold_0\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/REA_0621_epochs_200_train_size_full_fold_1\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/REA_0621_epochs_200_train_size_full_fold_2\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/REA_0621_epochs_200_train_size_full_fold_3\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/REA_0621_epochs_200_train_size_full_fold_4\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Validation Classification Report In DataFrame Format:\n",
      "                                    precision    recall  f1-score  support\n",
      "Conspiracy Theory                    0.687411  0.480847  0.552613     58.4\n",
      "Education                            0.730303  0.429890  0.525641     10.4\n",
      "Election Campaign                    0.801481  0.758515  0.769324     25.6\n",
      "Environment                          0.883333  0.430000  0.509437      7.8\n",
      "Government/Public                    0.767561  0.843493  0.802921    264.6\n",
      "Health                               0.826398  0.776403  0.799018     38.0\n",
      "Immigration/Integration              0.799991  0.749074  0.765486     45.0\n",
      "Justice/Crime                        0.668921  0.650658  0.642362     61.2\n",
      "Labor/Employment                     0.763694  0.626554  0.664569     19.0\n",
      "Macroeconomics/Economic Regulation   0.809107  0.686624  0.739111     52.6\n",
      "Media/Journalism                     0.823833  0.716326  0.766201     43.2\n",
      "Others                               0.875636  0.788505  0.829051    220.2\n",
      "Religion                             0.658825  0.599094  0.604638     15.0\n",
      "Science/Technology                   0.640000  0.307692  0.388841     10.2\n",
      "War/Terror                           0.924661  0.932227  0.928212    232.2\n",
      "micro avg                            0.813226  0.781126  0.796831   1103.4\n",
      "macro avg                            0.777410  0.651727  0.685828   1103.4\n",
      "weighted avg                         0.819531  0.781126  0.791965   1103.4\n",
      "samples avg                          0.823938  0.806892  0.799619   1103.4\n",
      "\n",
      "Average Test Classification Report In DataFrame Format:\n",
      "                                    precision    recall  f1-score  support\n",
      "Conspiracy Theory                    0.000000  0.000000  0.000000      7.0\n",
      "Education                            0.532204  0.650000  0.582534     24.0\n",
      "Election Campaign                    0.848087  0.894737  0.869760     38.0\n",
      "Environment                          0.630927  0.545455  0.566884     33.0\n",
      "Government/Public                    0.606955  0.845370  0.704051    216.0\n",
      "Health                               0.701369  0.797143  0.745157     70.0\n",
      "Immigration/Integration              0.498224  0.433333  0.456720     12.0\n",
      "Justice/Crime                        0.877155  0.910670  0.892735    403.0\n",
      "Labor/Employment                     0.534487  0.380000  0.437733     30.0\n",
      "Macroeconomics/Economic Regulation   0.533540  0.702041  0.593377     49.0\n",
      "Media/Journalism                     0.319651  0.462500  0.373685     16.0\n",
      "Others                               0.834030  0.448458  0.579494    227.0\n",
      "Religion                             0.500000  0.514286  0.502262      7.0\n",
      "Science/Technology                   0.339468  0.471429  0.375531     14.0\n",
      "War/Terror                           0.553831  0.593939  0.570029     33.0\n",
      "micro avg                            0.712849  0.731976  0.722081   1179.0\n",
      "macro avg                            0.553995  0.576624  0.549997   1179.0\n",
      "weighted avg                         0.736758  0.731976  0.714626   1179.0\n",
      "samples avg                          0.713883  0.746733  0.713887   1179.0\n",
      "../models/UGANDA_0621_epochs_200_train_size_full_fold_0\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/UGANDA_0621_epochs_200_train_size_full_fold_1\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/UGANDA_0621_epochs_200_train_size_full_fold_2\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/UGANDA_0621_epochs_200_train_size_full_fold_3\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/UGANDA_0621_epochs_200_train_size_full_fold_4\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Validation Classification Report In DataFrame Format:\n",
      "                                    precision    recall  f1-score  support\n",
      "Conspiracy Theory                    0.705627  0.494221  0.569589     57.6\n",
      "Education                            0.702965  0.611826  0.633472     12.2\n",
      "Election Campaign                    0.847583  0.804204  0.823953     26.6\n",
      "Environment                          0.680635  0.471772  0.541874     13.2\n",
      "Government/Public                    0.795100  0.832414  0.812724    276.0\n",
      "Health                               0.827233  0.797318  0.810130     42.4\n",
      "Immigration/Integration              0.799117  0.781691  0.786809     46.6\n",
      "Justice/Crime                        0.807833  0.840449  0.823018    137.2\n",
      "Labor/Employment                     0.648528  0.638372  0.636435     21.4\n",
      "Macroeconomics/Economic Regulation   0.737292  0.733239  0.730586     58.6\n",
      "Media/Journalism                     0.818575  0.749999  0.781665     44.4\n",
      "Others                               0.805709  0.674731  0.730338    123.0\n",
      "Religion                             0.764530  0.508991  0.572683     12.6\n",
      "Science/Technology                   0.880000  0.300108  0.431197      9.0\n",
      "War/Terror                           0.926341  0.920486  0.922348    237.0\n",
      "micro avg                            0.816009  0.786608  0.800973   1117.8\n",
      "macro avg                            0.783138  0.677321  0.707121   1117.8\n",
      "weighted avg                         0.819584  0.786608  0.796303   1117.8\n",
      "samples avg                          0.825308  0.810846  0.801180   1117.8\n",
      "\n",
      "Average Test Classification Report In DataFrame Format:\n",
      "                                    precision    recall  f1-score  support\n",
      "Conspiracy Theory                    0.400000  0.036364  0.066667     11.0\n",
      "Education                            0.580036  0.533333  0.546727     15.0\n",
      "Election Campaign                    0.737389  0.442424  0.550699     33.0\n",
      "Environment                          0.485195  0.666667  0.555010      6.0\n",
      "Government/Public                    0.615455  0.553459  0.579793    159.0\n",
      "Health                               0.763587  0.725000  0.740891     48.0\n",
      "Immigration/Integration              0.396667  0.600000  0.468889      4.0\n",
      "Justice/Crime                        0.347773  0.808696  0.486030     23.0\n",
      "Labor/Employment                     0.644652  0.466667  0.527676     18.0\n",
      "Macroeconomics/Economic Regulation   0.466258  0.652632  0.543081     19.0\n",
      "Media/Journalism                     0.133333  0.120000  0.125784     10.0\n",
      "Others                               0.920726  0.854698  0.885601    713.0\n",
      "Religion                             0.443647  0.442105  0.385732     19.0\n",
      "Science/Technology                   0.387576  0.130000  0.187895     20.0\n",
      "War/Terror                           0.349677  0.866667  0.495603      9.0\n",
      "micro avg                            0.781327  0.741644  0.760924   1107.0\n",
      "macro avg                            0.511465  0.526581  0.476405   1107.0\n",
      "weighted avg                         0.796786  0.741644  0.758173   1107.0\n",
      "samples avg                          0.777233  0.767267  0.765133   1107.0\n",
      "../models/VENEZUELA_201901_2_epochs_200_train_size_full_fold_0\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/VENEZUELA_201901_2_epochs_200_train_size_full_fold_1\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/VENEZUELA_201901_2_epochs_200_train_size_full_fold_2\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/VENEZUELA_201901_2_epochs_200_train_size_full_fold_3\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/VENEZUELA_201901_2_epochs_200_train_size_full_fold_4\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_8276\\111891348.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Validation Classification Report In DataFrame Format:\n",
      "                                    precision    recall  f1-score  support\n",
      "Conspiracy Theory                    0.690004  0.470342  0.539328     42.6\n",
      "Education                            0.764827  0.506349  0.593203     12.2\n",
      "Election Campaign                    0.777995  0.760159  0.748479     23.6\n",
      "Environment                          0.745455  0.447143  0.517091     12.0\n",
      "Government/Public                    0.688327  0.753579  0.719102    185.8\n",
      "Health                               0.833720  0.745680  0.781349     45.4\n",
      "Immigration/Integration              0.815217  0.660411  0.728122     27.8\n",
      "Justice/Crime                        0.874638  0.814084  0.838640    108.2\n",
      "Labor/Employment                     0.769273  0.527962  0.602267     22.4\n",
      "Macroeconomics/Economic Regulation   0.773969  0.714081  0.736043     55.0\n",
      "Media/Journalism                     0.734397  0.594133  0.647782     31.4\n",
      "Others                               0.893678  0.780808  0.833258    233.2\n",
      "Religion                             0.684038  0.441299  0.482857      9.8\n",
      "Science/Technology                   0.357143  0.242308  0.278974      9.6\n",
      "War/Terror                           0.916683  0.938602  0.927135    230.4\n",
      "micro avg                            0.817397  0.766611  0.790677   1049.4\n",
      "macro avg                            0.754624  0.626463  0.664909   1049.4\n",
      "weighted avg                         0.821543  0.766611  0.785321   1049.4\n",
      "samples avg                          0.819504  0.798938  0.793889   1049.4\n",
      "\n",
      "Average Test Classification Report In DataFrame Format:\n",
      "                                    precision    recall  f1-score  support\n",
      "Conspiracy Theory                    0.264024  0.102326  0.145959     86.0\n",
      "Education                            0.440000  0.200000  0.270286     15.0\n",
      "Election Campaign                    0.738394  0.845833  0.770236     48.0\n",
      "Environment                          0.722222  0.333333  0.417239     12.0\n",
      "Government/Public                    0.898731  0.800328  0.845749    610.0\n",
      "Health                               0.801219  0.684848  0.696398     33.0\n",
      "Immigration/Integration              0.911034  0.512245  0.640324     98.0\n",
      "Justice/Crime                        0.738864  0.571429  0.624562    168.0\n",
      "Labor/Employment                     0.589177  0.569231  0.559126     13.0\n",
      "Macroeconomics/Economic Regulation   0.635394  0.751351  0.684491     37.0\n",
      "Media/Journalism                     0.712243  0.736000  0.717323     75.0\n",
      "Others                               0.686364  0.656790  0.668962    162.0\n",
      "Religion                             0.886074  0.515152  0.633420     33.0\n",
      "Science/Technology                   0.638333  0.329412  0.428842     17.0\n",
      "War/Terror                           0.590600  0.809524  0.679402     42.0\n",
      "micro avg                            0.786949  0.667219  0.721082   1449.0\n",
      "macro avg                            0.683512  0.561187  0.585488   1449.0\n",
      "weighted avg                         0.774443  0.667219  0.701619   1449.0\n",
      "samples avg                          0.764917  0.700567  0.709123   1449.0\n"
     ]
    }
   ],
   "source": [
    "def model_summary(model):\n",
    "    print(\"Model summary:\")\n",
    "    print(\"---------------------------\")\n",
    "    total_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        param_count = param.numel()\n",
    "        total_params += param_count\n",
    "    print(f\"Total parameters: {total_params}\")\n",
    "    \n",
    "\"\"\"def print_report(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, target_names=categories)\n",
    "    print(report)\n",
    "    sns.heatmap(cm, annot=True, xticklabels=categories, yticklabels=categories, fmt='g')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\"\"\"\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, x, y, mlb, tokenizer):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.mlb = mlb\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoded_tweets = self.preprocess_text(self.x)\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        return self.tokenizer(text, return_attention_mask=True, return_tensors='pt', padding=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.y[idx]\n",
    "        return {'input_ids': self.encoded_tweets['input_ids'][idx],\n",
    "                'attention_mask': self.encoded_tweets['attention_mask'][idx],\n",
    "                'label': torch.tensor(label, dtype=torch.float32)}\n",
    "        \n",
    "class MultiLabelDataCollator(DataCollatorWithPadding):\n",
    "    def __init__(self, tokenizer):\n",
    "        super().__init__(tokenizer)\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, torch.Tensor]]):\n",
    "        batch = super().__call__(features)\n",
    "        batch[\"labels\"] = torch.stack([feature[\"label\"] for feature in features])\n",
    "        return batch\n",
    "    \n",
    "def get_classification_report(data_loader, model, target_names, label_names):\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    for batch in data_loader:\n",
    "        batch_inputs = {'input_ids': batch['input_ids'].to(device),\n",
    "                        'attention_mask': batch['attention_mask'].to(device)}\n",
    "        with torch.no_grad():\n",
    "            logits = model(**batch_inputs).logits\n",
    "        sigmoid = torch.nn.Sigmoid()\n",
    "        probs = sigmoid(torch.Tensor(logits))\n",
    "        batch_predictions = (probs >= 0.5).detach().cpu().numpy().astype(int)\n",
    "        \n",
    "        predictions.append(batch_predictions)\n",
    "        labels.append(batch['labels'].detach().cpu().numpy().astype(int))\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    labels = np.concatenate(labels, axis = 0)\n",
    "\n",
    "    #cm = multilabel_confusion_matrix(labels, predictions)\n",
    "    dict_report = classification_report(labels, predictions, target_names=target_names, labels=label_names, zero_division=0, output_dict=True)\n",
    "    report = classification_report(labels, predictions, target_names=target_names, labels=label_names, zero_division=0)\n",
    "    return dict_report, report\n",
    "    \n",
    "def calculate_average_report(reports):\n",
    "    avg_report = {}\n",
    "    for report in reports:\n",
    "        for key, scores in report.items():\n",
    "            if key not in avg_report:\n",
    "                avg_report[key] = {}\n",
    "                for score_key, score_value in scores.items():\n",
    "                    avg_report[key][score_key] = score_value\n",
    "            else:\n",
    "                for score_key, score_value in scores.items():\n",
    "                    avg_report[key][score_key] += score_value\n",
    "\n",
    "    num_reports = len(reports)\n",
    "    for key, scores in avg_report.items():\n",
    "        for score_key in scores:\n",
    "            avg_report[key][score_key] /= num_reports\n",
    "\n",
    "    return avg_report\n",
    "\n",
    "def average_report_to_dataframe(average_report):\n",
    "    data = {\n",
    "        \"precision\": [],\n",
    "        \"recall\": [],\n",
    "        \"f1-score\": [],\n",
    "        \"support\": []\n",
    "    }\n",
    "    index = []\n",
    "\n",
    "    for class_name, metrics in average_report.items():\n",
    "        if class_name == 'accuracy':\n",
    "            continue\n",
    "\n",
    "        index.append(class_name)\n",
    "        data[\"precision\"].append(metrics[\"precision\"])\n",
    "        data[\"recall\"].append(metrics[\"recall\"])\n",
    "        data[\"f1-score\"].append(metrics[\"f1-score\"])\n",
    "        data[\"support\"].append(metrics[\"support\"])\n",
    "\n",
    "    return pd.DataFrame(data, index=index)\n",
    "\n",
    "def calculate_metrics(task):\n",
    "    k = 5\n",
    "    \n",
    "    val_classification_reports = []\n",
    "    test_classification_reports = []\n",
    "\n",
    "    # Loop over each fold and load the corresponding model\n",
    "    for fold in range(k):\n",
    "        model_path = f\"../models/{task}_epochs_200_train_size_full_fold_{fold}\"\n",
    "        # find the latest checkpoint file\n",
    "        #checkpoint_files = [f for f in os.listdir(model_path) if f.startswith(\"checkpoint\")]\n",
    "        latest_checkpoint = os.path.join(model_path, \"\")  # use \"\" for models that were manually saved after training. use sorted(checkpoint_files)[0] for the first automatically saved checkpoint \n",
    "        print(latest_checkpoint)\n",
    "        \n",
    "        # Load the model and tokenizer\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(latest_checkpoint)\n",
    "        model.to(device)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-large\")\n",
    "\n",
    "        filename = f\"../data/labeled_data/{task}_test_{fold}.json\"\n",
    "        with open(filename) as f:\n",
    "            data = json.load(f)\n",
    "        train_df = pd.DataFrame(data[\"train\"])\n",
    "        val_df = pd.DataFrame(data[\"valid\"])\n",
    "        test_df = pd.DataFrame(data[\"test\"])\n",
    "        \n",
    "        train_annotations = train_df[\"annotations\"].tolist()\n",
    "        classes = set()\n",
    "        for annotation in train_annotations:\n",
    "            classes.update(annotation)\n",
    "        classes = sorted(list(classes))\n",
    "        \n",
    "        checkpoint = torch.load(os.path.join(model_path, \"pytorch_model.bin\"))\n",
    "        model.load_state_dict(checkpoint)\n",
    "        \n",
    "        mlb = MultiLabelBinarizer(classes=classes)\n",
    "        \n",
    "        train_labels = mlb.fit_transform(train_df[\"annotations\"])\n",
    "        val_labels = mlb.transform(val_df[\"annotations\"])\n",
    "        test_labels = mlb.transform(test_df[\"annotations\"])\n",
    "        \n",
    "        train_dataset = TweetDataset(train_df['text'].to_list(), torch.tensor(train_labels), mlb, tokenizer)\n",
    "        val_dataset = TweetDataset(val_df['text'].to_list(), torch.tensor(val_labels), mlb, tokenizer)\n",
    "        test_dataset = TweetDataset(test_df['text'].to_list(), torch.tensor(test_labels), mlb, tokenizer)\n",
    "        \n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            val_dataset, batch_size=4, shuffle=False, collate_fn=MultiLabelDataCollator(tokenizer)\n",
    "        )\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            test_dataset, batch_size=4, shuffle=False, collate_fn=MultiLabelDataCollator(tokenizer)\n",
    "        )\n",
    "        \n",
    "        model.eval()\n",
    "        val_report_dict, val_report = get_classification_report(val_loader, model, classes, range(len(classes)))\n",
    "        test_report_dict, test_report = get_classification_report(test_loader, model, classes, range(len(classes)))\n",
    "        val_classification_reports.append(val_report_dict)\n",
    "        test_classification_reports.append(test_report_dict)\n",
    "\n",
    "    val_average_report = calculate_average_report(val_classification_reports)\n",
    "    test_average_report = calculate_average_report(test_classification_reports)\n",
    "    val_average_report_df = average_report_to_dataframe(val_average_report)\n",
    "    test_average_report_df = average_report_to_dataframe(test_average_report)\n",
    "    print(\"\\nAverage Validation Classification Report In DataFrame Format:\")\n",
    "    print(val_average_report_df) \n",
    "    print(\"\\nAverage Test Classification Report In DataFrame Format:\")\n",
    "    print(test_average_report_df) \n",
    "    return val_average_report_df, test_average_report_df\n",
    "\n",
    "generic_val_average_report_df, generic_test_average_report_df = calculate_metrics(\"generic\")\n",
    "GRU_202012_val_average_report_df, GRU_202012_test_average_report_df = calculate_metrics(\"GRU_202012\")\n",
    "IRA_202012_val_average_report_df, IRA_202012_test_average_report_df = calculate_metrics(\"IRA_202012\")\n",
    "REA_0621_val_average_report_df, REA_0621_test_average_report_df = calculate_metrics(\"REA_0621\")\n",
    "UGANDA_0621_val_average_report_df, UGANDA_0621_test_average_report_df = calculate_metrics(\"UGANDA_0621\")\n",
    "VENEZUELA_201901_2_val_average_report_df, VENEZUELA_201901_2_test_average_report_df = calculate_metrics(\"VENEZUELA_201901_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataframes = {\n",
    "    \"generic_val_average_report\": generic_val_average_report_df,\n",
    "    \"generic_test_average_report\": generic_test_average_report_df,\n",
    "    \"GRU_202012_val_average_report\": GRU_202012_val_average_report_df,\n",
    "    \"GRU_202012_test_average_report\": GRU_202012_test_average_report_df,\n",
    "    \"IRA_202012_val_average_report\": IRA_202012_val_average_report_df,\n",
    "    \"IRA_202012_test_average_report\": IRA_202012_test_average_report_df,\n",
    "    \"REA_0621_val_average_report\": REA_0621_val_average_report_df,\n",
    "    \"REA_0621_test_average_report\": REA_0621_test_average_report_df,\n",
    "    \"UGANDA_0621_val_average_report\": UGANDA_0621_val_average_report_df,\n",
    "    \"UGANDA_0621_test_average_report\": UGANDA_0621_test_average_report_df,\n",
    "    \"VENEZUELA_201901_2_val_average_report\": VENEZUELA_201901_2_val_average_report_df,\n",
    "    \"VENEZUELA_201901_2_test_average_report\": VENEZUELA_201901_2_test_average_report_df,\n",
    "}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    csv_filename = f\"../reports/{name}_normalized_tweets.csv\"\n",
    "    df.to_csv(csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7053414707928204"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generic_val_average_report_df[generic_val_average_report_df.index == \"macro avg\"][\"f1-score\"].values[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macro Averages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Train Data  Test Data  Validation  Test\n",
      "0            generic    generic        0.71  0.67\n",
      "1        All but GRU        GRU        0.67  0.44\n",
      "2        All but IRA        IRA        0.69  0.55\n",
      "3        All but REA        REA        0.69  0.55\n",
      "4     All but UGANDA     UGANDA        0.71  0.48\n",
      "5  All but VENEZUELA  VENEZUELA        0.66  0.59\n"
     ]
    }
   ],
   "source": [
    "def extract_macro_avg_value(df):\n",
    "    return df[df.index == \"macro avg\"][\"f1-score\"].values[0]\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    micro_avg_value = round(extract_macro_avg_value(df), 2)\n",
    "    train_data = name.split(\"_\")[0]\n",
    "\n",
    "    if \"val\" in name:\n",
    "        validation_value = micro_avg_value\n",
    "        test_value = None\n",
    "    elif \"test\" in name:\n",
    "        validation_value = None\n",
    "        test_value = micro_avg_value\n",
    "\n",
    "    test_data = train_data\n",
    "    if train_data != \"generic\":\n",
    "        train_data = \"All but \" + train_data\n",
    "\n",
    "    summary_data.append({\n",
    "        \"Train Data\": train_data,\n",
    "        \"Test Data\": test_data,\n",
    "        \"Validation\": validation_value,\n",
    "        \"Test\": test_value,\n",
    "    })\n",
    "\n",
    "# Combine rows with the same \"Train Data\" and \"Test Data\" into one\n",
    "macro_summary_df = pd.DataFrame(summary_data)\n",
    "macro_summary_df = macro_summary_df.groupby([\"Train Data\", \"Test Data\"], as_index=False).first()\n",
    "\n",
    "# Reorder columns\n",
    "macro_summary_df = macro_summary_df[[\"Train Data\", \"Test Data\", \"Validation\", \"Test\"]]\n",
    "macro_summary_df = macro_summary_df.reindex([macro_summary_df.index[-1]] + list(macro_summary_df.index[:-1]))\n",
    "macro_summary_df = macro_summary_df.reset_index(drop=True)\n",
    "\n",
    "print(macro_summary_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Micro Averages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no values:  micro avg    0.79461\n",
      "Name: f1-score, dtype: float64\n",
      "wotjvalies :  0.7946103414885577\n",
      "no values:  micro avg    0.776106\n",
      "Name: f1-score, dtype: float64\n",
      "wotjvalies :  0.776105954235275\n",
      "no values:  micro avg    0.776579\n",
      "Name: f1-score, dtype: float64\n",
      "wotjvalies :  0.7765792870382628\n",
      "no values:  micro avg    0.773844\n",
      "Name: f1-score, dtype: float64\n",
      "wotjvalies :  0.7738437287750406\n",
      "no values:  micro avg    0.814075\n",
      "Name: f1-score, dtype: float64\n",
      "wotjvalies :  0.8140749682464591\n",
      "no values:  micro avg    0.629581\n",
      "Name: f1-score, dtype: float64\n",
      "wotjvalies :  0.6295805199879053\n",
      "no values:  micro avg    0.796831\n",
      "Name: f1-score, dtype: float64\n",
      "wotjvalies :  0.7968310520875482\n",
      "no values:  micro avg    0.722081\n",
      "Name: f1-score, dtype: float64\n",
      "wotjvalies :  0.7220808010471017\n",
      "no values:  micro avg    0.800973\n",
      "Name: f1-score, dtype: float64\n",
      "wotjvalies :  0.8009728728839356\n",
      "no values:  micro avg    0.760924\n",
      "Name: f1-score, dtype: float64\n",
      "wotjvalies :  0.7609242914494959\n",
      "no values:  micro avg    0.790677\n",
      "Name: f1-score, dtype: float64\n",
      "wotjvalies :  0.7906766223201708\n",
      "no values:  micro avg    0.721082\n",
      "Name: f1-score, dtype: float64\n",
      "wotjvalies :  0.721082471647124\n",
      "          Train Data  Test Data  Validation  Test\n",
      "0            generic    generic        0.79  0.78\n",
      "1        All but GRU        GRU        0.78  0.77\n",
      "2        All but IRA        IRA        0.81  0.63\n",
      "3        All but REA        REA        0.80  0.72\n",
      "4     All but UGANDA     UGANDA        0.80  0.76\n",
      "5  All but VENEZUELA  VENEZUELA        0.79  0.72\n"
     ]
    }
   ],
   "source": [
    "def extract_micro_avg_value(df):\n",
    "    print(\"no values: \", df[df.index == \"micro avg\"][\"f1-score\"])\n",
    "    print(\"wotjvalies : \", df[df.index == \"micro avg\"][\"f1-score\"].values[0])\n",
    "    return df[df.index == \"micro avg\"][\"f1-score\"].values[0]\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    micro_avg_value = round(extract_micro_avg_value(df), 2)\n",
    "    train_data = name.split(\"_\")[0]\n",
    "\n",
    "    if \"val\" in name:\n",
    "        validation_value = micro_avg_value\n",
    "        test_value = None\n",
    "    elif \"test\" in name:\n",
    "        validation_value = None\n",
    "        test_value = micro_avg_value\n",
    "\n",
    "    test_data = train_data\n",
    "    if train_data != \"generic\":\n",
    "        train_data = \"All but \" + train_data\n",
    "\n",
    "    summary_data.append({\n",
    "        \"Train Data\": train_data,\n",
    "        \"Test Data\": test_data,\n",
    "        \"Validation\": validation_value,\n",
    "        \"Test\": test_value,\n",
    "    })\n",
    "\n",
    "# Combine rows with the same \"Train Data\" and \"Test Data\" into one\n",
    "micro_summary_df = pd.DataFrame(summary_data)\n",
    "micro_summary_df = micro_summary_df.groupby([\"Train Data\", \"Test Data\"], as_index=False).first()\n",
    "\n",
    "# Reorder columns\n",
    "micro_summary_df = micro_summary_df[[\"Train Data\", \"Test Data\", \"Validation\", \"Test\"]]\n",
    "micro_summary_df = micro_summary_df.reindex([micro_summary_df.index[-1]] + list(micro_summary_df.index[:-1]))\n",
    "micro_summary_df = micro_summary_df.reset_index(drop=True)\n",
    "\n",
    "print(micro_summary_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
