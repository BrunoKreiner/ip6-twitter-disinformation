{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bruno\\.conda\\envs\\my_env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       loss  accuracy  micro_precision  micro_recall  micro_f1  \\\n",
      "0  0.121153       0.0         0.819905      0.800926  0.810304   \n",
      "1  0.138046       0.0         0.787618      0.772483  0.779977   \n",
      "2  0.141095       0.0         0.798335      0.784545  0.791380   \n",
      "3  0.140656       0.0         0.793049      0.771714  0.782236   \n",
      "4  0.109987       0.0         0.808470      0.786517  0.797342   \n",
      "5  0.116768       0.0         0.796516      0.773251  0.784711   \n",
      "6  0.125396       0.0         0.819923      0.794063  0.806786   \n",
      "7  0.132581       0.0         0.795075      0.769408  0.782031   \n",
      "8  0.111217       0.0         0.816387      0.773620  0.794428   \n",
      "9  0.118092       0.0         0.790660      0.754804  0.772316   \n",
      "\n",
      "   macro_precision  macro_recall  macro_f1  runtime  samples_per_second  \\\n",
      "0         0.785532      0.732025  0.750269    6.767             118.221   \n",
      "1         0.706886      0.657421  0.674854    8.425             118.694   \n",
      "2         0.782169      0.704428  0.732035    7.043             113.588   \n",
      "3         0.714138      0.672311  0.687991    8.301             120.467   \n",
      "4         0.778960      0.636299  0.681311    6.625             120.755   \n",
      "5         0.760337      0.651862  0.685468    8.277             120.817   \n",
      "6         0.797461      0.705027  0.738831    6.962             114.910   \n",
      "7         0.716366      0.663523  0.685407    8.477             117.966   \n",
      "8         0.741366      0.679100  0.693930    7.301             109.574   \n",
      "9         0.750471      0.657315  0.688811    9.509             105.164   \n",
      "\n",
      "   steps_per_second  num_epochs dataset  fold  \n",
      "0            29.555         7.5   valid     1  \n",
      "1            29.674         7.5    test     1  \n",
      "2            28.397         8.0   valid     2  \n",
      "3            30.117         8.0    test     2  \n",
      "4            30.189         5.0   valid     3  \n",
      "5            30.204         5.0    test     3  \n",
      "6            28.727         7.5   valid     4  \n",
      "7            29.492         7.5    test     4  \n",
      "8            27.394         5.0   valid     5  \n",
      "9            26.291         5.0    test     5  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import confusion_matrix, classification_report, multilabel_confusion_matrix\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from typing import List, Dict\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the dataset class\n",
    "\n",
    "# Load data from json file\n",
    "with open('../reports/generic_epochs_200_train_size_full.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "dfs = []\n",
    "for k, v in data.items():\n",
    "    valid_metrics = v['valid']\n",
    "    valid_metrics['dataset'] = 'valid'\n",
    "    valid_metrics['fold'] = int(k) + 1\n",
    "    dfs.append(pd.DataFrame([valid_metrics]))\n",
    "    \n",
    "    test_metrics = v['test']\n",
    "    test_metrics['dataset'] = 'test'\n",
    "    test_metrics['fold'] = int(k) + 1\n",
    "    dfs.append(pd.DataFrame([test_metrics]))\n",
    "\n",
    "# Concatenate all dataframes together\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Rename columns\n",
    "df.columns = df.columns.str.replace('eval_', '')\n",
    "df = df.rename(columns={'epoch': 'num_epochs'})\n",
    "\n",
    "# Print the final dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>micro_precision</th>\n",
       "      <th>micro_recall</th>\n",
       "      <th>micro_f1</th>\n",
       "      <th>macro_precision</th>\n",
       "      <th>macro_recall</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>runtime</th>\n",
       "      <th>samples_per_second</th>\n",
       "      <th>steps_per_second</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>dataset</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.121153</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.819905</td>\n",
       "      <td>0.800926</td>\n",
       "      <td>0.810304</td>\n",
       "      <td>0.785532</td>\n",
       "      <td>0.732025</td>\n",
       "      <td>0.750269</td>\n",
       "      <td>6.767</td>\n",
       "      <td>118.221</td>\n",
       "      <td>29.555</td>\n",
       "      <td>7.5</td>\n",
       "      <td>valid</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.138046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.787618</td>\n",
       "      <td>0.772483</td>\n",
       "      <td>0.779977</td>\n",
       "      <td>0.706886</td>\n",
       "      <td>0.657421</td>\n",
       "      <td>0.674854</td>\n",
       "      <td>8.425</td>\n",
       "      <td>118.694</td>\n",
       "      <td>29.674</td>\n",
       "      <td>7.5</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.141095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.798335</td>\n",
       "      <td>0.784545</td>\n",
       "      <td>0.791380</td>\n",
       "      <td>0.782169</td>\n",
       "      <td>0.704428</td>\n",
       "      <td>0.732035</td>\n",
       "      <td>7.043</td>\n",
       "      <td>113.588</td>\n",
       "      <td>28.397</td>\n",
       "      <td>8.0</td>\n",
       "      <td>valid</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.140656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.793049</td>\n",
       "      <td>0.771714</td>\n",
       "      <td>0.782236</td>\n",
       "      <td>0.714138</td>\n",
       "      <td>0.672311</td>\n",
       "      <td>0.687991</td>\n",
       "      <td>8.301</td>\n",
       "      <td>120.467</td>\n",
       "      <td>30.117</td>\n",
       "      <td>8.0</td>\n",
       "      <td>test</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.109987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.808470</td>\n",
       "      <td>0.786517</td>\n",
       "      <td>0.797342</td>\n",
       "      <td>0.778960</td>\n",
       "      <td>0.636299</td>\n",
       "      <td>0.681311</td>\n",
       "      <td>6.625</td>\n",
       "      <td>120.755</td>\n",
       "      <td>30.189</td>\n",
       "      <td>5.0</td>\n",
       "      <td>valid</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.116768</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.796516</td>\n",
       "      <td>0.773251</td>\n",
       "      <td>0.784711</td>\n",
       "      <td>0.760337</td>\n",
       "      <td>0.651862</td>\n",
       "      <td>0.685468</td>\n",
       "      <td>8.277</td>\n",
       "      <td>120.817</td>\n",
       "      <td>30.204</td>\n",
       "      <td>5.0</td>\n",
       "      <td>test</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.125396</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.819923</td>\n",
       "      <td>0.794063</td>\n",
       "      <td>0.806786</td>\n",
       "      <td>0.797461</td>\n",
       "      <td>0.705027</td>\n",
       "      <td>0.738831</td>\n",
       "      <td>6.962</td>\n",
       "      <td>114.910</td>\n",
       "      <td>28.727</td>\n",
       "      <td>7.5</td>\n",
       "      <td>valid</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.132581</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.795075</td>\n",
       "      <td>0.769408</td>\n",
       "      <td>0.782031</td>\n",
       "      <td>0.716366</td>\n",
       "      <td>0.663523</td>\n",
       "      <td>0.685407</td>\n",
       "      <td>8.477</td>\n",
       "      <td>117.966</td>\n",
       "      <td>29.492</td>\n",
       "      <td>7.5</td>\n",
       "      <td>test</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.111217</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.816387</td>\n",
       "      <td>0.773620</td>\n",
       "      <td>0.794428</td>\n",
       "      <td>0.741366</td>\n",
       "      <td>0.679100</td>\n",
       "      <td>0.693930</td>\n",
       "      <td>7.301</td>\n",
       "      <td>109.574</td>\n",
       "      <td>27.394</td>\n",
       "      <td>5.0</td>\n",
       "      <td>valid</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.118092</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.790660</td>\n",
       "      <td>0.754804</td>\n",
       "      <td>0.772316</td>\n",
       "      <td>0.750471</td>\n",
       "      <td>0.657315</td>\n",
       "      <td>0.688811</td>\n",
       "      <td>9.509</td>\n",
       "      <td>105.164</td>\n",
       "      <td>26.291</td>\n",
       "      <td>5.0</td>\n",
       "      <td>test</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy  micro_precision  micro_recall  micro_f1  \\\n",
       "0  0.121153       0.0         0.819905      0.800926  0.810304   \n",
       "1  0.138046       0.0         0.787618      0.772483  0.779977   \n",
       "2  0.141095       0.0         0.798335      0.784545  0.791380   \n",
       "3  0.140656       0.0         0.793049      0.771714  0.782236   \n",
       "4  0.109987       0.0         0.808470      0.786517  0.797342   \n",
       "5  0.116768       0.0         0.796516      0.773251  0.784711   \n",
       "6  0.125396       0.0         0.819923      0.794063  0.806786   \n",
       "7  0.132581       0.0         0.795075      0.769408  0.782031   \n",
       "8  0.111217       0.0         0.816387      0.773620  0.794428   \n",
       "9  0.118092       0.0         0.790660      0.754804  0.772316   \n",
       "\n",
       "   macro_precision  macro_recall  macro_f1  runtime  samples_per_second  \\\n",
       "0         0.785532      0.732025  0.750269    6.767             118.221   \n",
       "1         0.706886      0.657421  0.674854    8.425             118.694   \n",
       "2         0.782169      0.704428  0.732035    7.043             113.588   \n",
       "3         0.714138      0.672311  0.687991    8.301             120.467   \n",
       "4         0.778960      0.636299  0.681311    6.625             120.755   \n",
       "5         0.760337      0.651862  0.685468    8.277             120.817   \n",
       "6         0.797461      0.705027  0.738831    6.962             114.910   \n",
       "7         0.716366      0.663523  0.685407    8.477             117.966   \n",
       "8         0.741366      0.679100  0.693930    7.301             109.574   \n",
       "9         0.750471      0.657315  0.688811    9.509             105.164   \n",
       "\n",
       "   steps_per_second  num_epochs dataset  fold  \n",
       "0            29.555         7.5   valid     1  \n",
       "1            29.674         7.5    test     1  \n",
       "2            28.397         8.0   valid     2  \n",
       "3            30.117         8.0    test     2  \n",
       "4            30.189         5.0   valid     3  \n",
       "5            30.204         5.0    test     3  \n",
       "6            28.727         7.5   valid     4  \n",
       "7            29.492         7.5    test     4  \n",
       "8            27.394         5.0   valid     5  \n",
       "9            26.291         5.0    test     5  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTotal parameters: \u001b[39m\u001b[39m{\u001b[39;00mtotal_params\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[39m\"\"\"def print_report(y_true, y_pred):\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m    cm = confusion_matrix(y_true, y_pred)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m    report = classification_report(y_true, y_pred, target_names=categories)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39m    plt.ylabel('True')\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m    plt.show()\"\"\"\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mTweetDataset\u001b[39;00m(Dataset):\n\u001b[1;32m     20\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, x, y, mlb, tokenizer):\n\u001b[1;32m     21\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m x\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "def model_summary(model):\n",
    "    print(\"Model summary:\")\n",
    "    print(\"---------------------------\")\n",
    "    total_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        param_count = param.numel()\n",
    "        total_params += param_count\n",
    "    print(f\"Total parameters: {total_params}\")\n",
    "    \n",
    "\"\"\"def print_report(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, target_names=categories)\n",
    "    print(report)\n",
    "    sns.heatmap(cm, annot=True, xticklabels=categories, yticklabels=categories, fmt='g')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\"\"\"\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, x, y, mlb, tokenizer):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.mlb = mlb\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoded_tweets = self.preprocess_text(self.x)\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        return self.tokenizer(text, return_attention_mask=True, return_tensors='pt', padding=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.y[idx]\n",
    "        return {'input_ids': self.encoded_tweets['input_ids'][idx],\n",
    "                'attention_mask': self.encoded_tweets['attention_mask'][idx],\n",
    "                'label': torch.tensor(label, dtype=torch.float32)}\n",
    "        \n",
    "class MultiLabelDataCollator(DataCollatorWithPadding):\n",
    "    def __init__(self, tokenizer):\n",
    "        super().__init__(tokenizer)\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, torch.Tensor]]):\n",
    "        batch = super().__call__(features)\n",
    "        batch[\"labels\"] = torch.stack([feature[\"label\"] for feature in features])\n",
    "        return batch\n",
    "    \n",
    "def get_classification_report(data_loader, model, target_names, label_names):\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    for batch in data_loader:\n",
    "        batch_inputs = {'input_ids': batch['input_ids'].to(device),\n",
    "                        'attention_mask': batch['attention_mask'].to(device)}\n",
    "        with torch.no_grad():\n",
    "            logits = model(**batch_inputs).logits\n",
    "        sigmoid = torch.nn.Sigmoid()\n",
    "        probs = sigmoid(torch.Tensor(logits))\n",
    "        batch_predictions = (probs >= 0.5).detach().cpu().numpy().astype(int)\n",
    "        \n",
    "        predictions.append(batch_predictions)\n",
    "        labels.append(batch['labels'].detach().cpu().numpy().astype(int))\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    labels = np.concatenate(labels, axis = 0)\n",
    "\n",
    "    #cm = multilabel_confusion_matrix(labels, predictions)\n",
    "    dict_report = classification_report(labels, predictions, target_names=target_names, labels=label_names, zero_division=0, output_dict=True)\n",
    "    report = classification_report(labels, predictions, target_names=target_names, labels=label_names, zero_division=0)\n",
    "    return dict_report, report\n",
    "    \n",
    "def calculate_average_report(reports):\n",
    "    avg_report = {}\n",
    "    for report in reports:\n",
    "        for key, scores in report.items():\n",
    "            if key not in avg_report:\n",
    "                avg_report[key] = {}\n",
    "                for score_key, score_value in scores.items():\n",
    "                    avg_report[key][score_key] = score_value\n",
    "            else:\n",
    "                for score_key, score_value in scores.items():\n",
    "                    avg_report[key][score_key] += score_value\n",
    "\n",
    "    num_reports = len(reports)\n",
    "    for key, scores in avg_report.items():\n",
    "        for score_key in scores:\n",
    "            avg_report[key][score_key] /= num_reports\n",
    "\n",
    "    return avg_report\n",
    "\n",
    "def average_report_to_dataframe(average_report):\n",
    "    data = {\n",
    "        \"precision\": [],\n",
    "        \"recall\": [],\n",
    "        \"f1-score\": [],\n",
    "        \"support\": []\n",
    "    }\n",
    "    index = []\n",
    "\n",
    "    for class_name, metrics in average_report.items():\n",
    "        if class_name == 'accuracy':\n",
    "            continue\n",
    "\n",
    "        index.append(class_name)\n",
    "        data[\"precision\"].append(metrics[\"precision\"])\n",
    "        data[\"recall\"].append(metrics[\"recall\"])\n",
    "        data[\"f1-score\"].append(metrics[\"f1-score\"])\n",
    "        data[\"support\"].append(metrics[\"support\"])\n",
    "\n",
    "    return pd.DataFrame(data, index=index)\n",
    "\n",
    "def calculate_metrics(task):\n",
    "    k = 5\n",
    "    \n",
    "    val_classification_reports = []\n",
    "    test_classification_reports = []\n",
    "\n",
    "    # Loop over each fold and load the corresponding model\n",
    "    for fold in range(k):\n",
    "        model_path = f\"../models/{task}_epochs_200_train_size_full_fold_{fold}\"\n",
    "        # find the latest checkpoint file\n",
    "        #checkpoint_files = [f for f in os.listdir(model_path) if f.startswith(\"checkpoint\")]\n",
    "        latest_checkpoint = os.path.join(model_path, \"\")  # use \"\" for models that were manually saved after training. use sorted(checkpoint_files)[0] for the first automatically saved checkpoint \n",
    "        print(latest_checkpoint)\n",
    "        \n",
    "        # Load the model and tokenizer\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(latest_checkpoint)\n",
    "        model.to(device)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-large\")\n",
    "\n",
    "        filename = f\"../data/labeled_data/{task}_test_{fold}.json\"\n",
    "        with open(filename) as f:\n",
    "            data = json.load(f)\n",
    "        train_df = pd.DataFrame(data[\"train\"])\n",
    "        val_df = pd.DataFrame(data[\"valid\"])\n",
    "        test_df = pd.DataFrame(data[\"test\"])\n",
    "        \n",
    "        train_annotations = train_df[\"annotations\"].tolist()\n",
    "        classes = set()\n",
    "        for annotation in train_annotations:\n",
    "            classes.update(annotation)\n",
    "        classes = sorted(list(classes))\n",
    "        \n",
    "        checkpoint = torch.load(os.path.join(model_path, \"pytorch_model.bin\"))\n",
    "        model.load_state_dict(checkpoint)\n",
    "        \n",
    "        mlb = MultiLabelBinarizer(classes=classes)\n",
    "        \n",
    "        train_labels = mlb.fit_transform(train_df[\"annotations\"])\n",
    "        val_labels = mlb.transform(val_df[\"annotations\"])\n",
    "        test_labels = mlb.transform(test_df[\"annotations\"])\n",
    "        \n",
    "        train_dataset = TweetDataset(train_df['text'].to_list(), torch.tensor(train_labels), mlb, tokenizer)\n",
    "        val_dataset = TweetDataset(val_df['text'].to_list(), torch.tensor(val_labels), mlb, tokenizer)\n",
    "        test_dataset = TweetDataset(test_df['text'].to_list(), torch.tensor(test_labels), mlb, tokenizer)\n",
    "        \n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            val_dataset, batch_size=4, shuffle=False, collate_fn=MultiLabelDataCollator(tokenizer)\n",
    "        )\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            test_dataset, batch_size=4, shuffle=False, collate_fn=MultiLabelDataCollator(tokenizer)\n",
    "        )\n",
    "        \n",
    "        model.eval()\n",
    "        val_report_dict, val_report = get_classification_report(val_loader, model, classes, range(len(classes)))\n",
    "        test_report_dict, test_report = get_classification_report(test_loader, model, classes, range(len(classes)))\n",
    "        val_classification_reports.append(val_report_dict)\n",
    "        test_classification_reports.append(test_report_dict)\n",
    "\n",
    "    val_average_report = calculate_average_report(val_classification_reports)\n",
    "    test_average_report = calculate_average_report(test_classification_reports)\n",
    "    val_average_report_df = average_report_to_dataframe(val_average_report)\n",
    "    test_average_report_df = average_report_to_dataframe(test_average_report)\n",
    "    print(\"\\nAverage Validation Classification Report In DataFrame Format:\")\n",
    "    print(val_average_report_df) \n",
    "    print(\"\\nAverage Test Classification Report In DataFrame Format:\")\n",
    "    print(test_average_report_df) \n",
    "    return val_average_report_df, test_average_report_df\n",
    "\n",
    "generic_val_average_report_df, generic_test_average_report_df = calculate_metrics(\"generic\")\n",
    "GRU_202012_val_average_report_df, GRU_202012_test_average_report_df = calculate_metrics(\"GRU_202012\")\n",
    "IRA_202012_val_average_report_df, IRA_202012_test_average_report_df = calculate_metrics(\"IRA_202012\")\n",
    "REA_0621_val_average_report_df, REA_0621_test_average_report_df = calculate_metrics(\"REA_0621\")\n",
    "UGANDA_0621_val_average_report_df, UGANDA_0621_test_average_report_df = calculate_metrics(\"UGANDA_0621\")\n",
    "VENEZUELA_201901_2_val_average_report_df, VENEZUELA_201901_2_test_average_report_df = calculate_metrics(\"VENEZUELA_201901_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataframes = {\n",
    "    \"generic_val_average_report\": generic_val_average_report_df,\n",
    "    \"generic_test_average_report\": generic_test_average_report_df,\n",
    "    \"GRU_202012_val_average_report\": GRU_202012_val_average_report_df,\n",
    "    \"GRU_202012_test_average_report\": GRU_202012_test_average_report_df,\n",
    "    \"IRA_202012_val_average_report\": IRA_202012_val_average_report_df,\n",
    "    \"IRA_202012_test_average_report\": IRA_202012_test_average_report_df,\n",
    "    \"REA_0621_val_average_report\": REA_0621_val_average_report_df,\n",
    "    \"REA_0621_test_average_report\": REA_0621_test_average_report_df,\n",
    "    \"UGANDA_0621_val_average_report\": UGANDA_0621_val_average_report_df,\n",
    "    \"UGANDA_0621_test_average_report\": UGANDA_0621_test_average_report_df,\n",
    "    \"VENEZUELA_201901_2_val_average_report\": VENEZUELA_201901_2_val_average_report_df,\n",
    "    \"VENEZUELA_201901_2_test_average_report\": VENEZUELA_201901_2_test_average_report_df,\n",
    "}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    csv_filename = f\"../reports/{name}_normalized_tweets.csv\"\n",
    "    df.to_csv(csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7053414707928204"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generic_val_average_report_df[generic_val_average_report_df.index == \"macro avg\"][\"f1-score\"].values[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macro Averages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Train Data  Test Data  Validation  Test\n",
      "0            generic    generic        0.71  0.67\n",
      "1        All but GRU        GRU        0.67  0.44\n",
      "2        All but IRA        IRA        0.69  0.55\n",
      "3        All but REA        REA        0.69  0.55\n",
      "4     All but UGANDA     UGANDA        0.71  0.48\n",
      "5  All but VENEZUELA  VENEZUELA        0.66  0.59\n"
     ]
    }
   ],
   "source": [
    "def extract_macro_avg_value(df):\n",
    "    return df[df.index == \"macro avg\"][\"f1-score\"].values[0]\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    micro_avg_value = round(extract_macro_avg_value(df), 2)\n",
    "    train_data = name.split(\"_\")[0]\n",
    "\n",
    "    if \"val\" in name:\n",
    "        validation_value = micro_avg_value\n",
    "        test_value = None\n",
    "    elif \"test\" in name:\n",
    "        validation_value = None\n",
    "        test_value = micro_avg_value\n",
    "\n",
    "    test_data = train_data\n",
    "    if train_data != \"generic\":\n",
    "        train_data = \"All but \" + train_data\n",
    "\n",
    "    summary_data.append({\n",
    "        \"Train Data\": train_data,\n",
    "        \"Test Data\": test_data,\n",
    "        \"Validation\": validation_value,\n",
    "        \"Test\": test_value,\n",
    "    })\n",
    "\n",
    "# Combine rows with the same \"Train Data\" and \"Test Data\" into one\n",
    "macro_summary_df = pd.DataFrame(summary_data)\n",
    "macro_summary_df = macro_summary_df.groupby([\"Train Data\", \"Test Data\"], as_index=False).first()\n",
    "\n",
    "# Reorder columns\n",
    "macro_summary_df = macro_summary_df[[\"Train Data\", \"Test Data\", \"Validation\", \"Test\"]]\n",
    "macro_summary_df = macro_summary_df.reindex([macro_summary_df.index[-1]] + list(macro_summary_df.index[:-1]))\n",
    "macro_summary_df = macro_summary_df.reset_index(drop=True)\n",
    "\n",
    "print(macro_summary_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Micro Averages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no values:  micro avg    0.79461\n",
      "Name: f1-score, dtype: float64\n",
      "wotjvalies :  0.7946103414885577\n",
      "no values:  micro avg    0.776106\n",
      "Name: f1-score, dtype: float64\n",
      "wotjvalies :  0.776105954235275\n",
      "no values:  micro avg    0.776579\n",
      "Name: f1-score, dtype: float64\n",
      "wotjvalies :  0.7765792870382628\n",
      "no values:  micro avg    0.773844\n",
      "Name: f1-score, dtype: float64\n",
      "wotjvalies :  0.7738437287750406\n",
      "no values:  micro avg    0.814075\n",
      "Name: f1-score, dtype: float64\n",
      "wotjvalies :  0.8140749682464591\n",
      "no values:  micro avg    0.629581\n",
      "Name: f1-score, dtype: float64\n",
      "wotjvalies :  0.6295805199879053\n",
      "no values:  micro avg    0.796831\n",
      "Name: f1-score, dtype: float64\n",
      "wotjvalies :  0.7968310520875482\n",
      "no values:  micro avg    0.722081\n",
      "Name: f1-score, dtype: float64\n",
      "wotjvalies :  0.7220808010471017\n",
      "no values:  micro avg    0.800973\n",
      "Name: f1-score, dtype: float64\n",
      "wotjvalies :  0.8009728728839356\n",
      "no values:  micro avg    0.760924\n",
      "Name: f1-score, dtype: float64\n",
      "wotjvalies :  0.7609242914494959\n",
      "no values:  micro avg    0.790677\n",
      "Name: f1-score, dtype: float64\n",
      "wotjvalies :  0.7906766223201708\n",
      "no values:  micro avg    0.721082\n",
      "Name: f1-score, dtype: float64\n",
      "wotjvalies :  0.721082471647124\n",
      "          Train Data  Test Data  Validation  Test\n",
      "0            generic    generic        0.79  0.78\n",
      "1        All but GRU        GRU        0.78  0.77\n",
      "2        All but IRA        IRA        0.81  0.63\n",
      "3        All but REA        REA        0.80  0.72\n",
      "4     All but UGANDA     UGANDA        0.80  0.76\n",
      "5  All but VENEZUELA  VENEZUELA        0.79  0.72\n"
     ]
    }
   ],
   "source": [
    "def extract_micro_avg_value(df):\n",
    "    print(\"no values: \", df[df.index == \"micro avg\"][\"f1-score\"])\n",
    "    print(\"wotjvalies : \", df[df.index == \"micro avg\"][\"f1-score\"].values[0])\n",
    "    return df[df.index == \"micro avg\"][\"f1-score\"].values[0]\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    micro_avg_value = round(extract_micro_avg_value(df), 2)\n",
    "    train_data = name.split(\"_\")[0]\n",
    "\n",
    "    if \"val\" in name:\n",
    "        validation_value = micro_avg_value\n",
    "        test_value = None\n",
    "    elif \"test\" in name:\n",
    "        validation_value = None\n",
    "        test_value = micro_avg_value\n",
    "\n",
    "    test_data = train_data\n",
    "    if train_data != \"generic\":\n",
    "        train_data = \"All but \" + train_data\n",
    "\n",
    "    summary_data.append({\n",
    "        \"Train Data\": train_data,\n",
    "        \"Test Data\": test_data,\n",
    "        \"Validation\": validation_value,\n",
    "        \"Test\": test_value,\n",
    "    })\n",
    "\n",
    "# Combine rows with the same \"Train Data\" and \"Test Data\" into one\n",
    "micro_summary_df = pd.DataFrame(summary_data)\n",
    "micro_summary_df = micro_summary_df.groupby([\"Train Data\", \"Test Data\"], as_index=False).first()\n",
    "\n",
    "# Reorder columns\n",
    "micro_summary_df = micro_summary_df[[\"Train Data\", \"Test Data\", \"Validation\", \"Test\"]]\n",
    "micro_summary_df = micro_summary_df.reindex([micro_summary_df.index[-1]] + list(micro_summary_df.index[:-1]))\n",
    "micro_summary_df = micro_summary_df.reset_index(drop=True)\n",
    "\n",
    "print(micro_summary_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
