{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from emoji import demojize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import openai\n",
    "import time\n",
    "\n",
    "# Define a list of filenames to load\n",
    "filenames = [\"../data/labeled_data/generic_test_0.json\"]\n",
    "\n",
    "# Load all JSON data and concatenate into one DataFrame\n",
    "dfs = []\n",
    "for filename in filenames:\n",
    "    with open(filename) as f:\n",
    "        data = json.load(f)\n",
    "    df = pd.DataFrame(data[\"train\"])\n",
    "    dfs.append(df)\n",
    "    df = pd.DataFrame(data[\"test\"])\n",
    "    dfs.append(df)\n",
    "    df = pd.DataFrame(data[\"valid\"])\n",
    "    dfs.append(df)\n",
    "df_all = pd.concat(dfs)\n",
    "\n",
    "def normalizeToken(token):\n",
    "    lowercased_token = token.lower()\n",
    "    if token.startswith(\"@\"):\n",
    "        return \"@USER\"\n",
    "    elif lowercased_token.startswith(\"http\") or lowercased_token.startswith(\"www\"):\n",
    "        return \"[url]\"\n",
    "    elif len(token) == 1:\n",
    "        return demojize(token)\n",
    "    else:\n",
    "        if token == \"‚Äô\":\n",
    "            return \"'\"\n",
    "        elif token == \"‚Ä¶\":\n",
    "            return \"...\"\n",
    "        else:\n",
    "            return token\n",
    "    \n",
    "def normalizeTweet(tweet):\n",
    "    tokens = TweetTokenizer().tokenize(tweet.replace(\"‚Äô\", \"'\"))\n",
    "    normTweet = \" \".join([normalizeToken(token) for token in tokens])\n",
    "\n",
    "    normTweet = (\n",
    "        normTweet.replace(\"n 't\", \"n't\")\n",
    "            .replace(\"ca n't\", \"can't\")\n",
    "            .replace(\"ai n't\", \"ain't\")\n",
    "    )\n",
    "    normTweet = (\n",
    "        normTweet.replace(\"p . m .\", \"pm\")\n",
    "            .replace(\"p . m\", \"pm\")\n",
    "            .replace(\"a . m .\", \"am\")\n",
    "            .replace(\"a . m\", \"am\")\n",
    "    )\n",
    "    return \" \".join(normTweet.split())\n",
    "\n",
    "def api(prompt):\n",
    "    import requests\n",
    "\n",
    "# For local streaming, the websockets are hosted without ssl - http://\n",
    "HOST = 'http://127.0.0.1:5000'\n",
    "URI = f'{HOST}/api/v1/generate'\n",
    "\n",
    "# For reverse-proxied streaming, the remote will likely host with ssl - https://\n",
    "# URI = 'https://your-uri-here.trycloudflare.com/api/v1/generate'\n",
    "\n",
    "def get_openai_prompt_without_context_elaboration_first(tweet_text, label):\n",
    "    prompt = f\"Elaborate on whether you think the Tweet is about {label} or something else.\\n\\nTweet: {tweet_text}\\n\\n\"\n",
    "    followup = f\"\\nAssign the label 1 if it's about {label} or 0 for not based on the elaboration. Only output the number.\"\n",
    "    return prompt, followup\n",
    "\n",
    "def get_openai_prompt_without_context_only_classification(tweet_text, label):\n",
    "    prompt = f\"Classify the Tweet based on if it's about {label}. Use 1 or 0 as class.\\n\\nTweet: {tweet_text}\\nClass: \"\n",
    "    return prompt, \"\"\n",
    "\n",
    "def get_model_by_type(model_type):\n",
    "    if model_type == \"llama\":\n",
    "        return #get_llama_response\n",
    "    elif model_type == \"vicuna\":\n",
    "        return #get_vicuna_response\n",
    "    elif model_type == \"openassistant\":\n",
    "        return #get_openassistant_response\n",
    "    elif \"openai\" in model_type:\n",
    "        return get_openai_response\n",
    "    elif \"gpt-3.5\" in model_type:\n",
    "        return get_openai_response\n",
    "\n",
    "def get_response(prompt, first_model_type, second_model_type = \"\", follow_up = \"\", prompting_type = \"simple\", context = \"\", openai_model = \"\"):\n",
    "    \n",
    "    valid_models = [\"llama\", \"vicuna\", \"openassistant\", \"openai-davinci\", \"openai-gpt-3.5-turbo\"]\n",
    "    assert first_model_type in valid_models, \"First model type needs to be one of the following: \" + \", \".join(valid_models)\n",
    "    first_model = get_model_by_type(first_model_type)\n",
    "\n",
    "    if prompting_type == \"two-way\":\n",
    "        if second_model_type == \"\":\n",
    "            second_model = get_model_by_type(first_model_type)\n",
    "        else:\n",
    "            assert second_model_type in valid_models, \"Second model type needs to be one of the following: \" + \", \".join(valid_models)\n",
    "            assert follow_up != \"\", \"Follow up needs to be specified for two_way prompting type\"\n",
    "            second_model = get_model_by_type(second_model_type)\n",
    "\n",
    "        if openai_model != \"\":\n",
    "            #print(\"first prompt: \", prompt)\n",
    "            first_response = first_model(prompt, context = context, model = openai_model)\n",
    "            if \"gpt\" in second_model_type:\n",
    "                first_response = [prompt, {\"role\": \"assistant\", \"content\": first_response}]\n",
    "            #time.sleep(2)\n",
    "            #print(\"First response: \", first_response)\n",
    "            second_response = second_model(follow_up, context = prompt + first_response, model = openai_model)\n",
    "            return second_response\n",
    "        \n",
    "    if prompting_type == \"simple\":\n",
    "        return first_model(prompt)\n",
    "\n",
    "\n",
    "def get_openai_response(prompt, context = [], model = \"gpt-3.5-turbo\"):\n",
    "    # Use OpenAI's ChatCompletion API to get the chatbot's response\n",
    "\n",
    "    if \"gpt\" in model:\n",
    "        messages = []\n",
    "        if context != []:\n",
    "            for c in context:\n",
    "                messages.append(c)\n",
    "        messages.append(prompt)\n",
    "    else:\n",
    "        prompt = context + prompt\n",
    "        #print(\"Context: \", context)\n",
    "        #print(\"Full prompt: \", prompt)\n",
    "\n",
    "    if model == \"gpt-3.5-turbo\":\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",  # The name of the OpenAI chatbot model to use\n",
    "            messages=messages,   # The conversation history up to this point, as a list of dictionaries\n",
    "            max_tokens=200,        # The maximum number of tokens (words or subwords) in the generated response\n",
    "            stop=None,              # The stopping sequence for the generated response, if any (not used here)\n",
    "            temperature=0.7,        # The \"creativity\" of the generated response (higher temperature = more creative)\n",
    "        )\n",
    "\n",
    "    elif model == \"davinci\":\n",
    "        response = openai.Completion.create(\n",
    "            model=\"text-davinci-003\",  # The name of the OpenAI chatbot model to use\n",
    "            prompt=prompt,   # The conversation history up to this point, as a list of dictionaries\n",
    "            max_tokens=200,        # The maximum number of tokens (words or subwords) in the generated response\n",
    "            stop=None,              # The stopping sequence for the generated response, if any (not used here)\n",
    "            temperature=0.7,        # The \"creativity\" of the generated response (higher temperature = more creative)\n",
    "        )\n",
    "\n",
    "    # Find the first response from the chatbot that has text in it (some responses may not have text)\n",
    "    for choice in response.choices:\n",
    "        if \"text\" in choice:\n",
    "            return choice.text\n",
    "\n",
    "    # If no response with text is found, return the first response's content (which may be empty)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Your original DataFrame: df_all\n",
    "# all_labels: list of labels\n",
    "\n",
    "balanced_dfs = []\n",
    "\n",
    "rules = [\"Oxford dictionary's definition of war: ‚Äúsituation in which two or more countries or groups of people fight against each other over a period of time‚Äù. Oxford dictionary's definition of terror (terrorism): ‚Äúviolent action or the threat of violent action that is intended to cause fear, usually for political purposes‚Äù. Remark: This category includes also causes and consequences of war/terror (e.g. ‚Äúthe current situation in Ukraine may cause a supply crisis for wheat products‚Äù).\",\n",
    "\"Oxford dictionary's definition of conspiracy: ‚Äúa secret plan by a group of people to do something harmful or illegal‚Äù. Remark: Assignment of this category may depend on viewpoint and political stance of rater, which can be mitigated by focusing on the definition above. If the content of a tweet describes a conspiratorial activity/process, it will be labeled ‚Äúconspiracy theory‚Äù.\",\n",
    "\"Oxford dictionary's definition of education: ‚Äúa process of teaching, training and learning, especially in schools, colleges or universities, to improve knowledge and develop skills‚Äù. Remark: Does not include education/training of soldiers (ü°™war/terror).\",\n",
    "\"Oxford dictionary's definition of election: ‚Äúthe process of choosing a person or a group of people for a position, especially a political position, by voting‚Äù. Remark: This category includes all activities aimed at rallying the population for participation in a public election, description of election outcomes, and conduct of the election itself.\",\n",
    "\"Oxford dictionary's definition of environment: ‚Äúthe natural world in which people, animals and plants live‚Äù. Remark: This category is typically used for tweet content revolving around activities and processes affecting the environment in some way.\",\n",
    "\"Oxford dictionary's definition of government: ‚Äúthe group of people who are responsible for controlling a country or a state‚Äù. Oxford dictionary's definition of public: ‚Äúordinary people who are not members of a particular group or organization‚Äù Remark: This category includes also statements/content about the public perception of activities/processes of government (i.e. voiced criticism or praise for a government).\",\n",
    "\"Oxford dictionary's definition of health: ‚Äúthe condition of a person's body or mind‚Äù. Remark: This category includes also statements related to public health. In such a case both Health and Government/Public must be selected.\",\n",
    "\"Oxford dictionary's definition of immigration: ‚Äúthe process of coming to live permanently in a different country from the one you were born in‚Äù. Oxford dictionary's definition of integration: ‚Äúthe act or process of mixing people who have previously been separated, usually because of colour, race, religion, etc.‚Äù\",\n",
    "\"Oxford dictionary's definition of justice: ‚Äúthe legal system used to punish people who have committed crimes‚Äù. Oxford dictionary's definition of crime: ‚Äúactivities that involve breaking the law‚Äù. Remark: This category does not include statements/content on war crimes (ü°™ war/terror).\",\n",
    "\"Oxford dictionary's definition of labor: ‚Äúwork, especially physical work‚Äù. Oxford dictionary's definition of employment: ‚Äúwork, especially when it is done to earn money; the state of being employed‚Äù.\",\n",
    "\"Oxford dictionary's definition of macroeconomics: ‚Äúthe study of large economic systems, such as those of whole countries or areas of the world‚Äù. Oxford dictionary's definition of regulation: ‚Äùan official rule made by a government or some other authority‚Äù. Remark: In case of statements/content on economic regulations, this category may likely co-occur with Government/Public category.\", \n",
    "\"Oxford dictionary's definition of media: ‚Äúthe main ways that large numbers of people receive information and entertainment, that is television, radio, newspapers and the internet‚Äù. Oxford dictionary's definition of journalism: ‚Äúthe work of collecting and writing news stories for newspapers, magazines, radio, television or online news sites; the news stories that are written‚Äù. Remark: This category will be used for statements/content which explicitly references other media outlets or journalists (e.g. ‚ÄúBBC has reported that ‚Ä¶‚Äù, ‚ÄúBellingcat has discovered a secret operation of X‚Äù). Content which appears ‚Äúnews-worthy‚Äù does not generally fall into this category (ü°™ newsworthiness is very subjective and context-dependent).\",\n",
    "\"Oxford dictionary's definition of religion: ‚Äúthe belief in the existence of a god or gods, and the activities that are connected with the worship of them, or in the teachings of a spiritual leader‚Äù.\",\n",
    "\"Oxford dictionary's definition of science: ‚Äúknowledge about the structure and behavior of the natural and physical world, based on facts that you can prove, for example by experiments‚Äù. Oxford dictionary's definition of technology: ‚Äúscientific knowledge used in practical ways in industry, for example in designing new machines‚Äù.\"]\n",
    "\n",
    "all_labels = [\"War/Terror\", \"Conspiracy Theory\", \"Education\", \"Election Campaign\", \"Environment\", \n",
    "              \"Government/Public\", \"Health\", \"Immigration/Integration\", \n",
    "              \"Justice/Crime\", \"Labor/Employment\", \n",
    "              \"Macroeconomics/Economic Regulation\", \"Media/Journalism\", \"Religion\", \"Science/Technology\"]\n",
    "\n",
    "for label in all_labels:\n",
    "    # Initialize an empty DataFrame for the balanced dataset\n",
    "    balanced_df = pd.DataFrame()\n",
    "    # Get the rows with the current label\n",
    "    label_rows = df_all[df_all['annotations'].apply(lambda x: label in x)]\n",
    "    \n",
    "    # Get the rows without the current label\n",
    "    non_label_rows = df_all[df_all['annotations'].apply(lambda x: label not in x)]\n",
    "    \n",
    "    # Sample 65 rows with the current label\n",
    "    sample_label_rows = label_rows.sample(n=65, random_state=42)\n",
    "    \n",
    "    # Sample 65 rows without the current label\n",
    "    sample_non_label_rows = non_label_rows.sample(n=65, random_state=42)\n",
    "    \n",
    "    # Combine the samples\n",
    "    combined_sample = pd.concat([sample_label_rows, sample_non_label_rows], ignore_index=True)\n",
    "    \n",
    "    # Add the samples to the balanced DataFrame\n",
    "    balanced_df = pd.concat([balanced_df, combined_sample], ignore_index=True)\n",
    "\n",
    "    balanced_dfs.append(balanced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting requesting for label: War/Terror\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/130 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full prompt:  Elaborate on whether you think the Tweet is about War/Terror or something else.\n",
      "\n",
      "Tweet: The Western coalition led by the #UnitedStates struck a blow at the village of Hoveibaria , where a temporary camp for refugees from Iraq was placed . As a result of the #attack , at least 18 people were killed #Syria #Khaseke [url]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/130 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 40\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"prompt, followup = get_openai_prompt_without_context_only_classification(tweet_text, label)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39mresponse = get_response(prompt, \"openai-davinci\", prompting_type = \"simple\")\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m prompt, followup \u001b[39m=\u001b[39m get_openai_prompt_without_context_elaboration_first(tweet_text, label)\n\u001b[0;32m---> 40\u001b[0m response \u001b[39m=\u001b[39m get_response(prompt, \u001b[39m\"\u001b[39m\u001b[39mopenai-davinci\u001b[39m\u001b[39m\"\u001b[39m, prompting_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtwo-way\u001b[39m\u001b[39m\"\u001b[39m, follow_up\u001b[39m=\u001b[39mfollowup, openai_model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdavinci\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[39m#print(response)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m \u001b[39m# Save the response in the 'api_results' column\u001b[39;00m\n\u001b[1;32m     44\u001b[0m df_all\u001b[39m.\u001b[39mloc[\u001b[39mlambda\u001b[39;00m df: df[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m row[\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m], new_column_name] \u001b[39m=\u001b[39m response\n",
      "Cell \u001b[0;32mIn[89], line 117\u001b[0m, in \u001b[0;36mget_response\u001b[0;34m(prompt, first_model_type, second_model_type, follow_up, prompting_type, context, openai_model)\u001b[0m\n\u001b[1;32m    113\u001b[0m     second_model \u001b[39m=\u001b[39m get_model_by_type(second_model_type)\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m openai_model \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    116\u001b[0m     \u001b[39m#print(\"first prompt: \", prompt)\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     first_response \u001b[39m=\u001b[39m first_model(prompt, context \u001b[39m=\u001b[39m context, model \u001b[39m=\u001b[39m openai_model)\n\u001b[1;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mgpt\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m second_model_type:\n\u001b[1;32m    119\u001b[0m         first_response \u001b[39m=\u001b[39m [prompt, {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: first_response}]\n",
      "Cell \u001b[0;32mIn[89], line 153\u001b[0m, in \u001b[0;36mget_openai_response\u001b[0;34m(prompt, context, model)\u001b[0m\n\u001b[1;32m    144\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39mChatCompletion\u001b[39m.\u001b[39mcreate(\n\u001b[1;32m    145\u001b[0m         model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m\"\u001b[39m,  \u001b[39m# The name of the OpenAI chatbot model to use\u001b[39;00m\n\u001b[1;32m    146\u001b[0m         messages\u001b[39m=\u001b[39mmessages,   \u001b[39m# The conversation history up to this point, as a list of dictionaries\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m         temperature\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m,        \u001b[39m# The \"creativity\" of the generated response (higher temperature = more creative)\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     )\n\u001b[1;32m    152\u001b[0m \u001b[39melif\u001b[39;00m model \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdavinci\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 153\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39mCompletion\u001b[39m.\u001b[39mcreate(\n\u001b[1;32m    154\u001b[0m         model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtext-davinci-003\u001b[39m\u001b[39m\"\u001b[39m,  \u001b[39m# The name of the OpenAI chatbot model to use\u001b[39;00m\n\u001b[1;32m    155\u001b[0m         prompt\u001b[39m=\u001b[39mprompt,   \u001b[39m# The conversation history up to this point, as a list of dictionaries\u001b[39;00m\n\u001b[1;32m    156\u001b[0m         max_tokens\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m,        \u001b[39m# The maximum number of tokens (words or subwords) in the generated response\u001b[39;00m\n\u001b[1;32m    157\u001b[0m         stop\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,              \u001b[39m# The stopping sequence for the generated response, if any (not used here)\u001b[39;00m\n\u001b[1;32m    158\u001b[0m         temperature\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m,        \u001b[39m# The \"creativity\" of the generated response (higher temperature = more creative)\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     )\n\u001b[1;32m    161\u001b[0m \u001b[39m# Find the first response from the chatbot that has text in it (some responses may not have text)\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mfor\u001b[39;00m choice \u001b[39min\u001b[39;00m response\u001b[39m.\u001b[39mchoices:\n",
      "File \u001b[0;32m~/anaconda3/envs/my_env/lib/python3.11/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/anaconda3/envs/my_env/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/anaconda3/envs/my_env/lib/python3.11/site-packages/openai/api_requestor.py:216\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    217\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    218\u001b[0m         url,\n\u001b[1;32m    219\u001b[0m         params\u001b[39m=\u001b[39mparams,\n\u001b[1;32m    220\u001b[0m         supplied_headers\u001b[39m=\u001b[39mheaders,\n\u001b[1;32m    221\u001b[0m         files\u001b[39m=\u001b[39mfiles,\n\u001b[1;32m    222\u001b[0m         stream\u001b[39m=\u001b[39mstream,\n\u001b[1;32m    223\u001b[0m         request_id\u001b[39m=\u001b[39mrequest_id,\n\u001b[1;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    225\u001b[0m     )\n\u001b[1;32m    226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/anaconda3/envs/my_env/lib/python3.11/site-packages/openai/api_requestor.py:516\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    514\u001b[0m     _thread_context\u001b[39m.\u001b[39msession \u001b[39m=\u001b[39m _make_session()\n\u001b[1;32m    515\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 516\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39msession\u001b[39m.\u001b[39mrequest(\n\u001b[1;32m    517\u001b[0m         method,\n\u001b[1;32m    518\u001b[0m         abs_url,\n\u001b[1;32m    519\u001b[0m         headers\u001b[39m=\u001b[39mheaders,\n\u001b[1;32m    520\u001b[0m         data\u001b[39m=\u001b[39mdata,\n\u001b[1;32m    521\u001b[0m         files\u001b[39m=\u001b[39mfiles,\n\u001b[1;32m    522\u001b[0m         stream\u001b[39m=\u001b[39mstream,\n\u001b[1;32m    523\u001b[0m         timeout\u001b[39m=\u001b[39mrequest_timeout \u001b[39mif\u001b[39;00m request_timeout \u001b[39melse\u001b[39;00m TIMEOUT_SECS,\n\u001b[1;32m    524\u001b[0m         proxies\u001b[39m=\u001b[39m_thread_context\u001b[39m.\u001b[39msession\u001b[39m.\u001b[39mproxies,\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    527\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mTimeout(\u001b[39m\"\u001b[39m\u001b[39mRequest timed out: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/my_env/lib/python3.11/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/my_env/lib/python3.11/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/envs/my_env/lib/python3.11/site-packages/requests/adapters.py:487\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    484\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    486\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 487\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(\n\u001b[1;32m    488\u001b[0m         method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[1;32m    489\u001b[0m         url\u001b[39m=\u001b[39murl,\n\u001b[1;32m    490\u001b[0m         body\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mbody,\n\u001b[1;32m    491\u001b[0m         headers\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mheaders,\n\u001b[1;32m    492\u001b[0m         redirect\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    493\u001b[0m         assert_same_host\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    494\u001b[0m         preload_content\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    495\u001b[0m         decode_content\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    496\u001b[0m         retries\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_retries,\n\u001b[1;32m    497\u001b[0m         timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[1;32m    498\u001b[0m         chunked\u001b[39m=\u001b[39mchunked,\n\u001b[1;32m    499\u001b[0m     )\n\u001b[1;32m    501\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    502\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/anaconda3/envs/my_env/lib/python3.11/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/my_env/lib/python3.11/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/my_env/lib/python3.11/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/my_env/lib/python3.11/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         response\u001b[39m.\u001b[39mbegin()\n\u001b[1;32m   1376\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/my_env/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/my_env/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mreadline(_MAXLINE \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/my_env/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sock\u001b[39m.\u001b[39mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/my_env/lib/python3.11/ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1275\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1276\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1277\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread(nbytes, buffer)\n\u001b[1;32m   1279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/anaconda3/envs/my_env/lib/python3.11/ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m, buffer)\n\u001b[1;32m   1135\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Usage:\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "df_all['normalized_tweet'] = None\n",
    "normalized_tweets_db = {}\n",
    "\n",
    "for idx, label in enumerate(all_labels):\n",
    "\n",
    "    sample_df = balanced_dfs[idx]\n",
    "\n",
    "    print(\"Starting requesting for label: \" + label + \"\\n\")\n",
    "\n",
    "    new_column_name = f'{label}_pred'\n",
    "\n",
    "    # Add a new column for the API results\n",
    "    df_all[new_column_name] = None\n",
    "    #current_timestamp = datetime.now().strftime(\"%H%M%S\")\n",
    "    output_folder = f\"../data/openai_text_davinci_003/generic_prompt_without_context_elaboration_first/\"\n",
    "\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    i = 0\n",
    "    # Iterate over the rows of the sample_df\n",
    "    for index, row in tqdm(sample_df.iterrows(), total=sample_df.shape[0]):\n",
    "\n",
    "        tweet_text = normalizeTweet(row['text'])\n",
    "        df_all.loc[lambda df: df['id'] == row[\"id\"], 'normalized_tweet'] = tweet_text\n",
    "\n",
    "        \"\"\"prompt, followup = get_openai_prompt_without_context_elaboration_first(tweet_text, label)\n",
    "        prompt = {\"role\": \"user\", \"content\": prompt}\n",
    "        followup = {\"role\": \"system\", \"content\": followup}\n",
    "        response = get_response(prompt, \"openai-gpt-3.5-turbo\", prompting_type = \"two-way\", follow_up=followup)\"\"\"\n",
    "\n",
    "        \"\"\"prompt, followup = get_openai_prompt_without_context_only_classification(tweet_text, label)\n",
    "        response = get_response(prompt, \"openai-davinci\", prompting_type = \"simple\")\"\"\"\n",
    "\n",
    "        prompt, followup = get_openai_prompt_without_context_elaboration_first(tweet_text, label)\n",
    "        response = get_response(prompt, \"openai-davinci\", prompting_type = \"two-way\", follow_up=followup, openai_model=\"davinci\")\n",
    "        #print(response)\n",
    "\n",
    "        # Save the response in the 'api_results' column\n",
    "        df_all.loc[lambda df: df['id'] == row[\"id\"], new_column_name] = response\n",
    "        \n",
    "        i+=1\n",
    "        # Save the DataFrame to a CSV file every 100 steps\n",
    "        if (i + 1) % 100 == 0:\n",
    "            output_path = os.path.join(output_folder, 'generic_test_0.csv')\n",
    "            df_all.to_csv(output_path, index=False)\n",
    "            print(f\"Saved progress at index {index}\")\n",
    "            print(\"Sample Tweet: \", tweet_text)\n",
    "            print(\"Sample Annotation: \", response)\n",
    "\n",
    "    # Save the final DataFrame to a CSV file\n",
    "    output_path = os.path.join(output_folder, 'generic_test_0.csv')\n",
    "    df_all.to_csv(output_path, index=False)\n",
    "\n",
    "    # Save the final DataFrame to a CSV file\n",
    "output_path = os.path.join(output_folder, 'generic_test_0.csv')\n",
    "df_all.to_csv(output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
