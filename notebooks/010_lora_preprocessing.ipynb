{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from emoji import demojize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def normalizeToken(token):\n",
    "    lowercased_token = token.lower()\n",
    "    if token.startswith(\"@\"):\n",
    "        return \"@USER\"\n",
    "    elif lowercased_token.startswith(\"http\") or lowercased_token.startswith(\"www\"):\n",
    "        return \"[url]\"\n",
    "    elif len(token) == 1:\n",
    "        return demojize(token)\n",
    "    else:\n",
    "        if token == \"’\":\n",
    "            return \"'\"\n",
    "        elif token == \"…\":\n",
    "            return \"...\"\n",
    "        else:\n",
    "            return token\n",
    "    \n",
    "def normalizeTweet(tweet):\n",
    "    tokens = TweetTokenizer().tokenize(tweet.replace(\"’\", \"'\"))\n",
    "    normTweet = \" \".join([normalizeToken(token) for token in tokens])\n",
    "\n",
    "    normTweet = (\n",
    "        normTweet.replace(\"n 't\", \"n't\")\n",
    "            .replace(\"ca n't\", \"can't\")\n",
    "            .replace(\"ai n't\", \"ain't\")\n",
    "    )\n",
    "    normTweet = (\n",
    "        normTweet.replace(\"p . m .\", \"pm\")\n",
    "            .replace(\"p . m\", \"pm\")\n",
    "            .replace(\"a . m .\", \"am\")\n",
    "            .replace(\"a . m\", \"am\")\n",
    "    )\n",
    "    return \" \".join(normTweet.split())\n",
    "\n",
    "def normalizeTweet_v02(tweet):\n",
    "    tokens = TweetTokenizer().tokenize(tweet.replace(\"’\", \"'\"))\n",
    "    normTweet = \" \".join([normalizeToken(token) for token in tokens])\n",
    "\n",
    "    normTweet = (\n",
    "        normTweet.replace(\"n 't\", \"n't\")\n",
    "            .replace(\"ca n't\", \"can't\")\n",
    "            .replace(\"ai n't\", \"ain't\")\n",
    "    )\n",
    "    normTweet = (\n",
    "        normTweet.replace(\"p . m .\", \"pm\")\n",
    "            .replace(\"p . m\", \"pm\")\n",
    "            .replace(\"a . m .\", \"am\")\n",
    "            .replace(\"a . m\", \"am\")\n",
    "    )\n",
    "    normTweet = (\n",
    "        normTweet.replace(\"p . m .\", \"pm\")\n",
    "    )\n",
    "    return \" \".join(normTweet.split())\n",
    "\n",
    "def reformat_json_v01(input_file, output_file, dataset_type=\"train\"):\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    formatted_data = []\n",
    "    for item in data[dataset_type]:\n",
    "        formatted_item = {\n",
    "            \"instruction\": \"Assign multilabel labels to the following tweet. Choose out of the following list of labels where 'Others' is only assigned if no other label fits: ['War/Terror', 'Conspiracy Theory', Education', 'Election Campaign', 'Environment', 'Government/Public', 'Health', 'Immigration/Integration', 'Justice/Crime', 'Labor/Employment', 'Macroeconomics/Economic Regulation', 'Media/Journalism', 'Religion', 'Science/Technology', 'Others']\",\n",
    "            \"input\": normalizeTweet(item[\"text\"]),\n",
    "            \"output\": str(item[\"annotations\"])\n",
    "        }\n",
    "        formatted_data.append(formatted_item)\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(formatted_data, f, indent=4)\n",
    "\n",
    "def reformat_json_v01_retest(input_file, output_file, dataset_type=\"train\"):\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    formatted_data = []\n",
    "    for item in data[dataset_type]:\n",
    "        tweet_text = normalizeTweet(item[\"text\"])\n",
    "        instruction = \"Out of the following list of topics, choose topics that best fit the tweet in your opinion. Output in format ['topic 1', 'topic 2', ...].\\n\\nTopics:\\n['War/Terror', 'Conspiracy Theory', 'Education', 'Election Campaign', 'Environment', 'Government/Public', 'Health', 'Immigration/Integration', 'Justice/Crime', 'Labor/Employment', 'Macroeconomics/Economic Regulation', 'Media/Journalism', 'Religion', 'Science/Technology']\"\n",
    "        prompt = f\"### Human:\\n{instruction}\\n\\nTweet: {tweet_text}\\n\\n### Assistant:\\n\"\n",
    "        formatted_item = {\n",
    "            \"instruction\": prompt,\n",
    "            \"output\": str(item[\"annotations\"])\n",
    "        }\n",
    "        formatted_data.append(formatted_item)\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(formatted_data, f, indent=4)\n",
    "\n",
    "def reformat_json_v02(input_file, output_file, dataset_type=\"train\"):\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    formatted_data = []\n",
    "    for item in data[dataset_type]:\n",
    "        formatted_item = {\n",
    "            \"instruction\": \"Out of the following list of topics, choose topics that best fit the text in your opinion. Here are the rules:\\n- Use clear indicators for labeling.\\n- If unclear, output 'Others'. \\n- No speculation.\\n- Don't mix content from different operations.\\n- Refer to Oxford definitions for topics.\\n\\nTopics: 'War/Terror', 'Conspiracy Theory', 'Education', 'Election Campaign', 'Environment', 'Government/Public', 'Health', 'Immigration/Integration', 'Justice/Crime', 'Labor/Employment', 'Macroeconomics/Economic Regulation', 'Media/Journalism', 'Religion', 'Science/Technology'\",\n",
    "            \"input\": normalizeTweet(item[\"text\"]),\n",
    "            \"output\": str(item[\"annotations\"])\n",
    "        }\n",
    "        formatted_data.append(formatted_item)\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(formatted_data, f, indent=4)\n",
    "\n",
    "def reformat_json_binary_v01(label, input_file, output_file, dataset_type=\"train\"):\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    positive_samples = []\n",
    "    negative_samples = []\n",
    "    \n",
    "    for item in data[dataset_type]:\n",
    "        formatted_item = {\n",
    "            \"instruction\": f\"Classify the input based on if it's about {label}. Use 1 (True) or 0 (False) as output.\",\n",
    "            \"input\": normalizeTweet(item[\"text\"]),\n",
    "            \"output\": str(int(label in item[\"annotations\"]))\n",
    "        }\n",
    "        if label in item[\"annotations\"]:\n",
    "            positive_samples.append(formatted_item)\n",
    "        else:\n",
    "            negative_samples.append(formatted_item)\n",
    "\n",
    "    # Choose the same amount of negative samples as there are positive samples\n",
    "    if len(negative_samples) > len(positive_samples):\n",
    "        negative_samples = random.sample(negative_samples, len(positive_samples))\n",
    "\n",
    "    # Merge positive and negative samples\n",
    "    formatted_data = positive_samples + negative_samples\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(formatted_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage binary:\n",
    "label = \"Election Campaign\"\n",
    "input_file = \"../data/labeled_data/generic_test_0.json\"\n",
    "output_file_train = f\"../data/labeled_data/lora_binary_train_{label.split('/')[0]}_v01.json\"\n",
    "output_file_test = f\"../data/labeled_data/lora_binary_test_{label.split('/')[0]}_v01.json\"\n",
    "output_file_valid = f\"../data/labeled_data/lora_binary_valid_{label.split('/')[0]}_v01.json\"\n",
    "\n",
    "reformat_json_binary_v01(label, input_file, output_file_train, dataset_type=\"train\")\n",
    "reformat_json_binary_v01(label, input_file, output_file_test, dataset_type=\"test\")\n",
    "reformat_json_binary_v01(label, input_file, output_file_valid, dataset_type=\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage multilabel:\n",
    "input_file = \"../data/labeled_data/generic_test_0.json\" # generic \"test\" refers to previous report test and not the actual test set, this file has train test and validation set included\n",
    "output_file_train = f\"../data/labeled_data/lora_no_context_multilabel_v01_train.json\"\n",
    "output_file_test = f\"../data/labeled_data/lora_no_context_multilabel_v01_test.json\"\n",
    "output_file_valid = f\"../data/labeled_data/lora_no_context_multilabel_v01_valid.json\"\n",
    "\n",
    "reformat_json_v01(input_file, output_file_train, dataset_type=\"train\")\n",
    "reformat_json_v01(input_file, output_file_test, dataset_type=\"test\")\n",
    "reformat_json_v01(input_file, output_file_valid, dataset_type=\"valid\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the amount of 1 or 0s in the binary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 754, '0': 754}\n",
      "{'1': 255, '0': 255}\n",
      "{'1': 185, '0': 185}\n"
     ]
    }
   ],
   "source": [
    "def count_labels(file):\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    label_count = {'1': 0, '0': 0}\n",
    "    for item in data:\n",
    "        label_count[item[\"output\"]] += 1\n",
    "\n",
    "    return label_count\n",
    "\n",
    "# Check the distribution of labels in the output files\n",
    "print(count_labels(output_file_train))\n",
    "print(count_labels(output_file_test))\n",
    "print(count_labels(output_file_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
