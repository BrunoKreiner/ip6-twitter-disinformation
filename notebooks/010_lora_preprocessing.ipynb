{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from emoji import demojize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../src\")\n",
    "import prompt_utils\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "random.seed(42)\n",
    "\n",
    "def normalizeToken(token):\n",
    "    lowercased_token = token.lower()\n",
    "    if token.startswith(\"@\"):\n",
    "        return \"@USER\"\n",
    "    elif lowercased_token.startswith(\"http\") or lowercased_token.startswith(\"www\"):\n",
    "        return \"[url]\"\n",
    "    elif len(token) == 1:\n",
    "        return demojize(token)\n",
    "    else:\n",
    "        if token == \"’\":\n",
    "            return \"'\"\n",
    "        elif token == \"…\":\n",
    "            return \"...\"\n",
    "        else:\n",
    "            return token\n",
    "    \n",
    "def normalizeTweet(tweet):\n",
    "    tokens = TweetTokenizer().tokenize(tweet.replace(\"’\", \"'\"))\n",
    "    normTweet = \" \".join([normalizeToken(token) for token in tokens])\n",
    "\n",
    "    normTweet = (\n",
    "        normTweet.replace(\"n 't\", \"n't\")\n",
    "            .replace(\"ca n't\", \"can't\")\n",
    "            .replace(\"ai n't\", \"ain't\")\n",
    "    )\n",
    "    normTweet = (\n",
    "        normTweet.replace(\"p . m .\", \"pm\")\n",
    "            .replace(\"p . m\", \"pm\")\n",
    "            .replace(\"a . m .\", \"am\")\n",
    "            .replace(\"a . m\", \"am\")\n",
    "    )\n",
    "    return \" \".join(normTweet.split())\n",
    "\n",
    "def normalizeTweet_v02(tweet):\n",
    "    tokens = TweetTokenizer().tokenize(tweet.replace(\"’\", \"'\"))\n",
    "    normTweet = \" \".join([normalizeToken(token) for token in tokens])\n",
    "\n",
    "    normTweet = (\n",
    "        normTweet.replace(\"n 't\", \"n't\")\n",
    "            .replace(\"ca n't\", \"can't\")\n",
    "            .replace(\"ai n't\", \"ain't\")\n",
    "    )\n",
    "    normTweet = (\n",
    "        normTweet.replace(\"p . m .\", \"pm\")\n",
    "            .replace(\"p . m\", \"pm\")\n",
    "            .replace(\"a . m .\", \"am\")\n",
    "            .replace(\"a . m\", \"am\")\n",
    "    )\n",
    "    normTweet = (\n",
    "        normTweet.replace(\"p . m .\", \"pm\")\n",
    "    )\n",
    "    return \" \".join(normTweet.split())\n",
    "\n",
    "def reformat_json_v01(input_file, output_file, dataset_type=\"train\"):\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    formatted_data = []\n",
    "    for item in data[dataset_type]:\n",
    "        formatted_item = {\n",
    "            \"instruction\": \"Assign multilabel labels to the following tweet. Choose out of the following list of labels where 'Others' is only assigned if no other label fits: ['War/Terror', 'Conspiracy Theory', Education', 'Election Campaign', 'Environment', 'Government/Public', 'Health', 'Immigration/Integration', 'Justice/Crime', 'Labor/Employment', 'Macroeconomics/Economic Regulation', 'Media/Journalism', 'Religion', 'Science/Technology', 'Others']\",\n",
    "            \"input\": normalizeTweet(item[\"text\"]),\n",
    "            \"output\": str(item[\"annotations\"])\n",
    "        }\n",
    "        formatted_data.append(formatted_item)\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(formatted_data, f, indent=4)\n",
    "\n",
    "def reformat_json_v01_retest(input_file, output_file, dataset_type=\"train\"):\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    formatted_data = []\n",
    "    for item in data[dataset_type]:\n",
    "        tweet_text = normalizeTweet(item[\"text\"])\n",
    "        instruction = \"Out of the following list of topics, choose topics that best fit the tweet in your opinion. Output in format ['topic 1', 'topic 2', ...].\\n\\nTopics:\\n['War/Terror', 'Conspiracy Theory', 'Education', 'Election Campaign', 'Environment', 'Government/Public', 'Health', 'Immigration/Integration', 'Justice/Crime', 'Labor/Employment', 'Macroeconomics/Economic Regulation', 'Media/Journalism', 'Religion', 'Science/Technology']\"\n",
    "        prompt = f\"### Human:\\n{instruction}\\n\\nTweet: {tweet_text}\\n\\n### Assistant:\\n\"\n",
    "        formatted_item = {\n",
    "            \"instruction\": prompt,\n",
    "            \"output\": str(item[\"annotations\"])\n",
    "        }\n",
    "        formatted_data.append(formatted_item)\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(formatted_data, f, indent=4)\n",
    "\n",
    "def reformat_json_v02(input_file, output_file, dataset_type=\"train\"):\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    formatted_data = []\n",
    "    for item in data[dataset_type]:\n",
    "        formatted_item = {\n",
    "            \"instruction\": \"Out of the following list of topics, choose topics that best fit the text in your opinion. Here are the rules:\\n- Use clear indicators for labeling.\\n- If unclear, output 'Others'. \\n- No speculation.\\n- Don't mix content from different operations.\\n- Refer to Oxford definitions for topics.\\n\\nTopics: 'War/Terror', 'Conspiracy Theory', 'Education', 'Election Campaign', 'Environment', 'Government/Public', 'Health', 'Immigration/Integration', 'Justice/Crime', 'Labor/Employment', 'Macroeconomics/Economic Regulation', 'Media/Journalism', 'Religion', 'Science/Technology'\",\n",
    "            \"input\": normalizeTweet(item[\"text\"]),\n",
    "            \"output\": str(item[\"annotations\"])\n",
    "        }\n",
    "        formatted_data.append(formatted_item)\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(formatted_data, f, indent=4)\n",
    "\n",
    "def reformat_json_binary_v01(label, input_file):\n",
    "    \n",
    "    output_file_train = f\"../data/labeled_data/lora_binary_train_{label.split('/')[0]}_v01.json\"\n",
    "    output_file_valid = f\"../data/labeled_data/lora_binary_valid_{label.split('/')[0]}_v01.json\"\n",
    "    \n",
    "    # Load both train and validation data\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    positive_samples = []\n",
    "    negative_samples = []\n",
    "    \n",
    "    # Combine train and validation data for processing\n",
    "    combined_data = data[\"train\"] + data[\"valid\"]\n",
    "    \n",
    "    for item in combined_data:\n",
    "        formatted_item = {\n",
    "            \"instruction\": f\"Classify the input based on if it's about {label}. Use 1 or 0 as output.\",\n",
    "            \"input\": normalizeTweet(item[\"text\"]),\n",
    "            \"output\": str(int(label in item[\"annotations\"]))\n",
    "        }\n",
    "        if label in item[\"annotations\"]:\n",
    "            positive_samples.append(formatted_item)\n",
    "        else:\n",
    "            negative_samples.append(formatted_item)\n",
    "\n",
    "    # Balance the positive and negative samples\n",
    "    if len(negative_samples) > len(positive_samples):\n",
    "        negative_samples = random.sample(negative_samples, len(positive_samples), random_state=42)\n",
    "\n",
    "    # Merge positive and negative samples\n",
    "    formatted_data = positive_samples + negative_samples\n",
    "\n",
    "    print(\"---------------------\")\n",
    "    print(\"Label:\", label)\n",
    "    print(\"Positive samples:\", len(positive_samples))\n",
    "    print(\"Negative samples:\", len(negative_samples))\n",
    "    print(\"Total samples:\", len(formatted_data))\n",
    "\n",
    "    # Split the data into train and validation sets (80/20 ratio)\n",
    "    train_data, valid_data = train_test_split(formatted_data, test_size=0.2, random_state=42)\n",
    "\n",
    "    print(\"Train samples:\", len(train_data))\n",
    "    print(\"Valid samples:\", len(valid_data))\n",
    "    print(\"---------------------\")\n",
    "\n",
    "    # Save the train and validation data to their respective files\n",
    "    with open(output_file_train, 'w') as f:\n",
    "        json.dump(train_data, f, indent=4)\n",
    "    \n",
    "    with open(output_file_valid, 'w') as f:\n",
    "        json.dump(valid_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "Label: War/Terror\n",
      "Positive samples: 939\n",
      "Negative samples: 939\n",
      "Total samples: 1878\n",
      "Train samples: 1502\n",
      "Valid samples: 376\n",
      "---------------------\n",
      "---------------------\n",
      "Label: Conspiracy Theory\n",
      "Positive samples: 254\n",
      "Negative samples: 254\n",
      "Total samples: 508\n",
      "Train samples: 406\n",
      "Valid samples: 102\n",
      "---------------------\n",
      "---------------------\n",
      "Label: Education\n",
      "Positive samples: 63\n",
      "Negative samples: 63\n",
      "Total samples: 126\n",
      "Train samples: 100\n",
      "Valid samples: 26\n",
      "---------------------\n",
      "---------------------\n",
      "Label: Election Campaign\n",
      "Positive samples: 133\n",
      "Negative samples: 133\n",
      "Total samples: 266\n",
      "Train samples: 212\n",
      "Valid samples: 54\n",
      "---------------------\n",
      "---------------------\n",
      "Label: Environment\n",
      "Positive samples: 58\n",
      "Negative samples: 58\n",
      "Total samples: 116\n",
      "Train samples: 92\n",
      "Valid samples: 24\n",
      "---------------------\n",
      "---------------------\n",
      "Label: Government/Public\n",
      "Positive samples: 1248\n",
      "Negative samples: 1248\n",
      "Total samples: 2496\n",
      "Train samples: 1996\n",
      "Valid samples: 500\n",
      "---------------------\n",
      "---------------------\n",
      "Label: Health\n",
      "Positive samples: 214\n",
      "Negative samples: 214\n",
      "Total samples: 428\n",
      "Train samples: 342\n",
      "Valid samples: 86\n",
      "---------------------\n",
      "---------------------\n",
      "Label: Immigration/Integration\n",
      "Positive samples: 201\n",
      "Negative samples: 201\n",
      "Total samples: 402\n",
      "Train samples: 321\n",
      "Valid samples: 81\n",
      "---------------------\n",
      "---------------------\n",
      "Label: Justice/Crime\n",
      "Positive samples: 572\n",
      "Negative samples: 572\n",
      "Total samples: 1144\n",
      "Train samples: 915\n",
      "Valid samples: 229\n",
      "---------------------\n",
      "---------------------\n",
      "Label: Labor/Employment\n",
      "Positive samples: 97\n",
      "Negative samples: 97\n",
      "Total samples: 194\n",
      "Train samples: 155\n",
      "Valid samples: 39\n",
      "---------------------\n",
      "---------------------\n",
      "Label: Macroeconomics/Economic Regulation\n",
      "Positive samples: 250\n",
      "Negative samples: 250\n",
      "Total samples: 500\n",
      "Train samples: 400\n",
      "Valid samples: 100\n",
      "---------------------\n",
      "---------------------\n",
      "Label: Media/Journalism\n",
      "Positive samples: 184\n",
      "Negative samples: 184\n",
      "Total samples: 368\n",
      "Train samples: 294\n",
      "Valid samples: 74\n",
      "---------------------\n",
      "---------------------\n",
      "Label: Religion\n",
      "Positive samples: 71\n",
      "Negative samples: 71\n",
      "Total samples: 142\n",
      "Train samples: 113\n",
      "Valid samples: 29\n",
      "---------------------\n",
      "---------------------\n",
      "Label: Science/Technology\n",
      "Positive samples: 54\n",
      "Negative samples: 54\n",
      "Total samples: 108\n",
      "Train samples: 86\n",
      "Valid samples: 22\n",
      "---------------------\n",
      "---------------------\n",
      "Label: Others\n",
      "Positive samples: 1057\n",
      "Negative samples: 1057\n",
      "Total samples: 2114\n",
      "Train samples: 1691\n",
      "Valid samples: 423\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "# Example usage binary:\n",
    "for class_ in prompt_utils.ALL_LABELS: \n",
    "    label = class_\n",
    "    input_file = \"../data/labeled_data/generic_test_0.json\"\n",
    "\n",
    "    reformat_json_binary_v01(label, input_file)\n",
    "    #reformat_json_binary_v01(label, input_file, dataset_type=\"test\")\n",
    "    #reformat_json_binary_v01(label, input_file, dataset_type=\"valid\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the amount of 1 or 0s in the binary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 852, '0': 852}\n",
      "{'1': 271, '0': 271}\n",
      "{'1': 205, '0': 205}\n"
     ]
    }
   ],
   "source": [
    "def count_labels(file):\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    label_count = {'1': 0, '0': 0}\n",
    "    for item in data:\n",
    "        label_count[item[\"output\"]] += 1\n",
    "\n",
    "    return label_count\n",
    "\n",
    "# Check the distribution of labels in the output files\n",
    "print(count_labels(output_file_train))\n",
    "print(count_labels(output_file_test))\n",
    "print(count_labels(output_file_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
