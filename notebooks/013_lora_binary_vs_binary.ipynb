{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../src\")\n",
    "import llm_utils\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import ast\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "classes = [\"War/Terror\", \"Conspiracy Theory\", \"Education\", \"Election Campaign\", \"Environment\", \n",
    "              \"Government/Public\", \"Health\", \"Immigration/Integration\", \n",
    "              \"Justice/Crime\", \"Labor/Employment\", \n",
    "              \"Macroeconomics/Economic Regulation\", \"Media/Journalism\", \"Religion\", \"Science/Technology\"]\n",
    "\n",
    "\n",
    "def calculate_binary_metrics_lora(df, class_full, extraction_function):\n",
    "    prediction_per_class = None\n",
    "    # Iterate through class labels and extract binary predictions\n",
    "    pred_column_name = f\"response\"\n",
    "    pred_column_df = df[df[pred_column_name].notna()].copy()\n",
    "    pred_column_df[pred_column_name] = pred_column_df[pred_column_name].apply(extraction_function)\n",
    "    prediction_per_class = pred_column_df\n",
    "\n",
    "    confusion_matrices = {}\n",
    "    classification_reports = {}\n",
    "    pred_column_name = f\"response\"\n",
    "\n",
    "    current_df = prediction_per_class\n",
    "    \n",
    "    # Ignore rows with NaN or invalid values in the predictions\n",
    "    try:\n",
    "        valid_rows = current_df[pred_column_name].notna()\n",
    "        \n",
    "        y_true = current_df.loc[valid_rows, 'annotations'].apply(lambda x: int(class_full in x))\n",
    "        y_pred = current_df.loc[valid_rows, pred_column_name].astype(int)\n",
    "    except KeyError:\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "    except TypeError:\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    confusion_matrices[class_full] = cm\n",
    "    cr = classification_report(y_true, y_pred, output_dict=True)\n",
    "    classification_reports[class_full] = cr\n",
    "    return prediction_per_class, confusion_matrices, classification_reports\n",
    "\n",
    "def load_model(path, class_full):\n",
    "    binary_df = pd.read_csv(path)\n",
    "    extraction_function = llm_utils.get_extraction_function(\"extract_using_class_token\", 1)\n",
    "    _, confusion_matrices, classification_reports = calculate_binary_metrics_lora(binary_df, class_full, extraction_function)\n",
    "    return {\"confusion_matrices\": confusion_matrices, \"classification_reports\": classification_reports}\n",
    "\n",
    "binary_war_lora = load_model(\"../data/vicuna_4bit/lora/binary_war_v01/test_generic_test_0.csv\", \"War/Terror\")\n",
    "binary_conspiracy_theory_lora = load_model(\"../data/vicuna_4bit/lora/binary_conspiracy_theory_v01/test_generic_test_0.csv\", \"Conspiracy Theory\")\n",
    "binary_education_lora = load_model(\"../data/vicuna_4bit/lora/binary_education_v01/test_generic_test_0.csv\", \"Education\")\n",
    "binary_election_campaign_lora = load_model(\"../data/vicuna_4bit/lora/binary_election_campaign_v01/test_generic_test_0.csv\", \"Election Campaign\")\n",
    "binary_environment_lora = load_model(\"../data/vicuna_4bit/lora/binary_environment_v01/test_generic_test_0.csv\", \"Environment\")\n",
    "binary_government_public_lora = load_model(\"../data/vicuna_4bit/lora/binary_government_public_v01/test_generic_test_0.csv\", \"Government/Public\")\n",
    "binary_health_lora = load_model(\"../data/vicuna_4bit/lora/binary_health_v01/test_generic_test_0.csv\", \"Health\")\n",
    "binary_immigration_integration_lora = load_model(\"../data/vicuna_4bit/lora/binary_immigration_integration_v01/test_generic_test_0.csv\", \"Immigration/Integration\")\n",
    "binary_justice_crime_lora = load_model(\"../data/vicuna_4bit/lora/binary_justice_crime_v01/test_generic_test_0.csv\", \"Justice/Crime\")\n",
    "binary_labor_employment_lora = load_model(\"../data/vicuna_4bit/lora/binary_labor_employment_v01/test_generic_test_0.csv\", \"Labor/Employment\")\n",
    "binary_macroeconomics_economic_regulation_lora = load_model(\"../data/vicuna_4bit/lora/binary_macroeconomics_economic_regulation_v01/test_generic_test_0.csv\", \"Macroeconomics/Economic Regulation\")\n",
    "binary_media_journalism_lora = load_model(\"../data/vicuna_4bit/lora/binary_media_journalism_v01/test_generic_test_0.csv\", \"Media/Journalism\")\n",
    "binary_religion_lora = load_model(\"../data/vicuna_4bit/lora/binary_religion_v01/test_generic_test_0.csv\", \"Religion\")\n",
    "binary_science_technology_lora = load_model(\"../data/vicuna_4bit/lora/binary_science_technology_v01/test_generic_test_0.csv\", \"Science/Technology\")\n",
    "binary_others_lora = load_model(\"../data/vicuna_4bit/lora/binary_others_v01/test_generic_test_0.csv\", \"Others\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [binary_war_lora, binary_conspiracy_theory_lora, binary_education_lora, binary_election_campaign_lora, binary_environment_lora, \n",
    "             binary_government_public_lora, binary_health_lora, binary_immigration_integration_lora, binary_justice_crime_lora, binary_labor_employment_lora, \n",
    "             binary_macroeconomics_economic_regulation_lora, binary_media_journalism_lora, binary_religion_lora, binary_science_technology_lora, binary_others_lora]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm_utils\u001b[39m.\u001b[39mclassification_reports_to_df(data_list)\n",
      "File \u001b[0;32m/media/bruno/0d2f61d2-2b9c-4043-9a46-8e4dfe74fc95/bruno/Documents/ip6-twitter-disinformation/notebooks/../src/llm_utils.py:97\u001b[0m, in \u001b[0;36mclassification_reports_to_df\u001b[0;34m(classification_reports, binary)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m     92\u001b[0m     \u001b[39m# Your code for creating the DataFrame and adding the results\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mf1_score_macro\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mprecision_macro\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrecall_macro\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msupport_macro\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     94\u001b[0m                                     \u001b[39m'\u001b[39m\u001b[39mf1_score_class_0\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39msupport_class_0\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     95\u001b[0m                                     \u001b[39m'\u001b[39m\u001b[39mf1_score_class_1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msupport_class_1\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 97\u001b[0m     \u001b[39mfor\u001b[39;00m label, cr \u001b[39min\u001b[39;00m classification_reports\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     98\u001b[0m         \u001b[39mtry\u001b[39;00m: \n\u001b[1;32m     99\u001b[0m             df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mappend({\n\u001b[1;32m    100\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m: label,\n\u001b[1;32m    101\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mf1_score_macro\u001b[39m\u001b[39m'\u001b[39m: cr[\u001b[39m'\u001b[39m\u001b[39mmacro avg\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mf1-score\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39msupport_class_1\u001b[39m\u001b[39m'\u001b[39m: cr[\u001b[39m'\u001b[39m\u001b[39m1\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39msupport\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    113\u001b[0m             }, ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "llm_utils.classification_reports_to_df(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 label  f1_score_macro  precision_macro  recall_macro  support_macro  f1_score_class_0  support_class_0  f1_score_class_1  support_class_1  precision_class_0  recall_class_0  precision_class_1  recall_class_1\n",
      "0                           War/Terror        0.934538         0.932357      0.936781           1000          0.966353              745          0.902724              255           0.968961        0.963758           0.895753        0.909804\n",
      "1                    Conspiracy Theory        0.571270         0.572070      0.797906           1000          0.881671              955          0.260870               45           0.988296        0.795812           0.155844        0.800000\n",
      "2                            Education        0.455351         0.512493      0.678953            949          0.849633              936          0.061069               13           0.992857        0.742521           0.032129        0.615385\n",
      "3                    Election Campaign        0.397663         0.527432      0.713651            994          0.681259              961          0.114068               33           0.994012        0.518210           0.060852        0.909091\n",
      "4                          Environment        0.393621         0.495296      0.424268            960          0.770713              946          0.016529               14           0.981997        0.634249           0.008596        0.214286\n",
      "5                    Government/Public        0.732328         0.755895      0.718984           1000          0.860811              709          0.603846              291           0.826200        0.898449           0.685590        0.539519\n",
      "6                               Health        0.601433         0.580434      0.721762            991          0.924791              945          0.278075               46           0.976471        0.878307           0.184397        0.565217\n",
      "7              Immigration/Integration        0.528341         0.546002      0.738301           1000          0.872180              964          0.184502               36           0.985621        0.782158           0.106383        0.694444\n",
      "8                        Justice/Crime        0.871271         0.846185      0.903395            999          0.961085              862          0.781457              137           0.977218        0.945476           0.715152        0.861314\n",
      "9                     Labor/Employment        0.458785         0.528382      0.741088            986          0.798005              959          0.119565               27           0.992248        0.667362           0.064516        0.814815\n",
      "10  Macroeconomics/Economic Regulation        0.656206         0.625745      0.841980            999          0.910868              937          0.401544               62           0.987531        0.845251           0.263959        0.838710\n",
      "11                    Media/Journalism        0.518971         0.517377      0.522115            999          0.947034              951          0.090909               48           0.954109        0.940063           0.080645        0.104167\n",
      "12                            Religion        0.494641         0.494111      0.495172            943          0.989282              932          0.000000               11           0.988223        0.990343           0.000000        0.000000\n",
      "13                  Science/Technology        0.415833         0.503303      0.563118            940          0.800513              929          0.031153               11           0.990476        0.671690           0.016129        0.454545\n",
      "14                              Others        0.805024         0.792110      0.855157           1000          0.868976              729          0.741071              271           0.963272        0.791495           0.620948        0.918819\n"
     ]
    }
   ],
   "source": [
    "# Extracting metrics and constructing DataFrame\n",
    "rows = []\n",
    "for data in data_list:\n",
    "    for label, report in data['classification_reports'].items():\n",
    "        row = {\n",
    "            'label': label,\n",
    "            'f1_score_macro': report['macro avg']['f1-score'],\n",
    "            'precision_macro': report['macro avg']['precision'],\n",
    "            'recall_macro': report['macro avg']['recall'],\n",
    "            'support_macro': report['macro avg']['support'],\n",
    "            'f1_score_class_0': report['0']['f1-score'],\n",
    "            'support_class_0': report['0']['support'],\n",
    "            'f1_score_class_1': report['1']['f1-score'],\n",
    "            'support_class_1': report['1']['support'],\n",
    "            'precision_class_0': report['0']['precision'],\n",
    "            'recall_class_0': report['0']['recall'],\n",
    "            'precision_class_1': report['1']['precision'],\n",
    "            'recall_class_1': report['1']['recall']\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3035672108984692\n"
     ]
    }
   ],
   "source": [
    "# Calculate TP, FP, and FN for class 1 for each label\n",
    "df['TP_class_1'] = df['precision_class_1'] * df['recall_class_1'] * df['support_class_1'] / (df['precision_class_1'] + df['recall_class_1'])\n",
    "df['FP_class_1'] = df['support_class_1'] - df['TP_class_1']\n",
    "df['FN_class_1'] = df['support_class_1'] - df['TP_class_1']\n",
    "\n",
    "# Calculate micro-average precision and recall for class 1\n",
    "precision_micro = df['TP_class_1'].sum() / (df['TP_class_1'].sum() + df['FP_class_1'].sum())\n",
    "recall_micro = df['TP_class_1'].sum() / (df['TP_class_1'].sum() + df['FN_class_1'].sum())\n",
    "\n",
    "# Calculate micro-average F1 score for class 1\n",
    "f1_micro = 2 * precision_micro * recall_micro / (precision_micro + recall_micro)\n",
    "\n",
    "print(f1_micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3058254649872635"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"f1_score_class_1\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_war_df = pd.read_csv(\"../data/vicuna_4bit/generic_prompt_without_context_only_classification/generic_test_0.csv\")\n",
    "extraction_function = llm_utils.get_extraction_function(\"extract_using_class_token\", 1)\n",
    "binary_war_predictions_per_class, confusion_matrices, classification_reports = llm_utils.calculate_binary_metrics(binary_war_df, [\"War/Terror\"], extraction_function)\n",
    "binary_war = {\"confusion_matrices\": confusion_matrices, \"classification_reports\": classification_reports}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8 0.15584415584415584 0.16359262229350438\n",
      "0.6153846153846154 0.0321285140562249 0.03402551913935451\n",
      "0.21428571428571427 0.008595988538681949 0.009110396570203644\n",
      "0.8148148148148148 0.06451612903225806 0.06821083348531826\n",
      "0.0 0.0 0.0\n",
      "0.45454545454545453 0.016129032258064516 0.017099175216254273\n",
      "0.292038546704635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.04867309111743917"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import prompt_utils\n",
    "def calculate_fbeta_score(beta, precision, recall):\n",
    "    if precision == 0 and recall == 0:\n",
    "        return 0.0\n",
    "    return (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n",
    "\n",
    "avg_fbeta_score_class_1_low_0_25 = 0\n",
    "try:\n",
    "    for class_ in prompt_utils.LOW_F1_LABELS:\n",
    "        avg_precision_class_1_low = df[df.label == class_][\"precision_class_1\"]\n",
    "        avg_recall_class_1_low = df[df.label == class_][\"recall_class_1\"]\n",
    "        avg_precision_class_1_low = avg_precision_class_1_low.values[0]\n",
    "        #print(avg_precision_class_1_low)\n",
    "        avg_recall_class_1_low = avg_recall_class_1_low.values[0]\n",
    "        #print(avg_recall_class_1_low)\n",
    "        avg_fbeta_score_class_1_low_0_25 = avg_fbeta_score_class_1_low_0_25 + calculate_fbeta_score(0.25, avg_precision_class_1_low, avg_recall_class_1_low)\n",
    "        print(avg_recall_class_1_low, avg_precision_class_1_low, calculate_fbeta_score(0.25, avg_precision_class_1_low, avg_recall_class_1_low))\n",
    "    print(avg_fbeta_score_class_1_low_0_25)\n",
    "except Exception as e:\n",
    "    print(\"Error\", e)\n",
    "avg_fbeta_score_class_1_low_0_25 = avg_fbeta_score_class_1_low_0_25/len(prompt_utils.LOW_F1_LABELS)\n",
    "avg_fbeta_score_class_1_low_0_25"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
