{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python pip install accelerate appdirs loralib black black[jupyter] datasets git+https://github.com/huggingface/peft.git git+https://github.com/huggingface/transformers.git sentencepiece gradio fire torch datasets bitsandbytes utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fire'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m List\n\u001b[0;32m----> 9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfire\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mimport\u001b[39;00m functional \u001b[39mas\u001b[39;00m F\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fire'"
     ]
    }
   ],
   "source": [
    "#!conda install cudatoolkit\n",
    "#!pip install git+https://github.com/huggingface/peft.git\n",
    "#!pip install Sentencepiece\n",
    "#!pip install git+https://github.com/huggingface/transformers.git # https://stackoverflow.com/questions/65854722/huggingface-albert-tokenizer-nonetype-error-with-colab\n",
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import fire\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, GenerationConfig\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, multilabel_confusion_matrix\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.device_count() > 0 else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649513887bf444c580a65458c9b1eccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#Causal Language Model: only consider words to the left\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    #\"tloen/alpaca-lora-7b\",\n",
    "    \"decapoda-research/llama-7b-hf\",\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
    "print(tokenizer.pad_token_id)  # unk. we want this to be different from the eos token\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.unk_token_id)\n",
    "tokenizer.pad_token_id = (1)\n",
    "tokenizer.padding_side = \"left\"  # Allow batched inference\n",
    "\n",
    "\n",
    "model = prepare_model_for_int8_training(model)\n",
    "#model = get_peft_model(model, config) #https://github.com/huggingface/peft State-of-the-art Parameter-Efficient Fine-Tuning (PEFT) methods\n",
    "\n",
    "resume_from_checkpoint = False\n",
    "\n",
    "if resume_from_checkpoint:\n",
    "    # Check the available weights and load them\n",
    "    checkpoint_name = os.path.join(\n",
    "        resume_from_checkpoint, \"pytorch_model.bin\"\n",
    "    )  # Full checkpoint\n",
    "    if not os.path.exists(checkpoint_name):\n",
    "        checkpoint_name = os.path.join(\n",
    "            resume_from_checkpoint, \"adapter_model.bin\"\n",
    "        )  # only LoRA model - LoRA config above has to fit\n",
    "        resume_from_checkpoint = (\n",
    "            False  # So the trainer won't try loading its state\n",
    "        )\n",
    "    # The two files above have a different name depending on how they were saved, but are actually the same.\n",
    "    if os.path.exists(checkpoint_name):\n",
    "        print(f\"Restarting from {checkpoint_name}\")\n",
    "        adapters_weights = torch.load(checkpoint_name)\n",
    "        model = set_peft_model_state_dict(model, adapters_weights)\n",
    "    else:\n",
    "        print(f\"Checkpoint {checkpoint_name} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training of 1. fold...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from emoji import demojize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from typing import List, Dict\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "class MultiLabelDataCollator(DataCollatorWithPadding):\n",
    "    def __init__(self, tokenizer):\n",
    "        super().__init__(tokenizer)\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, torch.Tensor]]):\n",
    "        batch = super().__call__(features)\n",
    "        batch[\"labels\"] = torch.stack([feature[\"label\"] for feature in features])\n",
    "        return batch\n",
    "        \n",
    "    @staticmethod\n",
    "    def loss(logits, labels):\n",
    "        # Use BCEWithLogitsLoss for multi-label classification\n",
    "        loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "        return loss_fct(logits, labels.float())\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    print(eval_pred)\n",
    "    predictions, labels = eval_pred\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    predictions = (probs >= 0.5).to(int)\n",
    "    labels = labels.astype(int)\n",
    "    report = classification_report(labels, predictions, labels=range(len(classes)), output_dict=True, zero_division=0)\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": np.mean(predictions == labels),\n",
    "        \"micro_precision\": report[\"micro avg\"][\"precision\"],\n",
    "        \"micro_recall\": report[\"micro avg\"][\"recall\"],\n",
    "        \"micro_f1\": report[\"micro avg\"][\"f1-score\"],\n",
    "        \"macro_precision\": report[\"macro avg\"][\"precision\"],\n",
    "        \"macro_recall\": report[\"macro avg\"][\"recall\"],\n",
    "        \"macro_f1\": report[\"macro avg\"][\"f1-score\"],\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, x, y, mlb, tokenizer, train = True):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.mlb = mlb\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoded_tweets = self.preprocess_text(self.x)\n",
    "        self.train = train\n",
    "        \n",
    "    @staticmethod\n",
    "    def normalizeToken(token):\n",
    "        lowercased_token = token.lower()\n",
    "        if token.startswith(\"@\"):\n",
    "            return \"@USER\"\n",
    "        elif lowercased_token.startswith(\"http\") or lowercased_token.startswith(\"www\"):\n",
    "            return \"HTTPURL\"\n",
    "        elif len(token) == 1:\n",
    "            return demojize(token)\n",
    "        else:\n",
    "            if token == \"’\":\n",
    "                return \"'\"\n",
    "            elif token == \"…\":\n",
    "                return \"...\"\n",
    "            else:\n",
    "                return token\n",
    "    \n",
    "    def normalizeTweet(self, tweet):\n",
    "        tokens = TweetTokenizer().tokenize(tweet.replace(\"’\", \"'\").replace(\"…\", \"...\"))\n",
    "        normTweet = \" \".join([self.normalizeToken(token) for token in tokens])\n",
    "\n",
    "        normTweet = (\n",
    "            normTweet.replace(\"cannot \", \"can not \")\n",
    "                .replace(\"n't \", \" n't \")\n",
    "                .replace(\"n 't \", \" n't \")\n",
    "                .replace(\"ca n't\", \"can't\")\n",
    "                .replace(\"ai n't\", \"ain't\")\n",
    "        )\n",
    "        normTweet = (\n",
    "            normTweet.replace(\"'m \", \" 'm \")\n",
    "                .replace(\"'re \", \" 're \")\n",
    "                .replace(\"'s \", \" 's \")\n",
    "                .replace(\"'ll \", \" 'll \")\n",
    "                .replace(\"'d \", \" 'd \")\n",
    "                .replace(\"'ve \", \" 've \")\n",
    "        )\n",
    "        normTweet = (\n",
    "            normTweet.replace(\" p . m .\", \"  p.m.\")\n",
    "                .replace(\" p . m \", \" p.m \")\n",
    "                .replace(\" a . m .\", \" a.m.\")\n",
    "                .replace(\" a . m \", \" a.m \")\n",
    "        )\n",
    "        return \" \".join(normTweet.split())\n",
    "\n",
    "    def generate_prompt(self, data_point):\n",
    "        #if self.train:\n",
    "        rules = \"War/Terror = 1, Non-War/Terror = 0\"\n",
    "        examples = \"\" #\"This tweet is about War. This tweet is about Terror.\"\n",
    "        instructions = \"Please label the following tweet as War/Terror or Non-War/Terror with the rules provided above and only provide the labels:\" \n",
    "\n",
    "        full_prompt = rules + \"\\n\" + examples + \"\\n\" + instructions + data_point\n",
    "\n",
    "        return full_prompt\n",
    "    \n",
    "    def tokenize(self, prompt, add_eos_token=True):\n",
    "        # there's probably a way to do this with the tokenizer settings\n",
    "        # but again, gotta move fast\n",
    "        cutoff_len = 100000\n",
    "        result = tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \"\"\"print(result)\n",
    "        print(result[\"input_ids\"][-1] != tokenizer.eos_token_id)\n",
    "        if (\n",
    "            result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "            and len(result[\"input_ids\"]) < cutoff_len\n",
    "            and add_eos_token\n",
    "        ):\n",
    "            result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "            result[\"attention_mask\"].append(1)\"\"\"\n",
    "\n",
    "        #result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "        return result\n",
    "\n",
    "    def preprocess_text(self, X):\n",
    "        X = [self.normalizeTweet(tweet) for tweet in X] #normalize\n",
    "        X = [self.generate_prompt(tweet) for tweet in X] #generate prompt\n",
    "        X = [self.tokenize(tweet) for tweet in X] #tokenize\n",
    "        return X\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.y[idx]\n",
    "        return {'input_ids': self.encoded_tweets[idx]['input_ids'],\n",
    "                'attention_mask': self.encoded_tweets[idx]['attention_mask'],\n",
    "                'label': torch.tensor(label, dtype=torch.float32)}\n",
    "                #'label_ids': self.labels[idx]}\n",
    "\n",
    "#model.print_trainable_parameters() \n",
    "\n",
    "i = 0\n",
    "train_size = \"full\"\n",
    "epochs = 200\n",
    "task = \"generic\"\n",
    "print(f\"Starting training of {i+1}. fold...\")\n",
    "output_dir = f\"./{task}_epochs_{epochs}_train_size_{train_size}_fold_{i}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load the data for this fold\n",
    "filename = f\"../data/labeled_data/{task}_test_{i}.json\"\n",
    "with open(filename) as f:\n",
    "    data = json.load(f)\n",
    "train_df = pd.DataFrame(data[\"train\"])\n",
    "val_df = pd.DataFrame(data[\"valid\"])\n",
    "test_df = pd.DataFrame(data[\"test\"])\n",
    "train_annotations = train_df[\"annotations\"].tolist()\n",
    "\n",
    "# Get all unique classes\n",
    "global classes\n",
    "classes = set()\n",
    "for annotation in train_annotations:\n",
    "    classes.update(annotation)\n",
    "classes = sorted(list(classes))\n",
    "\n",
    "# Convert the annotations to binary labels\n",
    "mlb = MultiLabelBinarizer(classes=classes)\n",
    "\n",
    "\"\"\"# train_size argument is used to control the size of the training set \n",
    "if train_size != \"full\":\n",
    "    train_df = train_df.sample(n=train_size)\n",
    "if validation_size != \"full\":\n",
    "    val_df = val_df.sample(n=validation_size)\n",
    "if test_size != \"full\":\n",
    "    test_df = test_df.sample(n=test_size)\"\"\"\n",
    "\n",
    "train_labels = mlb.fit_transform(train_df[\"annotations\"])\n",
    "val_labels = mlb.transform(val_df[\"annotations\"])\n",
    "test_labels = mlb.transform(test_df[\"annotations\"])\n",
    "\n",
    "train_dataset = TweetDataset(train_df['text'].to_list(), torch.tensor(train_labels), mlb, tokenizer)\n",
    "val_dataset = TweetDataset(val_df['text'].to_list(), torch.tensor(val_labels), mlb, tokenizer)\n",
    "test_dataset = TweetDataset(test_df['text'].to_list(), torch.tensor(test_labels), mlb, tokenizer)\n",
    "data_collator = MultiLabelDataCollator(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7560/3287045752.py:142: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    0,  3362, 29914, 29911,  2704,   353, 29871, 29896, 29892, 10050,\n",
       "         29899, 29956,   279, 29914, 29911,  2704,   353, 29871, 29900,    13,\n",
       "            13, 12148,  3858,   278,  1494,  7780,   300,   408,  3362, 29914,\n",
       "         29911,  2704,   470, 10050, 29899, 29956,   279, 29914, 29911,  2704,\n",
       "           411,   278,  6865,  4944,  2038,   322,   871,  3867,   278, 11073,\n",
       "         29901,  5328,  2834,  1122,  1284,   263,   982,   373, 12178,   595,\n",
       "           525, 29879, 18786,  7331,  4219,  7331,  4219]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7560/3287045752.py:142: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "/home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Input length of input_ids is 67, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "/home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages/transformers/generation/utils.py:1405: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Generate\n",
    "generate_ids = model.generate(\n",
    "    train_dataset[0][\"input_ids\"],\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    max_length=50,\n",
    "    num_return_sequences=1\n",
    ")\n",
    "t = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,  2799,  4080, 29901,  2391,   599, 11443, 28058,   297, 22968,\n",
       "           936,  1797, 29889]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m base_model \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdecapoda-research/llama-7b-hf\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m tokenizer \u001b[39m=\u001b[39m LlamaTokenizer\u001b[39m.\u001b[39mfrom_pretrained(base_model)\n\u001b[0;32m----> 8\u001b[0m model \u001b[39m=\u001b[39m LlamaForCausalLM\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      9\u001b[0m     base_model,\n\u001b[1;32m     10\u001b[0m     load_in_8bit\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m     torch_dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat16,\n\u001b[1;32m     12\u001b[0m     device_map\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m model \u001b[39m=\u001b[39m PeftModel\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     15\u001b[0m     model,\n\u001b[1;32m     16\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtloen/alpaca-lora-7b\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     torch_dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat16,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mpad_token_id \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m  \u001b[39m# unk\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/my_env/lib/python3.11/site-packages/transformers/modeling_utils.py:2740\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2736\u001b[0m         device_map_without_lm_head \u001b[39m=\u001b[39m {\n\u001b[1;32m   2737\u001b[0m             key: device_map[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m device_map\u001b[39m.\u001b[39mkeys() \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m modules_to_not_convert\n\u001b[1;32m   2738\u001b[0m         }\n\u001b[1;32m   2739\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m device_map_without_lm_head\u001b[39m.\u001b[39mvalues() \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdisk\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m device_map_without_lm_head\u001b[39m.\u001b[39mvalues():\n\u001b[0;32m-> 2740\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2741\u001b[0m                 \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2742\u001b[0m \u001b[39m                Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\u001b[39;00m\n\u001b[1;32m   2743\u001b[0m \u001b[39m                the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\u001b[39;00m\n\u001b[1;32m   2744\u001b[0m \u001b[39m                these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\u001b[39;00m\n\u001b[1;32m   2745\u001b[0m \u001b[39m                `device_map` to `from_pretrained`. Check\u001b[39;00m\n\u001b[1;32m   2746\u001b[0m \u001b[39m                https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\u001b[39;00m\n\u001b[1;32m   2747\u001b[0m \u001b[39m                for more details.\u001b[39;00m\n\u001b[1;32m   2748\u001b[0m \u001b[39m                \"\"\"\u001b[39;00m\n\u001b[1;32m   2749\u001b[0m             )\n\u001b[1;32m   2750\u001b[0m         \u001b[39mdel\u001b[39;00m device_map_without_lm_head\n\u001b[1;32m   2752\u001b[0m \u001b[39mif\u001b[39;00m from_tf:\n",
      "\u001b[0;31mValueError\u001b[0m: \n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        "
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, GenerationConfig\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.device_count() > 0 else \"cpu\")\n",
    "\n",
    "base_model = \"decapoda-research/llama-7b-hf\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    \"tloen/alpaca-lora-7b\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "model.config.bos_token_id = 1\n",
    "model.config.eos_token_id = 2\n",
    "\n",
    "prompt = \"Instruction: Hello, are you alive?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=40,\n",
    "    num_beams=4\n",
    ")\n",
    "\n",
    "generate_params = {\n",
    "    \"input_ids\": input_ids,\n",
    "    \"generation_config\": generation_config,\n",
    "    \"return_dict_in_generate\": True,\n",
    "    \"output_scores\": True,\n",
    "    \"max_new_tokens\": 128,\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=128,\n",
    "    )\n",
    "s = generation_output.sequences[0]\n",
    "output = tokenizer.decode(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"How life may find a way on Saturn's moon https://t.co/aCambG1yAm https://t.co/1jsT6ItnVh\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['text'].to_list()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"Is the Tweet '{train_df['text'].to_list()[0]}' about War/Terror? Answer with 0 for no and 1 for yes.\"\n",
    "inputs = tokenizer(\"the nutrition facts of a peanut are\", return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=40,\n",
    "    num_beams=4\n",
    ")\n",
    "\n",
    "generate_params = {\n",
    "    \"input_ids\": input_ids,\n",
    "    \"generation_config\": generation_config,\n",
    "    \"return_dict_in_generate\": True,\n",
    "    \"output_scores\": True,\n",
    "    \"max_new_tokens\": 128,\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=128,\n",
    "    )\n",
    "s = generation_output.sequences[0]\n",
    "output = tokenizer.decode(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>the nutrition facts of a peanut are the nutrition facts of a peanut are the nutration facts of a peanut are the nutration facts of a peanut are the nutration facts of a peanut are the nutration facts of a peanut are the nutration facts of a peanut are the nutration facts of a peanut are the nutration facts of a peanut are the nutration facts of a peanut are the nutration facts of a peanut are the nutration facts of a peanut are the nutration facts of a'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(generate_ids[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
