{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vicuna 4 bit quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m without_context_classification_only_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39m../data/vicuna_4bit/generic_prompt_without_context_only_classification/generic_test_0.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m extraction_function \u001b[39m=\u001b[39m llm_utils\u001b[39m.\u001b[39mget_extraction_function(\u001b[39m\"\u001b[39m\u001b[39mextract_first_character\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m prediction_per_class, confusion_matrices, classification_reports \u001b[39m=\u001b[39m llm_utils\u001b[39m.\u001b[39mcalculate_binary_metrics(without_context_classification_only_df, classes, extraction_function)\n\u001b[1;32m     23\u001b[0m without_context_classification_only \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mconfusion_matrices\u001b[39m\u001b[39m\"\u001b[39m: confusion_matrices, \u001b[39m\"\u001b[39m\u001b[39mclassification_reports\u001b[39m\u001b[39m\"\u001b[39m: classification_reports}\n\u001b[1;32m     25\u001b[0m \u001b[39m# without context and elaboration first\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39m## Example:\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[39m### Human: Elaborate on whether you think the Tweet is about {label} or something else.\\nTweet: {tweet_text}\\n### Assistant:\\nElaboration: \u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[39m#Followup :\\n Assign class 1 for {label} or 0 for not. \\n###Assistant:\\nClass:  \u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ip6-twitter-disinformation/notebooks/../src/llm_utils.py:125\u001b[0m, in \u001b[0;36mcalculate_binary_metrics\u001b[0;34m(df, classes, extraction_function)\u001b[0m\n\u001b[1;32m    123\u001b[0m     pred_column_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlabel\u001b[39m}\u001b[39;00m\u001b[39m_pred\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m     pred_column_df \u001b[39m=\u001b[39m df[df[pred_column_name]\u001b[39m.\u001b[39mnotna()]\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m--> 125\u001b[0m     pred_column_df[pred_column_name] \u001b[39m=\u001b[39m df[pred_column_name]\u001b[39m.\u001b[39mapply(extraction_function)\n\u001b[1;32m    126\u001b[0m     prediction_per_class\u001b[39m.\u001b[39mappend(pred_column_df)\n\u001b[1;32m    128\u001b[0m confusion_matrices \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/envs/my_env/lib/python3.11/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39mapply()\n",
      "File \u001b[0;32m~/anaconda3/envs/my_env/lib/python3.11/site-packages/pandas/core/apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m~/anaconda3/envs/my_env/lib/python3.11/site-packages/pandas/core/apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39mmap_infer(\n\u001b[1;32m   1175\u001b[0m             values,\n\u001b[1;32m   1176\u001b[0m             f,\n\u001b[1;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvert_dtype,\n\u001b[1;32m   1178\u001b[0m         )\n\u001b[1;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/envs/my_env/lib/python3.11/site-packages/pandas/_libs/lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../src\")\n",
    "import llm_utils\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "classes = [\"War/Terror\", \"Conspiracy Theory\", \"Education\", \"Election Campaign\", \"Environment\", \n",
    "              \"Government/Public\", \"Health\", \"Immigration/Integration\", \n",
    "              \"Justice/Crime\", \"Labor/Employment\", \n",
    "              \"Macroeconomics/Economic Regulation\", \"Media/Journalism\", \"Religion\", \"Science/Technology\"]\n",
    "\n",
    "# Without context and classification only\n",
    "## Example:\n",
    "    ### Human: Classify the Tweet based on if it's about {label}. Use 1 or 0 as class.\\n\\nTweet: {tweet_text}\\n### Assistant:\\nClass = \n",
    "without_context_classification_only_df = pd.read_csv(\"../data/vicuna_4bit/generic_prompt_without_context_only_classification/generic_test_0.csv\")\n",
    "extraction_function = llm_utils.get_extraction_function(\"extract_first_character\")\n",
    "prediction_per_class, confusion_matrices, classification_reports = llm_utils.calculate_binary_metrics(without_context_classification_only_df, classes, extraction_function)\n",
    "without_context_classification_only = {\"confusion_matrices\": confusion_matrices, \"classification_reports\": classification_reports}\n",
    "\n",
    "# without context and elaboration first\n",
    "## Example:\n",
    "    ### Human: Elaborate on whether you think the Tweet is about {label} or something else.\\nTweet: {tweet_text}\\n### Assistant:\\nElaboration: \n",
    "    #Followup :\\n Assign class 1 for {label} or 0 for not. \\n###Assistant:\\nClass:  \n",
    "without_context_elaboration_first_df = pd.read_csv(\"../data/vicuna_4bit/generic_prompt_without_context_elaboration_first/generic_test_0.csv\")\n",
    "extraction_function = llm_utils.get_extraction_function(\"extract_last_float\")\n",
    "prediction_per_class, confusion_matrices, classification_reports = llm_utils.calculate_binary_metrics(without_context_elaboration_first_df, classes, extraction_function)\n",
    "without_context_elaboration_first = {\"confusion_matrices\": confusion_matrices, \"classification_reports\": classification_reports}\n",
    "\n",
    "#with rules as context and classification only\n",
    "## Example:\n",
    "    ### Human: Based on rules, classify the Tweet based on if it's about {label}. Use 1 or 0 as class.\\n\\nRules: {rules}\\nTweet: {tweet_text}\\n### Assistant:\\nClass:\n",
    "with_rules_classification_only_df = pd.read_csv(\"../data/vicuna_4bit/generic_prompt_with_rules_only_classification/generic_test_0.csv\")\n",
    "extraction_function = llm_utils.get_extraction_function(\"extract_second_character\")\n",
    "prediction_per_class, confusion_matrices, classification_reports = llm_utils.calculate_binary_metrics(with_rules_classification_only_df, classes, extraction_function)\n",
    "with_rules_classification_only = {\"confusion_matrices\": confusion_matrices, \"classification_reports\": classification_reports}\n",
    "\n",
    "# with rules as context and elaboration first\n",
    "## Example:\n",
    "    ### Human: Based on rules, elaborate whether you think the Tweet is about {label}.\\nRules: {rules}\\nTweet: {tweet_text}\\n### Assistant:\\nElaboration: \n",
    "    #Followup :\\n Assign class 1 for {label} or 0 for not. \\n###Assistant:\\nClass: \n",
    "with_rules_elaboration_first_df = pd.read_csv(\"../data/vicuna_4bit/generic_prompt_with_rules_elaboration_first/generic_test_0.csv\")\n",
    "extraction_function = llm_utils.get_extraction_function(\"extract_first_character\")\n",
    "prediction_per_class, confusion_matrices, classification_reports = llm_utils.calculate_binary_metrics(with_rules_elaboration_first_df, classes, extraction_function)\n",
    "with_rules_elaboration_first = {\"confusion_matrices\": confusion_matrices, \"classification_reports\": classification_reports}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m oa_without_context_elaboration_first_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39m../data/openassistant_llama_30b_4bit/generic_prompt_without_context_elaboration_first_v02/generic_test_0.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m extraction_function \u001b[39m=\u001b[39m llm_utils\u001b[39m.\u001b[39mget_extraction_function(\u001b[39m\"\u001b[39m\u001b[39mextract_label\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m prediction_per_class, confusion_matrices, classification_reports \u001b[39m=\u001b[39m llm_utils\u001b[39m.\u001b[39mcalculate_binary_metrics(oa_without_context_elaboration_first_df, classes, extraction_function)\n\u001b[1;32m     25\u001b[0m oa_without_context_elaboration_first \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mconfusion_matrices\u001b[39m\u001b[39m\"\u001b[39m: confusion_matrices, \u001b[39m\"\u001b[39m\u001b[39mclassification_reports\u001b[39m\u001b[39m\"\u001b[39m: classification_reports}\n\u001b[1;32m     27\u001b[0m llm_utils\u001b[39m.\u001b[39mprint_confusion_matrix(oa_without_context_elaboration_first)\n",
      "File \u001b[0;32m~/Documents/ip6-twitter-disinformation/notebooks/../src/llm_utils.py:148\u001b[0m, in \u001b[0;36mcalculate_binary_metrics\u001b[0;34m(df, classes, extraction_function)\u001b[0m\n\u001b[1;32m    146\u001b[0m     pred_column_df \u001b[39m=\u001b[39m df[df[pred_column_name]\u001b[39m.\u001b[39mnotna()]\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    147\u001b[0m     \u001b[39m#print(pred_column_df.iloc[0][pred_column_name])\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m     pred_column_df[pred_column_name] \u001b[39m=\u001b[39m df[pred_column_name]\u001b[39m.\u001b[39mapply(extraction_function)\n\u001b[1;32m    149\u001b[0m     prediction_per_class\u001b[39m.\u001b[39mappend(pred_column_df)\n\u001b[1;32m    151\u001b[0m confusion_matrices \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/envs/my_env/lib/python3.11/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39mapply()\n",
      "File \u001b[0;32m~/anaconda3/envs/my_env/lib/python3.11/site-packages/pandas/core/apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m~/anaconda3/envs/my_env/lib/python3.11/site-packages/pandas/core/apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39mmap_infer(\n\u001b[1;32m   1175\u001b[0m             values,\n\u001b[1;32m   1176\u001b[0m             f,\n\u001b[1;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvert_dtype,\n\u001b[1;32m   1178\u001b[0m         )\n\u001b[1;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/envs/my_env/lib/python3.11/site-packages/pandas/_libs/lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/ip6-twitter-disinformation/notebooks/../src/llm_utils.py:80\u001b[0m, in \u001b[0;36mextract_label\u001b[0;34m(classification_str)\u001b[0m\n\u001b[1;32m     78\u001b[0m label_pattern \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(?:Label:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms*)?(\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+)$\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m \u001b[39mprint\u001b[39m(classification_str)\n\u001b[0;32m---> 80\u001b[0m match \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msearch(label_pattern, classification_str, re\u001b[39m.\u001b[39mMULTILINE)\n\u001b[1;32m     82\u001b[0m \u001b[39mif\u001b[39;00m match:\n\u001b[1;32m     83\u001b[0m     \u001b[39mprint\u001b[39m(match\u001b[39m.\u001b[39mgroup(\u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/my_env/lib/python3.11/re/__init__.py:176\u001b[0m, in \u001b[0;36msearch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msearch\u001b[39m(pattern, string, flags\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m    174\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Scan through string looking for a match to the pattern, returning\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m     \u001b[39mreturn\u001b[39;00m _compile(pattern, flags)\u001b[39m.\u001b[39msearch(string)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object, got 'float'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../src\")\n",
    "import llm_utils\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "classes = [\"War/Terror\", \"Conspiracy Theory\", \"Education\", \"Election Campaign\", \"Environment\", \n",
    "              \"Government/Public\", \"Health\", \"Immigration/Integration\", \n",
    "              \"Justice/Crime\", \"Labor/Employment\", \n",
    "              \"Macroeconomics/Economic Regulation\", \"Media/Journalism\", \"Religion\", \"Science/Technology\"]\n",
    "\n",
    "# without context and elaboration first\n",
    "## Example:\n",
    "    #\"Elaborate on whether you think the Tweet is about {label} or something else.\\nTweet: {tweet_text}\\nElaboration: \"\n",
    "    #Followup: \\n\\nAssign the label 1 for {label} or 0 for not.\\nClass: \n",
    "\n",
    "oa_without_context_elaboration_first_df = pd.read_csv(\"../data/openassistant_llama_30b_4bit/generic_prompt_without_context_elaboration_first_v02/generic_test_0.csv\")\n",
    "extraction_function = llm_utils.get_extraction_function(\"extract_label\")\n",
    "prediction_per_class, confusion_matrices, classification_reports = llm_utils.calculate_binary_metrics(oa_without_context_elaboration_first_df, classes, extraction_function)\n",
    "oa_without_context_elaboration_first = {\"confusion_matrices\": confusion_matrices, \"classification_reports\": classification_reports}\n",
    "\n",
    "llm_utils.print_confusion_matrix(oa_without_context_elaboration_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for War/Terror:\n",
      "[[56  9]\n",
      " [17 48]]\n",
      "\n",
      "Confusion matrix for Conspiracy Theory:\n",
      "[[54 11]\n",
      " [26 39]]\n",
      "\n",
      "Confusion matrix for Education:\n",
      "[[43 22]\n",
      " [ 9 56]]\n",
      "\n",
      "Confusion matrix for Election Campaign:\n",
      "[[42 23]\n",
      " [17 48]]\n",
      "\n",
      "Confusion matrix for Environment:\n",
      "[[36 29]\n",
      " [12 53]]\n",
      "\n",
      "Confusion matrix for Government/Public:\n",
      "[[32 33]\n",
      " [23 42]]\n",
      "\n",
      "Confusion matrix for Health:\n",
      "[[45 20]\n",
      " [16 49]]\n",
      "\n",
      "Confusion matrix for Immigration/Integration:\n",
      "[[52 13]\n",
      " [16 49]]\n",
      "\n",
      "Confusion matrix for Justice/Crime:\n",
      "[[28 37]\n",
      " [ 8 57]]\n",
      "\n",
      "Confusion matrix for Labor/Employment:\n",
      "[[47 18]\n",
      " [18 47]]\n",
      "\n",
      "Confusion matrix for Macroeconomics/Economic Regulation:\n",
      "[[63  2]\n",
      " [48 17]]\n",
      "\n",
      "Confusion matrix for Media/Journalism:\n",
      "[[31 94]\n",
      " [ 1  4]]\n",
      "\n",
      "Confusion matrix for Religion:\n",
      "[[97 31]\n",
      " [ 0  2]]\n",
      "\n",
      "Confusion matrix for Science/Technology:\n",
      "[]\n",
      "\n",
      "Confusion matrix for War/Terror:\n",
      "[[38 25]\n",
      " [ 6 59]]\n",
      "\n",
      "Confusion matrix for Conspiracy Theory:\n",
      "[[45 20]\n",
      " [26 38]]\n",
      "\n",
      "Confusion matrix for Education:\n",
      "[[24 41]\n",
      " [ 2 63]]\n",
      "\n",
      "Confusion matrix for Election Campaign:\n",
      "[[37 28]\n",
      " [ 7 58]]\n",
      "\n",
      "Confusion matrix for Environment:\n",
      "[[43 22]\n",
      " [ 7 57]]\n",
      "\n",
      "Confusion matrix for Government/Public:\n",
      "[[17 47]\n",
      " [ 2 62]]\n",
      "\n",
      "Confusion matrix for Health:\n",
      "[[43 21]\n",
      " [ 9 56]]\n",
      "\n",
      "Confusion matrix for Immigration/Integration:\n",
      "[[32 32]\n",
      " [ 8 57]]\n",
      "\n",
      "Confusion matrix for Justice/Crime:\n",
      "[[30 35]\n",
      " [ 5 60]]\n",
      "\n",
      "Confusion matrix for Labor/Employment:\n",
      "[[31 34]\n",
      " [ 5 59]]\n",
      "\n",
      "Confusion matrix for Macroeconomics/Economic Regulation:\n",
      "[[20 44]\n",
      " [ 4 61]]\n",
      "\n",
      "Confusion matrix for Media/Journalism:\n",
      "[[12 53]\n",
      " [ 9 55]]\n",
      "\n",
      "Confusion matrix for Religion:\n",
      "[[45 20]\n",
      " [10 54]]\n",
      "\n",
      "Confusion matrix for Science/Technology:\n",
      "[[35 29]\n",
      " [ 5 59]]\n",
      "\n",
      "Confusion matrix for War/Terror:\n",
      "[[61  4]\n",
      " [26 39]]\n",
      "\n",
      "Confusion matrix for Conspiracy Theory:\n",
      "[[62  3]\n",
      " [51 14]]\n",
      "\n",
      "Confusion matrix for Education:\n",
      "[[64  1]\n",
      " [29 36]]\n",
      "\n",
      "Confusion matrix for Election Campaign:\n",
      "[[61  4]\n",
      " [27 38]]\n",
      "\n",
      "Confusion matrix for Environment:\n",
      "[[58  7]\n",
      " [24 41]]\n",
      "\n",
      "Confusion matrix for Government/Public:\n",
      "[[50 15]\n",
      " [37 28]]\n",
      "\n",
      "Confusion matrix for Health:\n",
      "[[63  2]\n",
      " [35 30]]\n",
      "\n",
      "Confusion matrix for Immigration/Integration:\n",
      "[[63  2]\n",
      " [42 23]]\n",
      "\n",
      "Confusion matrix for Justice/Crime:\n",
      "[[55 10]\n",
      " [36 29]]\n",
      "\n",
      "Confusion matrix for Labor/Employment:\n",
      "[[63  2]\n",
      " [30 35]]\n",
      "\n",
      "Confusion matrix for Macroeconomics/Economic Regulation:\n",
      "[[64  1]\n",
      " [56  9]]\n",
      "\n",
      "Confusion matrix for Media/Journalism:\n",
      "[[50 15]\n",
      " [22 43]]\n",
      "\n",
      "Confusion matrix for Religion:\n",
      "[[63  2]\n",
      " [36 29]]\n",
      "\n",
      "Confusion matrix for Science/Technology:\n",
      "[[63  2]\n",
      " [40 25]]\n",
      "\n",
      "Confusion matrix for War/Terror:\n",
      "[[49 16]\n",
      " [23 42]]\n",
      "\n",
      "Confusion matrix for Conspiracy Theory:\n",
      "[[57  7]\n",
      " [25 40]]\n",
      "\n",
      "Confusion matrix for Education:\n",
      "[[24 41]\n",
      " [ 5 60]]\n",
      "\n",
      "Confusion matrix for Election Campaign:\n",
      "[[44 21]\n",
      " [12 53]]\n",
      "\n",
      "Confusion matrix for Environment:\n",
      "[[36 29]\n",
      " [ 5 60]]\n",
      "\n",
      "Confusion matrix for Government/Public:\n",
      "[[19 45]\n",
      " [ 3 62]]\n",
      "\n",
      "Confusion matrix for Health:\n",
      "[[38 26]\n",
      " [20 44]]\n",
      "\n",
      "Confusion matrix for Immigration/Integration:\n",
      "[[34 31]\n",
      " [11 54]]\n",
      "\n",
      "Confusion matrix for Justice/Crime:\n",
      "[[20 45]\n",
      " [10 55]]\n",
      "\n",
      "Confusion matrix for Labor/Employment:\n",
      "[[31 34]\n",
      " [13 51]]\n",
      "\n",
      "Confusion matrix for Macroeconomics/Economic Regulation:\n",
      "[[21 44]\n",
      " [11 54]]\n",
      "\n",
      "Confusion matrix for Media/Journalism:\n",
      "[[37 28]\n",
      " [12 53]]\n",
      "\n",
      "Confusion matrix for Religion:\n",
      "[[46 18]\n",
      " [11 54]]\n",
      "\n",
      "Confusion matrix for Science/Technology:\n",
      "[[30 33]\n",
      " [ 6 59]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "models = [\n",
    "    {\n",
    "        \"name\": \"No context - Classification only\",\n",
    "        \"data\": without_context_classification_only,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"No context - Elaboration first\",\n",
    "        \"data\": without_context_elaboration_first,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"With rules - Classification only\",\n",
    "        \"data\": with_rules_classification_only,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"With rules - Elaboration first\",\n",
    "        \"data\": with_rules_elaboration_first,\n",
    "    }\n",
    "\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    llm_utils.print_confusion_matrix(model[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.22.0-py2.py3-none-any.whl (8.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: altair<5,>=3.2.0 in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (from streamlit) (4.2.2)\n",
      "Collecting blinker>=1.0.0\n",
      "  Downloading blinker-1.6.2-py3-none-any.whl (13 kB)\n",
      "Collecting cachetools>=4.0\n",
      "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: click>=7.0 in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (from streamlit) (8.1.3)\n",
      "Collecting importlib-metadata>=1.4\n",
      "  Downloading importlib_metadata-6.6.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: numpy in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (from streamlit) (1.24.3)\n",
      "Requirement already satisfied: packaging>=14.1 in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (from streamlit) (23.0)\n",
      "Requirement already satisfied: pandas<3,>=0.25 in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (from streamlit) (1.5.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (from streamlit) (9.4.0)\n",
      "Collecting protobuf<4,>=3.12\n",
      "  Downloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyarrow>=4.0 in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (from streamlit) (11.0.0)\n",
      "Collecting pympler>=0.9\n",
      "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (from streamlit) (2.8.2)\n",
      "Requirement already satisfied: requests>=2.4 in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (from streamlit) (2.29.0)\n",
      "Collecting rich>=10.11.0\n",
      "  Downloading rich-13.3.5-py3-none-any.whl (238 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.7/238.7 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tenacity<9,>=8.0.0 in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (from streamlit) (8.2.2)\n",
      "Requirement already satisfied: toml in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (from streamlit) (4.5.0)\n",
      "Collecting tzlocal>=1.1\n",
      "  Downloading tzlocal-4.3-py3-none-any.whl (20 kB)\n",
      "Collecting validators>=0.2\n",
      "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19 in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (from streamlit) (3.1.31)\n",
      "Collecting pydeck>=0.1.dev5\n",
      "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tornado>=6.0.3 in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (from streamlit) (6.2)\n",
      "Collecting watchdog\n",
      "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: entrypoints in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (from altair<5,>=3.2.0->streamlit) (0.4)\n",
      "Requirement already satisfied: jinja2 in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (from altair<5,>=3.2.0->streamlit) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (from altair<5,>=3.2.0->streamlit) (4.17.3)\n",
      "Requirement already satisfied: toolz in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (from altair<5,>=3.2.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (from gitpython!=3.1.19->streamlit) (4.0.10)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (from pandas<3,>=0.25->streamlit) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (from python-dateutil->streamlit) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (from requests>=2.4->streamlit) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (from requests>=2.4->streamlit) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (from requests>=2.4->streamlit) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (from requests>=2.4->streamlit) (2022.12.7)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (from rich>=10.11.0->streamlit) (2.2.0)\n",
      "Collecting pygments<3.0.0,>=2.13.0\n",
      "  Downloading Pygments-2.15.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pytz-deprecation-shim\n",
      "  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: decorator>=3.4.0 in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (from validators>=0.2->streamlit) (5.1.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19->streamlit) (5.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (from jinja2->altair<5,>=3.2.0->streamlit) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (from jsonschema>=3.0->altair<5,>=3.2.0->streamlit) (22.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (from jsonschema>=3.0->altair<5,>=3.2.0->streamlit) (0.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->streamlit) (0.1.2)\n",
      "Collecting tzdata\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: validators\n",
      "  Building wheel for validators (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19581 sha256=55c8fa74348198d4344fe896c217ac8ef74d5fe65a57d200f2c5438bc8c94f12\n",
      "  Stored in directory: /home/bruno/.var/app/com.visualstudio.code/cache/pip/wheels/82/35/dc/f88ec71edf2a5596bd72a8fa1b697277e0fcd3cde83048b8bf\n",
      "Successfully built validators\n",
      "Installing collected packages: zipp, watchdog, validators, tzdata, pympler, pygments, protobuf, cachetools, blinker, rich, pytz-deprecation-shim, pydeck, importlib-metadata, tzlocal, streamlit\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.11.2\n",
      "    Uninstalling Pygments-2.11.2:\n",
      "      Successfully uninstalled Pygments-2.11.2\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.22.1\n",
      "    Uninstalling protobuf-4.22.1:\n",
      "      Successfully uninstalled protobuf-4.22.1\n",
      "Successfully installed blinker-1.6.2 cachetools-5.3.0 importlib-metadata-6.6.0 protobuf-3.20.3 pydeck-0.8.1b0 pygments-2.15.1 pympler-1.0.1 pytz-deprecation-shim-0.1.0.post0 rich-13.3.5 streamlit-1.22.0 tzdata-2023.3 tzlocal-4.3 validators-0.20.0 watchdog-3.0.0 zipp-3.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement streamlit-components (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for streamlit-components\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install streamlit-components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "\n",
    "# Title\n",
    "st.title(\"Classification Report and Confusion Matrix Viewer\")\n",
    "\n",
    "# Model selection\n",
    "model_names = [model[\"name\"] for model in models]\n",
    "selected_model = st.sidebar.selectbox(\"Select model\", model_names)\n",
    "\n",
    "# Class selection\n",
    "class_options = classes\n",
    "selected_class = st.sidebar.selectbox(\"Select class\", class_options)\n",
    "\n",
    "# Get the selected data\n",
    "selected_model_data = None\n",
    "for model in models:\n",
    "    if model[\"name\"] == selected_model:\n",
    "        selected_model_data = model[\"data\"]\n",
    "        break\n",
    "\n",
    "selected_confusion_matrix = selected_model_data[\"confusion_matrices\"][selected_class]\n",
    "selected_classification_report = selected_model_data[\"classification_reports\"][selected_class]\n",
    "\n",
    "# Display the confusion matrix and classification report\n",
    "st.header(\"Confusion Matrix\")\n",
    "st.write(selected_confusion_matrix)\n",
    "\n",
    "st.header(\"Classification Report\")\n",
    "st.write(pd.DataFrame(selected_classification_report).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: streamlit run [OPTIONS] TARGET [ARGS]...\n",
      "Try 'streamlit run --help' for help.\n",
      "\n",
      "Error: Invalid value: File does not exist: report_viewer.py\n"
     ]
    }
   ],
   "source": [
    "!streamlit run report_viewer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"['War/Terror']\",\n",
       " \"['Others']\",\n",
       " \"['Macroeconomics/Economic Regulation']\",\n",
       " \"['Government/Public', 'Macroeconomics/Economic Regulation']\",\n",
       " \"['Government/Public', 'Macroeconomics/Economic Regulation', 'Media/Journalism', 'War/Terror']\",\n",
       " \"['Government/Public', 'Macroeconomics/Economic Regulation']\",\n",
       " \"['Others']\",\n",
       " \"['Government/Public', 'Macroeconomics/Economic Regulation']\",\n",
       " \"['Education', 'Media/Journalism']\",\n",
       " \"['Justice/Crime']\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "without_contex_only_classification_df[without_contex_only_classification_df[\"Macroeconomics/Economic Regulation_pred\"].notna()].copy().annotations.to_list()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT @Ben_Jonson_1985: #Feylak Ar-Rahman militants located in #EasternGhouta do not release #civilians from the fighting zone. The use of a l…',\n",
       " 'Women’s March Organizer Wants to “Take Vagina Away” from Female Genital Mutilation Survivor https://t.co/MmBm9WD6Ux https://t.co/T9rBUrOySf',\n",
       " 'IMF has demanded to take credit apartments of Ukrainians\\nhttp://t.co/2hcqYxfQHc #Ukraine',\n",
       " 'Libertarianism is nothing but liberty for exploiters, racists, oppressors and parasites. Socialism is liberty for the people; the working class. https://t.co/uyuugJzTpq',\n",
       " '🇮🇱🇵🇸Israel is now experiencing a moment of heightened political, economic, and military power is no excuse to avoid dealing with the Palestinian issue, writes #valdaiclub expert Amos Yadlin.\\n\\nhttps://t.co/7IPavmsHJh',\n",
       " 'Following the power cut crisis faced by Zimbabwe where electricity is only available between 10pm and 6am, people have adjusted their daily routines to make the most out of the electricity when it is available.\\n\\nRead full article: https://t.co/bCS2WWcfA9',\n",
       " 'VIDEO : Shepard Smith Throws a HYSTERICAL Russian HISSY FIT https://t.co/JMtb2cWP3q https://t.co/fWZIiGEA0d',\n",
       " 'Tax cuts are coming, but not as big as Trump promised https://t.co/gp2l1KnM1Z https://t.co/TLRf2SPyyF',\n",
       " 'Session Four \"The Conflict Between Universalism and Self-Identity\" has begun https://t.co/NeVx1IDWej',\n",
       " 'WATCH: Security officer knocked down and run over in WC - https://t.co/3RzrYM8q7f night, a blood-curdling video became viral. It captured how a security guard was knocked down and run over by bakkie at Die Boord Center next to the R44 in Stellenbosch in the Western Cape. The v... https://t.co/kZ1uLanAgB']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "without_contex_only_classification_df[without_contex_only_classification_df[\"Macroeconomics/Economic Regulation_pred\"].notna()].copy().text.to_list()[0:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
