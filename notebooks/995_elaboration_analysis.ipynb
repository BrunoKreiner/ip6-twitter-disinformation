{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../src\")\n",
    "import llm_utils\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "classes = [\"War/Terror\", \"Conspiracy Theory\", \"Education\", \"Election Campaign\", \"Environment\", \n",
    "              \"Government/Public\", \"Health\", \"Immigration/Integration\", \n",
    "              \"Justice/Crime\", \"Labor/Employment\", \n",
    "              \"Macroeconomics/Economic Regulation\", \"Media/Journalism\", \"Religion\", \"Science/Technology\"]\n",
    "\n",
    "oa_without_context_elaboration_first_v04_df = pd.read_csv(\"../data/openassistant_llama_30b_4bit/generic_prompt_without_context_elaboration_first_v04/generic_test_0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new columns filled initially with NaN\n",
    "oa_without_context_elaboration_first_v04_df['elaboration'] = np.nan\n",
    "oa_without_context_elaboration_first_v04_df['tested_on'] = np.nan\n",
    "\n",
    "# Go through all columns and check for not NaN\n",
    "for class_name in classes:\n",
    "    pred_column = class_name + \"_pred\"\n",
    "    \n",
    "    # Mask where data is not NaN\n",
    "    not_nan_mask = oa_without_context_elaboration_first_v04_df[pred_column].notna()\n",
    "\n",
    "    # Update 'elaboration' and 'tested_on' where mask is True\n",
    "    oa_without_context_elaboration_first_v04_df.loc[not_nan_mask, 'elaboration'] = oa_without_context_elaboration_first_v04_df.loc[not_nan_mask, pred_column]\n",
    "    oa_without_context_elaboration_first_v04_df.loc[not_nan_mask, 'tested_on'] = class_name\n",
    "\n",
    "# If you want to drop the rows still containing NaN in 'elaboration' and 'tested_on', uncomment the following line\n",
    "# oa_without_context_elaboration_first_v04_df = oa_without_context_elaboration_first_v04_df.dropna(subset=['elaboration', 'tested_on'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                        891103871484870657\n",
       "campaign_name                                                             VENEZUELA_201901_2\n",
       "text                                       How life may find a way on Saturn's moon https...\n",
       "annotations                                                           ['Science/Technology']\n",
       "normalized_tweet                           How life may find a way on Saturn's moon [url]...\n",
       "War/Terror_pred                                                                          NaN\n",
       "Conspiracy Theory_pred                                                                   NaN\n",
       "Education_pred                                                                           NaN\n",
       "Election Campaign_pred                                                                   NaN\n",
       "Environment_pred                                                                         NaN\n",
       "Government/Public_pred                                                                   NaN\n",
       "Health_pred                                                                              NaN\n",
       "Immigration/Integration_pred                                                             NaN\n",
       "Justice/Crime_pred                                                                       NaN\n",
       "Labor/Employment_pred                                                                    NaN\n",
       "Macroeconomics/Economic Regulation_pred                                                  NaN\n",
       "Media/Journalism_pred                                                                    NaN\n",
       "Religion_pred                                                                            NaN\n",
       "Science/Technology_pred                    Explanation: This tweet is discussing science ...\n",
       "elaboration                                Explanation: This tweet is discussing science ...\n",
       "tested_on                                                                 Science/Technology\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oa_without_context_elaboration_first_v04_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: Science/Technology\n",
      "Most common terms: [('this', 158), ('to', 149), ('tweet', 147), ('explanation', 130), ('technology', 129), ('science', 121), ('class', 121), ('is', 116), ('related', 112), ('not', 110)]\n",
      "\n",
      "\n",
      "Category: nan\n",
      "No valid text data\n",
      "\n",
      "\n",
      "Category: Media/Journalism\n",
      "Most common terms: [('tweet', 149), ('this', 147), ('to', 145), ('is', 129), ('the', 127), ('media', 126), ('explanation', 122), ('class', 116), ('journalism', 105), ('related', 87)]\n",
      "\n",
      "\n",
      "Category: Labor/Employment\n",
      "Most common terms: [('to', 133), ('this', 131), ('tweet', 122), ('labor', 117), ('explanation', 115), ('class', 110), ('employment', 105), ('is', 104), ('related', 102), ('not', 94)]\n",
      "\n",
      "\n",
      "Category: Environment\n",
      "Most common terms: [('this', 132), ('tweet', 120), ('to', 112), ('explanation', 104), ('class', 99), ('environment', 98), ('is', 96), ('not', 87), ('related', 85), ('but', 60)]\n",
      "\n",
      "\n",
      "Category: Macroeconomics/Economic Regulation\n",
      "Most common terms: [('to', 142), ('this', 134), ('tweet', 128), ('economic', 125), ('explanation', 118), ('the', 113), ('is', 108), ('class', 107), ('and', 92), ('macroeconomics', 88)]\n",
      "\n",
      "\n",
      "Category: Conspiracy Theory\n",
      "Most common terms: [('the', 152), ('tweet', 111), ('this', 108), ('conspiracy', 105), ('is', 102), ('explanation', 97), ('theory', 97), ('class', 96), ('to', 82), ('not', 79)]\n",
      "\n",
      "\n",
      "Category: Government/Public\n",
      "Most common terms: [('this', 124), ('tweet', 121), ('to', 117), ('is', 111), ('government', 111), ('explanation', 109), ('class', 108), ('public', 94), ('related', 89), ('the', 84)]\n",
      "\n",
      "\n",
      "Category: Justice/Crime\n",
      "Most common terms: [('this', 142), ('tweet', 132), ('to', 129), ('crime', 119), ('is', 118), ('explanation', 114), ('class', 108), ('justice', 100), ('the', 85), ('related', 83)]\n",
      "\n",
      "\n",
      "Category: Religion\n",
      "Most common terms: [('this', 153), ('tweet', 150), ('religion', 139), ('explanation', 125), ('is', 125), ('to', 124), ('not', 123), ('class', 107), ('related', 90), ('the', 88)]\n",
      "\n",
      "\n",
      "Category: Health\n",
      "Most common terms: [('to', 118), ('this', 117), ('tweet', 111), ('health', 110), ('explanation', 103), ('class', 100), ('is', 97), ('not', 97), ('related', 93), ('but', 80)]\n",
      "\n",
      "\n",
      "Category: Election Campaign\n",
      "Most common terms: [('to', 135), ('this', 132), ('tweet', 123), ('not', 108), ('election', 102), ('campaign', 102), ('explanation', 100), ('is', 98), ('any', 95), ('related', 93)]\n",
      "\n",
      "\n",
      "Category: War/Terror\n",
      "Most common terms: [('this', 116), ('to', 103), ('tweet', 102), ('the', 100), ('is', 98), ('explanation', 95), ('class', 94), ('war', 87), ('terrorism', 73), ('or', 72)]\n",
      "\n",
      "\n",
      "Category: Education\n",
      "Most common terms: [('this', 113), ('to', 111), ('education', 102), ('tweet', 98), ('explanation', 89), ('not', 83), ('related', 81), ('is', 78), ('class', 77), ('but', 53)]\n",
      "\n",
      "\n",
      "Category: Immigration/Integration\n",
      "Most common terms: [('to', 133), ('this', 129), ('tweet', 120), ('immigration', 112), ('explanation', 110), ('is', 109), ('class', 101), ('not', 100), ('integration', 97), ('related', 92)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "def get_most_frequent_terms(texts, top_n=10):\n",
    "    if len(texts) == 0 or all(isinstance(text, (type(None), str)) and not text.strip() for text in texts):\n",
    "        return []\n",
    "    vectorizer = CountVectorizer().fit(texts)\n",
    "    bag_of_words = vectorizer.transform(texts)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:top_n]\n",
    "\n",
    "tested_on_categories = oa_without_context_elaboration_first_v04_df['tested_on'].unique()\n",
    "\n",
    "for category in tested_on_categories:\n",
    "    category_df = oa_without_context_elaboration_first_v04_df[oa_without_context_elaboration_first_v04_df['tested_on'] == category]\n",
    "    texts = category_df['elaboration'].values\n",
    "    most_common_terms = get_most_frequent_terms(texts)\n",
    "    if most_common_terms:\n",
    "        print(\"Category:\", category)\n",
    "        print(\"Most common terms:\", most_common_terms)\n",
    "    else:\n",
    "        print(\"Category:\", category)\n",
    "        print(\"No valid text data\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /media/bruno/0d2f61d2-2b9c-4043-9a46-8e4dfe74fc95/bruno/anaconda3/envs/my_env/bin/pip3: /home/bruno/anaconda3/envs/my_env/bin/python: bad interpreter: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1002)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "\n",
    "tested_on_categories = oa_without_context_elaboration_first_v04_df['tested_on'].unique()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for category in tested_on_categories:\n",
    "    category_df = oa_without_context_elaboration_first_v04_df[oa_without_context_elaboration_first_v04_df['tested_on'] == category]\n",
    "    texts = category_df['elaboration'].dropna().values\n",
    "    texts = [word_tokenize(text) for text in texts]\n",
    "    texts = [[token.lower() for token in text if token.isalpha() and token.lower() not in stop_words] for text in texts]\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    dictionary.filter_extremes(no_below=2, no_above=0.5)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    lda_model = LdaModel(corpus=corpus, num_topics=5, id2word=dictionary)\n",
    "\n",
    "    print(f\"\\nCategory: {category}\\n\")\n",
    "    pprint(lda_model.print_topics(num_words=10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
