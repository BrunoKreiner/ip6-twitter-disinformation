{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-cpp-python in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (0.1.34)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages (from llama-cpp-python) (4.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ../models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =  59.11 KB\n",
      "llama_model_load_internal: mem required  = 5809.32 MB (+ 1026.00 MB per state)\n",
      "llama_init_from_file: kv self size  =  256.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-8d741a9c-5b15-4eb3-8c97-85fe2837ac99', 'object': 'text_completion', 'created': 1681765549, 'model': '../models/7B/ggml-model-q4_0.bin', 'choices': [{'text': 'Q: Name the planets in the solar system? A: 1. Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto B. Mercury, Venus, Earth, Moon, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto C. Mercury, Venus, Earth, Moon, Jupiter, Saturn, Uranus, Neptune, Pluto D.', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 15, 'completion_tokens': 89, 'total_tokens': 104}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   948.27 ms\n",
      "llama_print_timings:      sample time =    44.05 ms /    89 runs   (    0.49 ms per run)\n",
      "llama_print_timings: prompt eval time =  1780.68 ms /    15 tokens (  118.71 ms per token)\n",
      "llama_print_timings:        eval time = 23346.13 ms /    88 runs   (  265.30 ms per run)\n",
      "llama_print_timings:       total time = 25182.61 ms\n"
     ]
    }
   ],
   "source": [
    "# Install Package\n",
    "!pip install llama-cpp-python\n",
    "\n",
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=\"../models/7B/ggml-model-q4_0.bin\")\n",
    "output = llm(\"Q: Name the planets in the solar system? A: \", max_tokens=128, stop=[\"Q:\", \"\\n\"], echo=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   948.27 ms\n",
      "llama_print_timings:      sample time =    15.29 ms /    29 runs   (    0.53 ms per run)\n",
      "llama_print_timings: prompt eval time =  1686.99 ms /    15 tokens (  112.47 ms per token)\n",
      "llama_print_timings:        eval time =  7652.55 ms /    28 runs   (  273.31 ms per run)\n",
      "llama_print_timings:       total time =  9358.66 ms\n",
      "\n",
      "llama_print_timings:        load time =   948.27 ms\n",
      "llama_print_timings:      sample time =    34.18 ms /    70 runs   (    0.49 ms per run)\n",
      "llama_print_timings: prompt eval time =  1705.06 ms /    15 tokens (  113.67 ms per token)\n",
      "llama_print_timings:        eval time = 17737.20 ms /    69 runs   (  257.06 ms per run)\n",
      "llama_print_timings:       total time = 19485.12 ms\n",
      "\n",
      "llama_print_timings:        load time =   948.27 ms\n",
      "llama_print_timings:      sample time =    44.14 ms /    94 runs   (    0.47 ms per run)\n",
      "llama_print_timings: prompt eval time =  1611.41 ms /    15 tokens (  107.43 ms per token)\n",
      "llama_print_timings:        eval time = 23779.37 ms /    93 runs   (  255.69 ms per run)\n",
      "llama_print_timings:       total time = 25447.22 ms\n",
      "\n",
      "llama_print_timings:        load time =   948.27 ms\n",
      "llama_print_timings:      sample time =    23.82 ms /    51 runs   (    0.47 ms per run)\n",
      "llama_print_timings: prompt eval time =  1631.16 ms /    15 tokens (  108.74 ms per token)\n",
      "llama_print_timings:        eval time = 12849.72 ms /    50 runs   (  256.99 ms per run)\n",
      "llama_print_timings:       total time = 14510.52 ms\n",
      "\n",
      "llama_print_timings:        load time =   948.27 ms\n",
      "llama_print_timings:      sample time =     9.41 ms /    20 runs   (    0.47 ms per run)\n",
      "llama_print_timings: prompt eval time =  1633.83 ms /    15 tokens (  108.92 ms per token)\n",
      "llama_print_timings:        eval time =  5151.48 ms /    19 runs   (  271.13 ms per run)\n",
      "llama_print_timings:       total time =  6796.82 ms\n",
      "\n",
      "llama_print_timings:        load time =   948.27 ms\n",
      "llama_print_timings:      sample time =    20.17 ms /    40 runs   (    0.50 ms per run)\n",
      "llama_print_timings: prompt eval time =  1601.34 ms /    15 tokens (  106.76 ms per token)\n",
      "llama_print_timings:        eval time = 10230.90 ms /    39 runs   (  262.33 ms per run)\n",
      "llama_print_timings:       total time = 11857.13 ms\n",
      "\n",
      "llama_print_timings:        load time =   948.27 ms\n",
      "llama_print_timings:      sample time =    27.08 ms /    54 runs   (    0.50 ms per run)\n",
      "llama_print_timings: prompt eval time =  1666.46 ms /    15 tokens (  111.10 ms per token)\n",
      "llama_print_timings:        eval time = 13718.77 ms /    53 runs   (  258.84 ms per run)\n",
      "llama_print_timings:       total time = 15418.55 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 11.88 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "13.7 s ± 7.17 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   948.27 ms\n",
      "llama_print_timings:      sample time =     1.19 ms /     3 runs   (    0.40 ms per run)\n",
      "llama_print_timings: prompt eval time =  1627.75 ms /    15 tokens (  108.52 ms per token)\n",
      "llama_print_timings:        eval time =   512.74 ms /     2 runs   (  256.37 ms per run)\n",
      "llama_print_timings:       total time =  2142.05 ms\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "llm(\"Q: Name the planets in the solar system? A: \", max_tokens=128, stop=[\"Q:\", \"\\n\"], echo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = \"War/Terror = 1, Non-War/Terror = 0\"\n",
    "examples = \"This tweet is about War. This tweet is about Terror.\"\n",
    "instructions = \"Please label the following tweet as War/Terror or Non-War/Terror with the rules provided above and only provide the labels:\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 3s, sys: 135 ms, total: 2min 3s\n",
      "Wall time: 20.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   948.27 ms\n",
      "llama_print_timings:      sample time =    25.32 ms /    54 runs   (    0.47 ms per run)\n",
      "llama_print_timings: prompt eval time =  6679.82 ms /    61 tokens (  109.51 ms per token)\n",
      "llama_print_timings:        eval time = 14182.87 ms /    53 runs   (  267.60 ms per run)\n",
      "llama_print_timings:       total time = 20894.92 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-b85197b0-12cb-409a-8f78-cf115b43828e',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1681765971,\n",
       " 'model': '../models/7B/ggml-model-q4_0.bin',\n",
       " 'choices': [{'text': 'War/Terror = 1, Non-War/Terror = 0, Please label the following tweet as War/Terror or Non-War/Terror with the rules provided above and only provide the labels: , What Label does the tweet have? A: War/Terror B: Non-War/Terror C. Neither D. Both E. Others 25%, The label that you choose is not correct, please go back to the question and choose the correct answer from your previous answers.',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 61, 'completion_tokens': 54, 'total_tokens': 115}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm(f\"{rules}, {instructions} {examples}, What Label does the tweet have? A:\", max_tokens=128, stop=[\"Q:\", \"\\n\"], echo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)model.bin.index.json: 100%|██████████| 33.4k/33.4k [00:00<00:00, 5.77MB/s]\n",
      "Downloading (…)l-00001-of-00003.bin: 100%|██████████| 9.95G/9.95G [01:31<00:00, 109MB/s]\n",
      "Downloading (…)l-00002-of-00003.bin: 100%|██████████| 9.90G/9.90G [01:31<00:00, 108MB/s]\n",
      "Downloading (…)l-00003-of-00003.bin: 100%|██████████| 6.18G/6.18G [01:08<00:00, 90.7MB/s]\n",
      "Downloading shards: 100%|██████████| 3/3 [04:12<00:00, 84.17s/it]\n",
      "/home/bruno/anaconda3/envs/gpt4all/lib/python3.11/site-packages/bitsandbytes/cuda_setup/paths.py:98: UserWarning: /home/bruno/anaconda3/envs/gpt4all did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(\n",
      "/home/bruno/anaconda3/envs/gpt4all/lib/python3.11/site-packages/bitsandbytes/cuda_setup/paths.py:110: UserWarning: /app/lib did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(\n",
      "WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('unix'), PosixPath('path=/run/flatpak/at-spi-bus')}\n",
      "WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('path=/run/dbus/system_bus_socket'), PosixPath('unix')}\n",
      "WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/share/gconf/pop.default.path')}\n",
      "WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home/bruno/.var/app/com.visualstudio.code/sandbox')}\n",
      "WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home/bruno/.local/share/flatpak/exports/share/applications/com.visualstudio.code.desktop')}\n",
      "WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/app/lib/girepository-1.0')}\n",
      "WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/run/user/1000/gnupg/S.gpg-agent'), PosixPath('1'), PosixPath('0')}\n",
      "WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/app/lib/gstreamer-1.0')}\n",
      "WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/share/gconf/pop.mandatory.path')}\n",
      "WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('unix')}\n",
      "WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('@/tmp/.ICE-unix/2525,unix/pop-os'), PosixPath('local/pop-os'), PosixPath('/tmp/.ICE-unix/2525')}\n",
      "WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/app/etc/xdg')}\n",
      "WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/etc/egl/egl_external_platform.d'), PosixPath('/usr/share/egl/egl_external_platform.d')}\n",
      "WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "/home/bruno/anaconda3/envs/gpt4all/lib/python3.11/site-packages/bitsandbytes/cextension.py:43: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "CUDA_SETUP: Loading binary /home/bruno/anaconda3/envs/gpt4all/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]/home/bruno/anaconda3/envs/gpt4all/lib/python3.11/site-packages/bitsandbytes/functional.py:227: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return ct.c_void_p(A.data.storage().data_ptr())\n",
      "Loading checkpoint shards:   0%|          | 0/3 [00:17<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">8</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>baseModel = <span style=\"color: #808000; text-decoration-color: #808000\">\"lmsys/vicuna-13b-delta-v0\"</span>                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>load_8bit = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span>tokenizer = AutoTokenizer.from_pretrained(<span style=\"color: #808000; text-decoration-color: #808000\">\"databricks/dolly-v2-7b\"</span>)                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 8 model = AutoModelForCausalLM.from_pretrained(baseModel, load_in_8bit=load_8bit, torch_dt    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9 </span>generator = pipeline(task=<span style=\"color: #808000; text-decoration-color: #808000\">'text-generation'</span>, model=model, tokenizer=tokenizer)              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">11 </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(generator(<span style=\"color: #808000; text-decoration-color: #808000\">\"Python code to remove duplicates from dataframe\"</span>))                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/bruno/anaconda3/envs/gpt4all/lib/python3.11/site-packages/transformers/models/auto/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">auto_fa</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">ctory.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">471</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_pretrained</span>                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">468 │   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">469 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>(config) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>._model_mapping.keys():                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">470 │   │   │   </span>model_class = _get_model_class(config, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>._model_mapping)                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>471 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> model_class.from_pretrained(                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">472 │   │   │   │   </span>pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs,   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">473 │   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">474 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/bruno/anaconda3/envs/gpt4all/lib/python3.11/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">795</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_pretrained</span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2792 │   │   │   │   </span>mismatched_keys,                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2793 │   │   │   │   </span>offload_index,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2794 │   │   │   │   </span>error_msgs,                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2795 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>) = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>._load_pretrained_model(                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2796 │   │   │   │   </span>model,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2797 │   │   │   │   </span>state_dict,                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2798 │   │   │   │   </span>loaded_state_dict_keys,  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># XXX: rename?</span>                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/bruno/anaconda3/envs/gpt4all/lib/python3.11/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">123</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_load_pretrained_model</span>                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3120 │   │   │   │   </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3121 │   │   │   │   </span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3122 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> low_cpu_mem_usage:                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3123 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>new_error_msgs, offload_index, state_dict_index = _load_state_dict_i  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3124 │   │   │   │   │   │   </span>model_to_load,                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3125 │   │   │   │   │   │   </span>state_dict,                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3126 │   │   │   │   │   │   </span>loaded_keys,                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/bruno/anaconda3/envs/gpt4all/lib/python3.11/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">7</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">06</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_load_state_dict_into_meta_model</span>                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 703 │   │   │   │   </span>fp16_statistics = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 704 │   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 705 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #808000; text-decoration-color: #808000\">\"SCB\"</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> param_name:                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 706 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>set_module_8bit_tensor_to_device(                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 707 │   │   │   │   │   </span>model, param_name, param_device, value=param, fp16_statistics=fp16_s  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 708 │   │   │   │   </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 709 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/bruno/anaconda3/envs/gpt4all/lib/python3.11/site-packages/transformers/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">bitsandbytes.</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">78</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">set_module_8bit_tensor_to_device</span>                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 75 │   │   │   │   │   │   </span>)                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 76 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 77 │   │   │   │   </span>new_value = torch.tensor(value, device=<span style=\"color: #808000; text-decoration-color: #808000\">\"cpu\"</span>)                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 78 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>new_value = bnb.nn.Int8Params(new_value, requires_grad=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>, has_fp16_weigh   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 79 │   │   │   </span>module._parameters[tensor_name] = new_value                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 80 │   │   │   </span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 81 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> fp16_statistics <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/bruno/anaconda3/envs/gpt4all/lib/python3.11/site-packages/bitsandbytes/nn/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modules.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">202</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">to</span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">199 │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> device.type == <span style=\"color: #808000; text-decoration-color: #808000\">\"cuda\"</span>                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">200 │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.data.device.type == <span style=\"color: #808000; text-decoration-color: #808000\">\"cpu\"</span>                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">201 │   │   </span>):                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>202 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.cuda(device)                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">203 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">204 │   │   │   </span>new_param = Int8Params(                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">205 │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">super</span>().to(                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/bruno/anaconda3/envs/gpt4all/lib/python3.11/site-packages/bitsandbytes/nn/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modules.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">166</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">cuda</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">163 │   │   │   # we store the 8-bit rows-major weight</span>                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">164 │   │   │   # we convert this weight to the turning/ampere weight during the first infer</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">165 │   │   │   </span>B = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.data.contiguous().half().cuda(device)                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>166 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>CB, CBt, SCB, SCBt, coo_tensorB = bnb.functional.double_quant(B)               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">167 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">del</span> CBt                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">168 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">del</span> SCBt                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">169 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.data = CB                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/bruno/anaconda3/envs/gpt4all/lib/python3.11/site-packages/bitsandbytes/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">functional.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1602</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">double_quant</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1599 │   │   </span>rows = A.shape[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>]                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1600 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1601 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> row_stats <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> col_stats <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1602 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>row_stats, col_stats, nnz_row_ptr = get_colrow_absmax(                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1603 │   │   │   </span>A, threshold=threshold                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1604 │   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1605 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/bruno/anaconda3/envs/gpt4all/lib/python3.11/site-packages/bitsandbytes/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">functional.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1491</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_colrow_absmax</span>                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1488 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1489 │   </span>prev_device = pre_call(A.device)                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1490 │   </span>is_on_gpu([A, row_stats, col_stats, nnz_block_ptr])                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1491 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>lib.cget_col_row_stats(ptrA, ptrRowStats, ptrColStats, ptrNnzrows, ct.c_float(thresh  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1492 │   </span>post_call(prev_device)                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1493 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1494 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> threshold &gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0.0</span>:                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/bruno/anaconda3/envs/gpt4all/lib/python3.11/ctypes/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">389</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__getattr__</span>          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">386 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__getattr__</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, name):                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">387 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> name.startswith(<span style=\"color: #808000; text-decoration-color: #808000\">'__'</span>) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> name.endswith(<span style=\"color: #808000; text-decoration-color: #808000\">'__'</span>):                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">388 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">AttributeError</span>(name)                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>389 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>func = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.<span style=\"color: #00ff00; text-decoration-color: #00ff00\">__getitem__</span>(name)                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">390 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">setattr</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, name, func)                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">391 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> func                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">392 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/bruno/anaconda3/envs/gpt4all/lib/python3.11/ctypes/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">394</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__getitem__</span>          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">391 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> func                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">392 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">393 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__getitem__</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, name_or_ordinal):                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>394 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>func = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._FuncPtr((name_or_ordinal, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>))                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">395 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(name_or_ordinal, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">int</span>):                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">396 │   │   │   </span>func.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__name__</span> = name_or_ordinal                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">397 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> func                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AttributeError: </span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">/home/bruno/anaconda3/envs/gpt4all/lib/python3.11/site-packages/bitsandbytes/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">libbitsandbytes_cpu.so</span>: undefined \n",
       "symbol: cget_col_row_stats\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m8\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 5 \u001b[0mbaseModel = \u001b[33m\"\u001b[0m\u001b[33mlmsys/vicuna-13b-delta-v0\u001b[0m\u001b[33m\"\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 6 \u001b[0mload_8bit = \u001b[94mTrue\u001b[0m                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 7 \u001b[0mtokenizer = AutoTokenizer.from_pretrained(\u001b[33m\"\u001b[0m\u001b[33mdatabricks/dolly-v2-7b\u001b[0m\u001b[33m\"\u001b[0m)                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 8 model = AutoModelForCausalLM.from_pretrained(baseModel, load_in_8bit=load_8bit, torch_dt    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 9 \u001b[0mgenerator = pipeline(task=\u001b[33m'\u001b[0m\u001b[33mtext-generation\u001b[0m\u001b[33m'\u001b[0m, model=model, tokenizer=tokenizer)              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m10 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m11 \u001b[0m\u001b[96mprint\u001b[0m(generator(\u001b[33m\"\u001b[0m\u001b[33mPython code to remove duplicates from dataframe\u001b[0m\u001b[33m\"\u001b[0m))                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/bruno/anaconda3/envs/gpt4all/lib/python3.11/site-packages/transformers/models/auto/\u001b[0m\u001b[1;33mauto_fa\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mctory.py\u001b[0m:\u001b[94m471\u001b[0m in \u001b[92mfrom_pretrained\u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m468 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m469 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m \u001b[96mtype\u001b[0m(config) \u001b[95min\u001b[0m \u001b[96mcls\u001b[0m._model_mapping.keys():                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m470 \u001b[0m\u001b[2m│   │   │   \u001b[0mmodel_class = _get_model_class(config, \u001b[96mcls\u001b[0m._model_mapping)                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m471 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m model_class.from_pretrained(                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m472 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mpretrained_model_name_or_path, *model_args, config=config, **hub_kwargs,   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m473 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m474 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/bruno/anaconda3/envs/gpt4all/lib/python3.11/site-packages/transformers/\u001b[0m\u001b[1;33mmodeling_utils.py\u001b[0m:\u001b[94m2\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m795\u001b[0m in \u001b[92mfrom_pretrained\u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2792 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mmismatched_keys,                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2793 \u001b[0m\u001b[2m│   │   │   │   \u001b[0moffload_index,                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2794 \u001b[0m\u001b[2m│   │   │   │   \u001b[0merror_msgs,                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2795 \u001b[2m│   │   │   \u001b[0m) = \u001b[96mcls\u001b[0m._load_pretrained_model(                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2796 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mmodel,                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2797 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mstate_dict,                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2798 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mloaded_state_dict_keys,  \u001b[2m# XXX: rename?\u001b[0m                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/bruno/anaconda3/envs/gpt4all/lib/python3.11/site-packages/transformers/\u001b[0m\u001b[1;33mmodeling_utils.py\u001b[0m:\u001b[94m3\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m123\u001b[0m in \u001b[92m_load_pretrained_model\u001b[0m                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3120 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3121 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3122 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m low_cpu_mem_usage:                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m3123 \u001b[2m│   │   │   │   │   \u001b[0mnew_error_msgs, offload_index, state_dict_index = _load_state_dict_i  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3124 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mmodel_to_load,                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3125 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mstate_dict,                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3126 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mloaded_keys,                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/bruno/anaconda3/envs/gpt4all/lib/python3.11/site-packages/transformers/\u001b[0m\u001b[1;33mmodeling_utils.py\u001b[0m:\u001b[94m7\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m06\u001b[0m in \u001b[92m_load_state_dict_into_meta_model\u001b[0m                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 703 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mfp16_statistics = \u001b[94mNone\u001b[0m                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 704 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 705 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[33m\"\u001b[0m\u001b[33mSCB\u001b[0m\u001b[33m\"\u001b[0m \u001b[95mnot\u001b[0m \u001b[95min\u001b[0m param_name:                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 706 \u001b[2m│   │   │   │   \u001b[0mset_module_8bit_tensor_to_device(                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 707 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mmodel, param_name, param_device, value=param, fp16_statistics=fp16_s  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 708 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 709 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/bruno/anaconda3/envs/gpt4all/lib/python3.11/site-packages/transformers/utils/\u001b[0m\u001b[1;33mbitsandbytes.\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mpy\u001b[0m:\u001b[94m78\u001b[0m in \u001b[92mset_module_8bit_tensor_to_device\u001b[0m                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 75 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m)                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 76 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 77 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mnew_value = torch.tensor(value, device=\u001b[33m\"\u001b[0m\u001b[33mcpu\u001b[0m\u001b[33m\"\u001b[0m)                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 78 \u001b[2m│   │   │   \u001b[0mnew_value = bnb.nn.Int8Params(new_value, requires_grad=\u001b[94mFalse\u001b[0m, has_fp16_weigh   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 79 \u001b[0m\u001b[2m│   │   │   \u001b[0mmodule._parameters[tensor_name] = new_value                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 80 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 81 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m fp16_statistics \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/bruno/anaconda3/envs/gpt4all/lib/python3.11/site-packages/bitsandbytes/nn/\u001b[0m\u001b[1;33mmodules.py\u001b[0m:\u001b[94m202\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mto\u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[95mand\u001b[0m device.type == \u001b[33m\"\u001b[0m\u001b[33mcuda\u001b[0m\u001b[33m\"\u001b[0m                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m200 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[95mand\u001b[0m \u001b[96mself\u001b[0m.data.device.type == \u001b[33m\"\u001b[0m\u001b[33mcpu\u001b[0m\u001b[33m\"\u001b[0m                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m201 \u001b[0m\u001b[2m│   │   \u001b[0m):                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m202 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.cuda(device)                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m203 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m204 \u001b[0m\u001b[2m│   │   │   \u001b[0mnew_param = Int8Params(                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m205 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96msuper\u001b[0m().to(                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/bruno/anaconda3/envs/gpt4all/lib/python3.11/site-packages/bitsandbytes/nn/\u001b[0m\u001b[1;33mmodules.py\u001b[0m:\u001b[94m166\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mcuda\u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m163 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# we store the 8-bit rows-major weight\u001b[0m                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m164 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# we convert this weight to the turning/ampere weight during the first infer\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m165 \u001b[0m\u001b[2m│   │   │   \u001b[0mB = \u001b[96mself\u001b[0m.data.contiguous().half().cuda(device)                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m166 \u001b[2m│   │   │   \u001b[0mCB, CBt, SCB, SCBt, coo_tensorB = bnb.functional.double_quant(B)               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m167 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mdel\u001b[0m CBt                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m168 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mdel\u001b[0m SCBt                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m169 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.data = CB                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/bruno/anaconda3/envs/gpt4all/lib/python3.11/site-packages/bitsandbytes/\u001b[0m\u001b[1;33mfunctional.py\u001b[0m:\u001b[94m1602\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mdouble_quant\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1599 \u001b[0m\u001b[2m│   │   \u001b[0mrows = A.shape[\u001b[94m0\u001b[0m]                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1600 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1601 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m row_stats \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m \u001b[95mor\u001b[0m col_stats \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1602 \u001b[2m│   │   \u001b[0mrow_stats, col_stats, nnz_row_ptr = get_colrow_absmax(                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1603 \u001b[0m\u001b[2m│   │   │   \u001b[0mA, threshold=threshold                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1604 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1605 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/bruno/anaconda3/envs/gpt4all/lib/python3.11/site-packages/bitsandbytes/\u001b[0m\u001b[1;33mfunctional.py\u001b[0m:\u001b[94m1491\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mget_colrow_absmax\u001b[0m                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1488 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1489 \u001b[0m\u001b[2m│   \u001b[0mprev_device = pre_call(A.device)                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1490 \u001b[0m\u001b[2m│   \u001b[0mis_on_gpu([A, row_stats, col_stats, nnz_block_ptr])                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1491 \u001b[2m│   \u001b[0mlib.cget_col_row_stats(ptrA, ptrRowStats, ptrColStats, ptrNnzrows, ct.c_float(thresh  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1492 \u001b[0m\u001b[2m│   \u001b[0mpost_call(prev_device)                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1493 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1494 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m threshold > \u001b[94m0.0\u001b[0m:                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/bruno/anaconda3/envs/gpt4all/lib/python3.11/ctypes/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m389\u001b[0m in \u001b[92m__getattr__\u001b[0m          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m386 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m__getattr__\u001b[0m(\u001b[96mself\u001b[0m, name):                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m387 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m name.startswith(\u001b[33m'\u001b[0m\u001b[33m__\u001b[0m\u001b[33m'\u001b[0m) \u001b[95mand\u001b[0m name.endswith(\u001b[33m'\u001b[0m\u001b[33m__\u001b[0m\u001b[33m'\u001b[0m):                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m388 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mAttributeError\u001b[0m(name)                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m389 \u001b[2m│   │   \u001b[0mfunc = \u001b[96mself\u001b[0m.\u001b[92m__getitem__\u001b[0m(name)                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m390 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96msetattr\u001b[0m(\u001b[96mself\u001b[0m, name, func)                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m391 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m func                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m392 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/bruno/anaconda3/envs/gpt4all/lib/python3.11/ctypes/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m394\u001b[0m in \u001b[92m__getitem__\u001b[0m          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m391 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m func                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m392 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m393 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m__getitem__\u001b[0m(\u001b[96mself\u001b[0m, name_or_ordinal):                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m394 \u001b[2m│   │   \u001b[0mfunc = \u001b[96mself\u001b[0m._FuncPtr((name_or_ordinal, \u001b[96mself\u001b[0m))                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m395 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96misinstance\u001b[0m(name_or_ordinal, \u001b[96mint\u001b[0m):                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m396 \u001b[0m\u001b[2m│   │   │   \u001b[0mfunc.\u001b[91m__name__\u001b[0m = name_or_ordinal                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m397 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m func                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mAttributeError: \u001b[0m\n",
       "\u001b[35m/home/bruno/anaconda3/envs/gpt4all/lib/python3.11/site-packages/bitsandbytes/\u001b[0m\u001b[95mlibbitsandbytes_cpu.so\u001b[0m: undefined \n",
       "symbol: cget_col_row_stats\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import accelerate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "  \n",
    "baseModel = \"lmsys/vicuna-13b-delta-v0\"\n",
    "load_8bit = True  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-7b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(baseModel, load_in_8bit=load_8bit, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "generator = pipeline(task='text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "print(generator(\"Python code to remove duplicates from dataframe\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.device_count() > 0 else \"cpu\")\n",
    "prompt = f\"Label the tweet '{examples}' as War/Terror or Non War/Terror. A:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "\n",
    "generate_params = {\n",
    "    \"input_ids\": input_ids,\n",
    "    \"return_dict_in_generate\": True,\n",
    "    \"output_scores\": True,\n",
    "    \"max_new_tokens\": 128,\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=128,\n",
    "    )\n",
    "s = generation_output.sequences[0]\n",
    "output = tokenizer.decode(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Label the tweet 'This tweet is about War. This tweet is about Terror.' as War/Terror or Non War/Terror. A: War/Terror B: Non War/Terror\\n\\n\\nThe tweet is about War.\\n\\n\\nThe tweet is about Terror.\\n\\n\\nA: War/Terror B: War/Terror\\n\\n\\nThe tweet is about War.\\n\\n\\nThe tweet is about Terror.\\n\\n\\nA: Non War/Terror B: Non War/Terror\\n\\n\\nThe tweet is about War.\\n\\n\\nThe tweet is about Terror.\\n\\n\\nA: War/Terror B: War/Terror\\n\\n\\nThe tweet is about War.\\n\\n\\nThe tweet is about Terror.\\n\\n\\nA: Non War/Terror\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello, are you alive? I\\'ve been trying to reach you for days.\" \"I\\'ve been busy.\" \"Busy doing what?\" \"I\\'m a spy.\" \"A spy for who?\" \"The government.\" \"The government?\" \"What do you do for them?\" \"I kill spies.\" \"Oh, my God.\" \"I\\'m so sorry.\" \"I\\'m so sorry.\" \"I should have told you.\" \"I should have told you a long time ago.\" \"It\\'s okay.\" \"It\\'s okay.\" \"I\\'m a spy, too.\" \"What?\" \"What do you do?\" \"I\\'m a spy, too.\" \"I'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=128,\n",
    "    )\n",
    "s = generation_output.sequences[0]\n",
    "tokenizer.decode(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
