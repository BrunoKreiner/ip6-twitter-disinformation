{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bruno\\.conda\\envs\\my_env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   accuracy  micro_precision  micro_recall  micro_f1  macro_precision  \\\n",
      "0  0.965167         0.814042      0.794444  0.804124         0.791356   \n",
      "1  0.962333         0.792063      0.767102  0.779383         0.740888   \n",
      "2  0.961583         0.827021      0.734545  0.778045         0.844839   \n",
      "3  0.964800         0.829497      0.747886  0.786580         0.787015   \n",
      "4  0.965667         0.857298      0.736891  0.792548         0.831705   \n",
      "5  0.964200         0.837456      0.728670  0.779285         0.790979   \n",
      "6  0.964833         0.821569      0.777365  0.798856         0.784618   \n",
      "7  0.964667         0.809639      0.774789  0.791830         0.742390   \n",
      "8  0.964167         0.809293      0.782039  0.795433         0.726300   \n",
      "9  0.963000         0.794164      0.774020  0.783963         0.722006   \n",
      "\n",
      "   macro_recall  macro_f1  runtime  samples_per_second  steps_per_second  \\\n",
      "0      0.708402  0.739394    8.677              92.198            23.049   \n",
      "1      0.647392  0.683971   15.968              62.625            15.656   \n",
      "2      0.615006  0.684758    9.193              87.023            21.756   \n",
      "3      0.608488  0.676405   15.502              64.508            16.127   \n",
      "4      0.595904  0.678940    7.258             110.223            27.556   \n",
      "5      0.580760  0.646839   15.526              64.408            16.102   \n",
      "6      0.672926  0.718014   15.658              51.092            12.773   \n",
      "7      0.667765  0.696235   21.475              46.566            11.641   \n",
      "8      0.704469  0.707340   11.861              67.448            16.862   \n",
      "9      0.692502  0.696317   18.055              55.386            13.847   \n",
      "\n",
      "   num_epochs dataset  fold  \n",
      "0         6.5   valid     1  \n",
      "1         6.5    test     1  \n",
      "2         5.0   valid     2  \n",
      "3         5.0    test     2  \n",
      "4         4.0   valid     3  \n",
      "5         4.0    test     3  \n",
      "6         7.0   valid     4  \n",
      "7         7.0    test     4  \n",
      "8         6.5   valid     5  \n",
      "9         6.5    test     5  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the dataset class\n",
    "\n",
    "# Load data from json file\n",
    "with open('../reports/generic_epochs_200_train_size_full.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "dfs = []\n",
    "for k, v in data.items():\n",
    "    valid_metrics = v['valid']\n",
    "    valid_metrics['dataset'] = 'valid'\n",
    "    valid_metrics['fold'] = int(k) + 1\n",
    "    dfs.append(pd.DataFrame([valid_metrics]))\n",
    "    \n",
    "    test_metrics = v['test']\n",
    "    test_metrics['dataset'] = 'test'\n",
    "    test_metrics['fold'] = int(k) + 1\n",
    "    dfs.append(pd.DataFrame([test_metrics]))\n",
    "\n",
    "# Concatenate all dataframes together\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Rename columns\n",
    "df.columns = df.columns.str.replace('eval_', '')\n",
    "df = df.rename(columns={'epoch': 'num_epochs'})\n",
    "\n",
    "# Print the final dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>micro_precision</th>\n",
       "      <th>micro_recall</th>\n",
       "      <th>micro_f1</th>\n",
       "      <th>macro_precision</th>\n",
       "      <th>macro_recall</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>runtime</th>\n",
       "      <th>samples_per_second</th>\n",
       "      <th>steps_per_second</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>dataset</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.965167</td>\n",
       "      <td>0.814042</td>\n",
       "      <td>0.794444</td>\n",
       "      <td>0.804124</td>\n",
       "      <td>0.791356</td>\n",
       "      <td>0.708402</td>\n",
       "      <td>0.739394</td>\n",
       "      <td>8.677</td>\n",
       "      <td>92.198</td>\n",
       "      <td>23.049</td>\n",
       "      <td>6.5</td>\n",
       "      <td>valid</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.962333</td>\n",
       "      <td>0.792063</td>\n",
       "      <td>0.767102</td>\n",
       "      <td>0.779383</td>\n",
       "      <td>0.740888</td>\n",
       "      <td>0.647392</td>\n",
       "      <td>0.683971</td>\n",
       "      <td>15.968</td>\n",
       "      <td>62.625</td>\n",
       "      <td>15.656</td>\n",
       "      <td>6.5</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.961583</td>\n",
       "      <td>0.827021</td>\n",
       "      <td>0.734545</td>\n",
       "      <td>0.778045</td>\n",
       "      <td>0.844839</td>\n",
       "      <td>0.615006</td>\n",
       "      <td>0.684758</td>\n",
       "      <td>9.193</td>\n",
       "      <td>87.023</td>\n",
       "      <td>21.756</td>\n",
       "      <td>5.0</td>\n",
       "      <td>valid</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.964800</td>\n",
       "      <td>0.829497</td>\n",
       "      <td>0.747886</td>\n",
       "      <td>0.786580</td>\n",
       "      <td>0.787015</td>\n",
       "      <td>0.608488</td>\n",
       "      <td>0.676405</td>\n",
       "      <td>15.502</td>\n",
       "      <td>64.508</td>\n",
       "      <td>16.127</td>\n",
       "      <td>5.0</td>\n",
       "      <td>test</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.965667</td>\n",
       "      <td>0.857298</td>\n",
       "      <td>0.736891</td>\n",
       "      <td>0.792548</td>\n",
       "      <td>0.831705</td>\n",
       "      <td>0.595904</td>\n",
       "      <td>0.678940</td>\n",
       "      <td>7.258</td>\n",
       "      <td>110.223</td>\n",
       "      <td>27.556</td>\n",
       "      <td>4.0</td>\n",
       "      <td>valid</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.964200</td>\n",
       "      <td>0.837456</td>\n",
       "      <td>0.728670</td>\n",
       "      <td>0.779285</td>\n",
       "      <td>0.790979</td>\n",
       "      <td>0.580760</td>\n",
       "      <td>0.646839</td>\n",
       "      <td>15.526</td>\n",
       "      <td>64.408</td>\n",
       "      <td>16.102</td>\n",
       "      <td>4.0</td>\n",
       "      <td>test</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.964833</td>\n",
       "      <td>0.821569</td>\n",
       "      <td>0.777365</td>\n",
       "      <td>0.798856</td>\n",
       "      <td>0.784618</td>\n",
       "      <td>0.672926</td>\n",
       "      <td>0.718014</td>\n",
       "      <td>15.658</td>\n",
       "      <td>51.092</td>\n",
       "      <td>12.773</td>\n",
       "      <td>7.0</td>\n",
       "      <td>valid</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.964667</td>\n",
       "      <td>0.809639</td>\n",
       "      <td>0.774789</td>\n",
       "      <td>0.791830</td>\n",
       "      <td>0.742390</td>\n",
       "      <td>0.667765</td>\n",
       "      <td>0.696235</td>\n",
       "      <td>21.475</td>\n",
       "      <td>46.566</td>\n",
       "      <td>11.641</td>\n",
       "      <td>7.0</td>\n",
       "      <td>test</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.964167</td>\n",
       "      <td>0.809293</td>\n",
       "      <td>0.782039</td>\n",
       "      <td>0.795433</td>\n",
       "      <td>0.726300</td>\n",
       "      <td>0.704469</td>\n",
       "      <td>0.707340</td>\n",
       "      <td>11.861</td>\n",
       "      <td>67.448</td>\n",
       "      <td>16.862</td>\n",
       "      <td>6.5</td>\n",
       "      <td>valid</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.963000</td>\n",
       "      <td>0.794164</td>\n",
       "      <td>0.774020</td>\n",
       "      <td>0.783963</td>\n",
       "      <td>0.722006</td>\n",
       "      <td>0.692502</td>\n",
       "      <td>0.696317</td>\n",
       "      <td>18.055</td>\n",
       "      <td>55.386</td>\n",
       "      <td>13.847</td>\n",
       "      <td>6.5</td>\n",
       "      <td>test</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  micro_precision  micro_recall  micro_f1  macro_precision  \\\n",
       "0  0.965167         0.814042      0.794444  0.804124         0.791356   \n",
       "1  0.962333         0.792063      0.767102  0.779383         0.740888   \n",
       "2  0.961583         0.827021      0.734545  0.778045         0.844839   \n",
       "3  0.964800         0.829497      0.747886  0.786580         0.787015   \n",
       "4  0.965667         0.857298      0.736891  0.792548         0.831705   \n",
       "5  0.964200         0.837456      0.728670  0.779285         0.790979   \n",
       "6  0.964833         0.821569      0.777365  0.798856         0.784618   \n",
       "7  0.964667         0.809639      0.774789  0.791830         0.742390   \n",
       "8  0.964167         0.809293      0.782039  0.795433         0.726300   \n",
       "9  0.963000         0.794164      0.774020  0.783963         0.722006   \n",
       "\n",
       "   macro_recall  macro_f1  runtime  samples_per_second  steps_per_second  \\\n",
       "0      0.708402  0.739394    8.677              92.198            23.049   \n",
       "1      0.647392  0.683971   15.968              62.625            15.656   \n",
       "2      0.615006  0.684758    9.193              87.023            21.756   \n",
       "3      0.608488  0.676405   15.502              64.508            16.127   \n",
       "4      0.595904  0.678940    7.258             110.223            27.556   \n",
       "5      0.580760  0.646839   15.526              64.408            16.102   \n",
       "6      0.672926  0.718014   15.658              51.092            12.773   \n",
       "7      0.667765  0.696235   21.475              46.566            11.641   \n",
       "8      0.704469  0.707340   11.861              67.448            16.862   \n",
       "9      0.692502  0.696317   18.055              55.386            13.847   \n",
       "\n",
       "   num_epochs dataset  fold  \n",
       "0         6.5   valid     1  \n",
       "1         6.5    test     1  \n",
       "2         5.0   valid     2  \n",
       "3         5.0    test     2  \n",
       "4         4.0   valid     3  \n",
       "5         4.0    test     3  \n",
       "6         7.0   valid     4  \n",
       "7         7.0    test     4  \n",
       "8         6.5   valid     5  \n",
       "9         6.5    test     5  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_17768\\2172152441.py:1: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  df[df.dataset == \"valid\"].mean()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accuracy               0.964283\n",
       "micro_precision        0.825845\n",
       "micro_recall           0.765057\n",
       "micro_f1               0.793801\n",
       "macro_precision        0.795764\n",
       "macro_recall           0.659342\n",
       "macro_f1               0.705689\n",
       "runtime               10.529400\n",
       "samples_per_second    81.596800\n",
       "steps_per_second      20.399200\n",
       "num_epochs             5.800000\n",
       "fold                   3.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.dataset == \"valid\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_17768\\952663639.py:1: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  df[df.dataset == \"test\"].mean()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accuracy               0.963800\n",
       "micro_precision        0.812564\n",
       "micro_recall           0.758493\n",
       "micro_f1               0.784208\n",
       "macro_precision        0.756656\n",
       "macro_recall           0.639382\n",
       "macro_f1               0.679954\n",
       "runtime               17.305200\n",
       "samples_per_second    58.698600\n",
       "steps_per_second      14.674600\n",
       "num_epochs             5.800000\n",
       "fold                   3.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.dataset == \"test\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../models/generic_epochs_200_train_size_full_fold_0\\checkpoint-5200 were not used when initializing RobertaModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_17768\\2650439182.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Classification report:\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "                 Conspiracy Theory       0.53      0.42      0.47        45\n",
      "                         Education       0.56      0.69      0.62        13\n",
      "                 Election Campaign       0.85      0.85      0.85        33\n",
      "                       Environment       0.67      0.57      0.62        14\n",
      "                 Government/Public       0.79      0.73      0.76       291\n",
      "                            Health       0.73      0.65      0.69        46\n",
      "           Immigration/Integration       0.81      0.69      0.75        36\n",
      "                     Justice/Crime       0.83      0.85      0.84       137\n",
      "                  Labor/Employment       0.56      0.64      0.60        28\n",
      "Macroeconomics/Economic Regulation       0.80      0.66      0.73        62\n",
      "                  Media/Journalism       0.73      0.62      0.67        48\n",
      "                            Others       0.87      0.76      0.81       271\n",
      "                          Religion       0.62      0.73      0.67        11\n",
      "                Science/Technology       0.50      0.45      0.48        11\n",
      "                        War/Terror       0.92      0.93      0.92       255\n",
      "\n",
      "                         micro avg       0.81      0.76      0.79      1301\n",
      "                         macro avg       0.72      0.68      0.70      1301\n",
      "                      weighted avg       0.81      0.76      0.79      1301\n",
      "                       samples avg       0.81      0.79      0.79      1301\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../models/generic_epochs_200_train_size_full_fold_1\\checkpoint-4000 were not used when initializing RobertaModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_17768\\2650439182.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Classification report:\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "                 Conspiracy Theory       0.64      0.16      0.25        45\n",
      "                         Education       0.50      0.69      0.58        13\n",
      "                 Election Campaign       0.77      0.73      0.75        33\n",
      "                       Environment       1.00      0.50      0.67        14\n",
      "                 Government/Public       0.80      0.71      0.75       291\n",
      "                            Health       0.81      0.63      0.71        46\n",
      "           Immigration/Integration       0.95      0.56      0.70        36\n",
      "                     Justice/Crime       0.83      0.77      0.80       137\n",
      "                  Labor/Employment       0.74      0.50      0.60        28\n",
      "Macroeconomics/Economic Regulation       0.84      0.69      0.76        62\n",
      "                  Media/Journalism       0.93      0.56      0.70        48\n",
      "                            Others       0.80      0.86      0.83       271\n",
      "                          Religion       0.75      0.55      0.63        11\n",
      "                Science/Technology       0.80      0.36      0.50        11\n",
      "                        War/Terror       0.95      0.90      0.92       255\n",
      "\n",
      "                         micro avg       0.83      0.74      0.79      1301\n",
      "                         macro avg       0.81      0.61      0.68      1301\n",
      "                      weighted avg       0.83      0.74      0.78      1301\n",
      "                       samples avg       0.83      0.78      0.79      1301\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../models/generic_epochs_200_train_size_full_fold_2\\checkpoint-3200 were not used when initializing RobertaModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_17768\\2650439182.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2\n",
      "Classification report:\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "                 Conspiracy Theory       0.56      0.49      0.52        45\n",
      "                         Education       0.38      0.38      0.38        13\n",
      "                 Election Campaign       0.88      0.64      0.74        33\n",
      "                       Environment       0.50      0.64      0.56        14\n",
      "                 Government/Public       0.66      0.89      0.76       291\n",
      "                            Health       0.74      0.61      0.67        46\n",
      "           Immigration/Integration       0.76      0.86      0.81        36\n",
      "                     Justice/Crime       0.73      0.87      0.80       137\n",
      "                  Labor/Employment       0.72      0.46      0.57        28\n",
      "Macroeconomics/Economic Regulation       0.85      0.53      0.65        62\n",
      "                  Media/Journalism       0.87      0.56      0.68        48\n",
      "                            Others       0.94      0.71      0.81       271\n",
      "                          Religion       0.73      0.73      0.73        11\n",
      "                Science/Technology       0.83      0.45      0.59        11\n",
      "                        War/Terror       0.94      0.87      0.91       255\n",
      "\n",
      "                         micro avg       0.78      0.76      0.77      1301\n",
      "                         macro avg       0.74      0.65      0.68      1301\n",
      "                      weighted avg       0.80      0.76      0.77      1301\n",
      "                       samples avg       0.80      0.80      0.78      1301\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../models/generic_epochs_200_train_size_full_fold_3\\checkpoint-5600 were not used when initializing RobertaModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_17768\\2650439182.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3\n",
      "Classification report:\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "                 Conspiracy Theory       0.63      0.42      0.51        45\n",
      "                         Education       0.42      0.62      0.50        13\n",
      "                 Election Campaign       0.81      0.88      0.84        33\n",
      "                       Environment       0.89      0.57      0.70        14\n",
      "                 Government/Public       0.73      0.77      0.75       291\n",
      "                            Health       0.76      0.54      0.63        46\n",
      "           Immigration/Integration       0.74      0.64      0.69        36\n",
      "                     Justice/Crime       0.67      0.88      0.76       137\n",
      "                  Labor/Employment       0.60      0.64      0.62        28\n",
      "Macroeconomics/Economic Regulation       0.77      0.60      0.67        62\n",
      "                  Media/Journalism       0.81      0.62      0.71        48\n",
      "                            Others       0.86      0.77      0.81       271\n",
      "                          Religion       0.65      1.00      0.79        11\n",
      "                Science/Technology       0.33      0.36      0.35        11\n",
      "                        War/Terror       0.92      0.91      0.92       255\n",
      "\n",
      "                         micro avg       0.78      0.77      0.77      1301\n",
      "                         macro avg       0.71      0.68      0.68      1301\n",
      "                      weighted avg       0.78      0.77      0.77      1301\n",
      "                       samples avg       0.79      0.80      0.78      1301\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../models/generic_epochs_200_train_size_full_fold_4\\checkpoint-5200 were not used when initializing RobertaModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_17768\\2650439182.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4\n",
      "Classification report:\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "                 Conspiracy Theory       0.54      0.42      0.48        45\n",
      "                         Education       0.56      0.69      0.62        13\n",
      "                 Election Campaign       0.88      0.85      0.86        33\n",
      "                       Environment       0.62      0.57      0.59        14\n",
      "                 Government/Public       0.78      0.78      0.78       291\n",
      "                            Health       0.73      0.65      0.69        46\n",
      "           Immigration/Integration       0.80      0.67      0.73        36\n",
      "                     Justice/Crime       0.89      0.74      0.81       137\n",
      "                  Labor/Employment       0.68      0.46      0.55        28\n",
      "Macroeconomics/Economic Regulation       0.87      0.65      0.74        62\n",
      "                  Media/Journalism       0.79      0.71      0.75        48\n",
      "                            Others       0.83      0.83      0.83       271\n",
      "                          Religion       0.54      0.64      0.58        11\n",
      "                Science/Technology       0.80      0.36      0.50        11\n",
      "                        War/Terror       0.90      0.91      0.91       255\n",
      "\n",
      "                         micro avg       0.82      0.77      0.79      1301\n",
      "                         macro avg       0.75      0.66      0.69      1301\n",
      "                      weighted avg       0.82      0.77      0.79      1301\n",
      "                       samples avg       0.83      0.80      0.80      1301\n",
      "\n",
      "\n",
      "Average classification report:\n",
      "{'Conspiracy Theory': {'precision': 0.5808868908868908, 'recall': 0.38222222222222224, 'f1-score': 0.44492239858906524, 'support': 45.0}, 'Education': {'precision': 0.4861336032388664, 'recall': 0.6153846153846153, 'f1-score': 0.5413279712501069, 'support': 13.0}, 'Election Campaign': {'precision': 0.8356467904855002, 'recall': 0.7878787878787878, 'f1-score': 0.8074890250862792, 'support': 33.0}, 'Environment': {'precision': 0.7341880341880341, 'recall': 0.5714285714285714, 'f1-score': 0.6265592097113836, 'support': 14.0}, 'Government/Public': {'precision': 0.7510188181888175, 'recall': 0.7786941580756013, 'f1-score': 0.76054061773471, 'support': 291.0}, 'Health': {'precision': 0.7526776105081625, 'recall': 0.6173913043478262, 'f1-score': 0.6772410954140097, 'support': 46.0}, 'Immigration/Integration': {'precision': 0.8113731220261512, 'recall': 0.6833333333333333, 'f1-score': 0.7334115478655934, 'support': 36.0}, 'Justice/Crime': {'precision': 0.7904449160753269, 'recall': 0.8204379562043795, 'f1-score': 0.7995985166380181, 'support': 137.0}, 'Labor/Employment': {'precision': 0.661154970760234, 'recall': 0.5428571428571429, 'f1-score': 0.5869686433379055, 'support': 28.0}, 'Macroeconomics/Economic Regulation': {'precision': 0.8267222440815791, 'recall': 0.6258064516129032, 'f1-score': 0.7107318047438962, 'support': 62.0}, 'Media/Journalism': {'precision': 0.8270436053993382, 'recall': 0.6166666666666667, 'f1-score': 0.702427081732176, 'support': 48.0}, 'Others': {'precision': 0.8596229177978524, 'recall': 0.7889298892988931, 'f1-score': 0.8199218790159503, 'support': 271.0}, 'Religion': {'precision': 0.6556355409296585, 'recall': 0.7272727272727273, 'f1-score': 0.6789131920710868, 'support': 11.0}, 'Science/Technology': {'precision': 0.6533333333333333, 'recall': 0.4, 'f1-score': 0.48245037145292907, 'support': 11.0}, 'War/Terror': {'precision': 0.9252132655823024, 'recall': 0.9043137254901961, 'f1-score': 0.914320248208087, 'support': 255.0}, 'micro avg': {'precision': 0.8042793419860835, 'recall': 0.7621829362029209, 'f1-score': 0.7823732734117895, 'support': 1301.0}, 'macro avg': {'precision': 0.7434063775654698, 'recall': 0.6575078368049244, 'f1-score': 0.6857882401900798, 'support': 1301.0}, 'weighted avg': {'precision': 0.8099444412031994, 'recall': 0.7621829362029209, 'f1-score': 0.7788151010875151, 'support': 1301.0}, 'samples avg': {'precision': 0.8095333333333334, 'recall': 0.7946699999999999, 'f1-score': 0.7872750793650793, 'support': 1301.0}}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, multilabel_confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainerCallback\n",
    ")\n",
    "from transformers import RobertaConfig, RobertaModel\n",
    "from transformers import RobertaPreTrainedModel\n",
    "from typing import List, Dict\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def model_summary(model):\n",
    "    print(\"Model summary:\")\n",
    "    print(\"---------------------------\")\n",
    "    total_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        param_count = param.numel()\n",
    "        total_params += param_count\n",
    "    print(f\"Total parameters: {total_params}\")\n",
    "    \n",
    "\"\"\"def print_report(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, target_names=categories)\n",
    "    print(report)\n",
    "    sns.heatmap(cm, annot=True, xticklabels=categories, yticklabels=categories, fmt='g')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\"\"\"\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, x, y, mlb, tokenizer):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.mlb = mlb\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoded_tweets = self.preprocess_text(self.x)\n",
    "\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        return self.tokenizer(text, return_attention_mask=True, return_tensors='pt', padding=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.y[idx]\n",
    "        return {'input_ids': self.encoded_tweets['input_ids'][idx],\n",
    "                'attention_mask': self.encoded_tweets['attention_mask'][idx],\n",
    "                'label': torch.tensor(label, dtype=torch.float32)}\n",
    "        \n",
    "class MultiLabelDataCollator(DataCollatorWithPadding):\n",
    "    def __init__(self, tokenizer):\n",
    "        super().__init__(tokenizer)\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, torch.Tensor]]):\n",
    "        batch = super().__call__(features)\n",
    "        batch[\"labels\"] = torch.stack([feature[\"label\"] for feature in features])\n",
    "        return batch\n",
    "    \n",
    "class RobertaForMultiLabelClassification(RobertaPreTrainedModel):\n",
    "    def __init__(self, config, num_classes):\n",
    "        super().__init__(config)\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_classes)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        pooled_output = torch.mean(last_hidden_state, dim=1)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "    \n",
    "def calculate_average_report(reports):\n",
    "    avg_report = {}\n",
    "    for report in reports:\n",
    "        for key, scores in report.items():\n",
    "            if key not in avg_report:\n",
    "                avg_report[key] = {}\n",
    "                for score_key, score_value in scores.items():\n",
    "                    avg_report[key][score_key] = score_value\n",
    "            else:\n",
    "                for score_key, score_value in scores.items():\n",
    "                    avg_report[key][score_key] += score_value\n",
    "\n",
    "    num_reports = len(reports)\n",
    "    for key, scores in avg_report.items():\n",
    "        for score_key in scores:\n",
    "            avg_report[key][score_key] /= num_reports\n",
    "\n",
    "    return avg_report\n",
    "\n",
    "def calculate_metrics(task):\n",
    "    k = 5\n",
    "    \n",
    "    classification_reports = []\n",
    "\n",
    "    # Loop over each fold and load the corresponding model\n",
    "    for fold in range(k):\n",
    "        model_path = f\"../models/generic_epochs_200_train_size_full_fold_{fold}\"\n",
    "        # find the latest checkpoint file\n",
    "        checkpoint_files = [f for f in os.listdir(model_path) if f.startswith(\"checkpoint\")]\n",
    "        latest_checkpoint = os.path.join(model_path, sorted(checkpoint_files)[-1])\n",
    "        \n",
    "        # Load the model and tokenizer\n",
    "        model = AutoModel.from_pretrained(latest_checkpoint)\n",
    "        model.to(device)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-large\")\n",
    "\n",
    "        filename = f\"../data/labeled_data/{task}_test_{fold}.json\"\n",
    "        with open(filename) as f:\n",
    "            data = json.load(f)\n",
    "        train_df = pd.DataFrame(data[\"train\"])\n",
    "        val_df = pd.DataFrame(data[\"valid\"])\n",
    "        test_df = pd.DataFrame(data[\"test\"])\n",
    "        \n",
    "        train_annotations = train_df[\"annotations\"].tolist()\n",
    "        classes = set()\n",
    "        for annotation in train_annotations:\n",
    "            classes.update(annotation)\n",
    "        classes = sorted(list(classes))\n",
    "        \n",
    "        model.classifier = nn.Linear(model.config.hidden_size, len(classes))\n",
    "        checkpoint = torch.load(os.path.join(latest_checkpoint, \"pytorch_model.bin\"))\n",
    "        model.load_state_dict(checkpoint)\n",
    "        \n",
    "        model.to(device)\n",
    "        \n",
    "        mlb = MultiLabelBinarizer(classes=classes)\n",
    "        \n",
    "        train_labels = mlb.fit_transform(train_df[\"annotations\"])\n",
    "        val_labels = mlb.transform(val_df[\"annotations\"])\n",
    "        test_labels = mlb.transform(test_df[\"annotations\"])\n",
    "        \n",
    "        train_dataset = TweetDataset(train_df['text'].to_list(), torch.tensor(train_labels), mlb, tokenizer)\n",
    "        val_dataset = TweetDataset(val_df['text'].to_list(), torch.tensor(val_labels), mlb, tokenizer)\n",
    "        test_dataset = TweetDataset(test_df['text'].to_list(), torch.tensor(test_labels), mlb, tokenizer)\n",
    "        \n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            test_dataset, batch_size=4, shuffle=False, collate_fn=MultiLabelDataCollator(tokenizer)\n",
    "        )\n",
    "        model.eval()\n",
    "        labels = []\n",
    "        predictions = []\n",
    "        for batch in test_loader:\n",
    "            batch_inputs = {'input_ids': batch['input_ids'].to(device),\n",
    "                            'attention_mask': batch['attention_mask'].to(device)}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch_inputs)\n",
    "                last_hidden_state = outputs.last_hidden_state\n",
    "                pooled_output = torch.mean(last_hidden_state, dim=1)\n",
    "                logits = model.classifier(pooled_output)\n",
    "            batch_predictions = (logits > 0.5).detach().cpu().numpy().astype(int)\n",
    "            predictions.append(batch_predictions)\n",
    "            labels.append(batch['labels'].detach().cpu().numpy().astype(int))\n",
    "\n",
    "        predictions = np.concatenate(predictions, axis=0)\n",
    "        labels = np.concatenate(labels, axis = 0)\n",
    "\n",
    "        #cm = multilabel_confusion_matrix(labels, predictions)\n",
    "        dict_report = classification_report(labels, predictions, target_names=classes, labels=range(len(classes)), zero_division=0, output_dict=True)\n",
    "        classification_reports.append(dict_report)\n",
    "        \n",
    "        report = classification_report(labels, predictions, target_names=classes, labels=range(len(classes)), zero_division=0)\n",
    "\n",
    "        print(f\"Fold {fold}\")\n",
    "        #print(f\"Micro F1 score: {micro_f1}\")\n",
    "        print(\"Classification report:\")\n",
    "        print(report)\n",
    "        #sns.heatmap(cm, annot=True, xticklabels=classes, yticklabels=classes, fmt='g')\n",
    "        #plt.xlabel('Predicted')\n",
    "        #plt.ylabel('True')\n",
    "        #plt.show()\n",
    "    # Calculate and print the average report\n",
    "    average_report = calculate_average_report(classification_reports)\n",
    "    print(\"\\nAverage classification report:\")\n",
    "    print(average_report)\n",
    "    \n",
    "calculate_metrics(\"generic\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
