{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bruno\\.conda\\envs\\my_env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       loss  accuracy  micro_precision  micro_recall  micro_f1  \\\n",
      "0  0.131072  0.966333         0.825000      0.794444  0.809434   \n",
      "1  0.140527  0.964000         0.803671      0.774020  0.788567   \n",
      "2  0.129314  0.963500         0.823242      0.766364  0.793785   \n",
      "3  0.127321  0.964267         0.814297      0.761722  0.787133   \n",
      "4  0.110308  0.965333         0.851293      0.739700  0.791583   \n",
      "5  0.117867  0.962333         0.825088      0.717909  0.767776   \n",
      "6  0.150143  0.964667         0.808491      0.794991  0.801684   \n",
      "7  0.152349  0.963667         0.798107      0.777863  0.787855   \n",
      "8  0.131018  0.962417         0.809000      0.756782  0.782020   \n",
      "9  0.129022  0.963200         0.804719      0.760184  0.781818   \n",
      "\n",
      "   macro_precision  macro_recall  macro_f1  runtime  samples_per_second  \\\n",
      "0         0.783455      0.711673  0.739942    7.138             112.076   \n",
      "1         0.711862      0.685473  0.688094    8.360             119.617   \n",
      "2         0.771415      0.666642  0.705737    7.235             110.574   \n",
      "3         0.729128      0.672005  0.694474    9.069             110.266   \n",
      "4         0.839969      0.584434  0.655538    6.944             115.207   \n",
      "5         0.787322      0.581559  0.636694    8.001             124.984   \n",
      "6         0.731478      0.678978  0.697062    6.679             119.778   \n",
      "7         0.703339      0.713500  0.702180    7.999             125.016   \n",
      "8         0.758180      0.643461  0.671983    6.788             117.855   \n",
      "9         0.766275      0.631091  0.682364    8.396             119.104   \n",
      "\n",
      "   steps_per_second  num_epochs dataset  fold  \n",
      "0            28.019         9.0   valid     1  \n",
      "1            29.904         9.0    test     1  \n",
      "2            27.643         7.0   valid     2  \n",
      "3            27.566         7.0    test     2  \n",
      "4            28.802         5.0   valid     3  \n",
      "5            31.246         5.0    test     3  \n",
      "6            29.945        10.5   valid     4  \n",
      "7            31.254        10.5    test     4  \n",
      "8            29.464         7.0   valid     5  \n",
      "9            29.776         7.0    test     5  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import confusion_matrix, classification_report, multilabel_confusion_matrix\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from typing import List, Dict\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the dataset class\n",
    "\n",
    "# Load data from json file\n",
    "with open('../reports/generic_epochs_200_train_size_full.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "dfs = []\n",
    "for k, v in data.items():\n",
    "    valid_metrics = v['valid']\n",
    "    valid_metrics['dataset'] = 'valid'\n",
    "    valid_metrics['fold'] = int(k) + 1\n",
    "    dfs.append(pd.DataFrame([valid_metrics]))\n",
    "    \n",
    "    test_metrics = v['test']\n",
    "    test_metrics['dataset'] = 'test'\n",
    "    test_metrics['fold'] = int(k) + 1\n",
    "    dfs.append(pd.DataFrame([test_metrics]))\n",
    "\n",
    "# Concatenate all dataframes together\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Rename columns\n",
    "df.columns = df.columns.str.replace('eval_', '')\n",
    "df = df.rename(columns={'epoch': 'num_epochs'})\n",
    "\n",
    "# Print the final dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>micro_precision</th>\n",
       "      <th>micro_recall</th>\n",
       "      <th>micro_f1</th>\n",
       "      <th>macro_precision</th>\n",
       "      <th>macro_recall</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>runtime</th>\n",
       "      <th>samples_per_second</th>\n",
       "      <th>steps_per_second</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>dataset</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.131072</td>\n",
       "      <td>0.966333</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.794444</td>\n",
       "      <td>0.809434</td>\n",
       "      <td>0.783455</td>\n",
       "      <td>0.711673</td>\n",
       "      <td>0.739942</td>\n",
       "      <td>7.138</td>\n",
       "      <td>112.076</td>\n",
       "      <td>28.019</td>\n",
       "      <td>9.0</td>\n",
       "      <td>valid</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.140527</td>\n",
       "      <td>0.964000</td>\n",
       "      <td>0.803671</td>\n",
       "      <td>0.774020</td>\n",
       "      <td>0.788567</td>\n",
       "      <td>0.711862</td>\n",
       "      <td>0.685473</td>\n",
       "      <td>0.688094</td>\n",
       "      <td>8.360</td>\n",
       "      <td>119.617</td>\n",
       "      <td>29.904</td>\n",
       "      <td>9.0</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.129314</td>\n",
       "      <td>0.963500</td>\n",
       "      <td>0.823242</td>\n",
       "      <td>0.766364</td>\n",
       "      <td>0.793785</td>\n",
       "      <td>0.771415</td>\n",
       "      <td>0.666642</td>\n",
       "      <td>0.705737</td>\n",
       "      <td>7.235</td>\n",
       "      <td>110.574</td>\n",
       "      <td>27.643</td>\n",
       "      <td>7.0</td>\n",
       "      <td>valid</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.127321</td>\n",
       "      <td>0.964267</td>\n",
       "      <td>0.814297</td>\n",
       "      <td>0.761722</td>\n",
       "      <td>0.787133</td>\n",
       "      <td>0.729128</td>\n",
       "      <td>0.672005</td>\n",
       "      <td>0.694474</td>\n",
       "      <td>9.069</td>\n",
       "      <td>110.266</td>\n",
       "      <td>27.566</td>\n",
       "      <td>7.0</td>\n",
       "      <td>test</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.110308</td>\n",
       "      <td>0.965333</td>\n",
       "      <td>0.851293</td>\n",
       "      <td>0.739700</td>\n",
       "      <td>0.791583</td>\n",
       "      <td>0.839969</td>\n",
       "      <td>0.584434</td>\n",
       "      <td>0.655538</td>\n",
       "      <td>6.944</td>\n",
       "      <td>115.207</td>\n",
       "      <td>28.802</td>\n",
       "      <td>5.0</td>\n",
       "      <td>valid</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.117867</td>\n",
       "      <td>0.962333</td>\n",
       "      <td>0.825088</td>\n",
       "      <td>0.717909</td>\n",
       "      <td>0.767776</td>\n",
       "      <td>0.787322</td>\n",
       "      <td>0.581559</td>\n",
       "      <td>0.636694</td>\n",
       "      <td>8.001</td>\n",
       "      <td>124.984</td>\n",
       "      <td>31.246</td>\n",
       "      <td>5.0</td>\n",
       "      <td>test</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.150143</td>\n",
       "      <td>0.964667</td>\n",
       "      <td>0.808491</td>\n",
       "      <td>0.794991</td>\n",
       "      <td>0.801684</td>\n",
       "      <td>0.731478</td>\n",
       "      <td>0.678978</td>\n",
       "      <td>0.697062</td>\n",
       "      <td>6.679</td>\n",
       "      <td>119.778</td>\n",
       "      <td>29.945</td>\n",
       "      <td>10.5</td>\n",
       "      <td>valid</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.152349</td>\n",
       "      <td>0.963667</td>\n",
       "      <td>0.798107</td>\n",
       "      <td>0.777863</td>\n",
       "      <td>0.787855</td>\n",
       "      <td>0.703339</td>\n",
       "      <td>0.713500</td>\n",
       "      <td>0.702180</td>\n",
       "      <td>7.999</td>\n",
       "      <td>125.016</td>\n",
       "      <td>31.254</td>\n",
       "      <td>10.5</td>\n",
       "      <td>test</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.131018</td>\n",
       "      <td>0.962417</td>\n",
       "      <td>0.809000</td>\n",
       "      <td>0.756782</td>\n",
       "      <td>0.782020</td>\n",
       "      <td>0.758180</td>\n",
       "      <td>0.643461</td>\n",
       "      <td>0.671983</td>\n",
       "      <td>6.788</td>\n",
       "      <td>117.855</td>\n",
       "      <td>29.464</td>\n",
       "      <td>7.0</td>\n",
       "      <td>valid</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.129022</td>\n",
       "      <td>0.963200</td>\n",
       "      <td>0.804719</td>\n",
       "      <td>0.760184</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.766275</td>\n",
       "      <td>0.631091</td>\n",
       "      <td>0.682364</td>\n",
       "      <td>8.396</td>\n",
       "      <td>119.104</td>\n",
       "      <td>29.776</td>\n",
       "      <td>7.0</td>\n",
       "      <td>test</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy  micro_precision  micro_recall  micro_f1  \\\n",
       "0  0.131072  0.966333         0.825000      0.794444  0.809434   \n",
       "1  0.140527  0.964000         0.803671      0.774020  0.788567   \n",
       "2  0.129314  0.963500         0.823242      0.766364  0.793785   \n",
       "3  0.127321  0.964267         0.814297      0.761722  0.787133   \n",
       "4  0.110308  0.965333         0.851293      0.739700  0.791583   \n",
       "5  0.117867  0.962333         0.825088      0.717909  0.767776   \n",
       "6  0.150143  0.964667         0.808491      0.794991  0.801684   \n",
       "7  0.152349  0.963667         0.798107      0.777863  0.787855   \n",
       "8  0.131018  0.962417         0.809000      0.756782  0.782020   \n",
       "9  0.129022  0.963200         0.804719      0.760184  0.781818   \n",
       "\n",
       "   macro_precision  macro_recall  macro_f1  runtime  samples_per_second  \\\n",
       "0         0.783455      0.711673  0.739942    7.138             112.076   \n",
       "1         0.711862      0.685473  0.688094    8.360             119.617   \n",
       "2         0.771415      0.666642  0.705737    7.235             110.574   \n",
       "3         0.729128      0.672005  0.694474    9.069             110.266   \n",
       "4         0.839969      0.584434  0.655538    6.944             115.207   \n",
       "5         0.787322      0.581559  0.636694    8.001             124.984   \n",
       "6         0.731478      0.678978  0.697062    6.679             119.778   \n",
       "7         0.703339      0.713500  0.702180    7.999             125.016   \n",
       "8         0.758180      0.643461  0.671983    6.788             117.855   \n",
       "9         0.766275      0.631091  0.682364    8.396             119.104   \n",
       "\n",
       "   steps_per_second  num_epochs dataset  fold  \n",
       "0            28.019         9.0   valid     1  \n",
       "1            29.904         9.0    test     1  \n",
       "2            27.643         7.0   valid     2  \n",
       "3            27.566         7.0    test     2  \n",
       "4            28.802         5.0   valid     3  \n",
       "5            31.246         5.0    test     3  \n",
       "6            29.945        10.5   valid     4  \n",
       "7            31.254        10.5    test     4  \n",
       "8            29.464         7.0   valid     5  \n",
       "9            29.776         7.0    test     5  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/generic_epochs_200_train_size_full_fold_0\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/generic_epochs_200_train_size_full_fold_1\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/generic_epochs_200_train_size_full_fold_2\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/generic_epochs_200_train_size_full_fold_3\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/generic_epochs_200_train_size_full_fold_4\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Validation Classification Report In DataFrame Format:\n",
      "                                    precision    recall  f1-score  support\n",
      "Conspiracy Theory                    0.757218  0.405258  0.523622     50.8\n",
      "Education                            0.781645  0.518074  0.611675     12.6\n",
      "Election Campaign                    0.816610  0.723478  0.764618     26.6\n",
      "Environment                          0.767778  0.623701  0.670835     11.6\n",
      "Government/Public                    0.765498  0.835984  0.798719    249.6\n",
      "Health                               0.828771  0.809300  0.814534     42.8\n",
      "Immigration/Integration              0.833065  0.705780  0.758313     40.2\n",
      "Justice/Crime                        0.807952  0.783177  0.790424    114.4\n",
      "Labor/Employment                     0.802183  0.589868  0.658678     19.4\n",
      "Macroeconomics/Economic Regulation   0.756704  0.688306  0.717625     50.0\n",
      "Media/Journalism                     0.750921  0.677937  0.709312     36.8\n",
      "Others                               0.885206  0.777230  0.825638    211.4\n",
      "Religion                             0.699167  0.605130  0.609169     14.2\n",
      "Science/Technology                   0.740000  0.351775  0.467322     10.8\n",
      "War/Terror                           0.903078  0.930420  0.916327    187.8\n",
      "micro avg                            0.821713  0.774194  0.797133   1079.0\n",
      "macro avg                            0.793053  0.668361  0.709121   1079.0\n",
      "weighted avg                         0.823948  0.774194  0.790014   1079.0\n",
      "samples avg                          0.825896  0.801642  0.797819   1079.0\n",
      "\n",
      "Average Test Classification Report In DataFrame Format:\n",
      "                                    precision    recall  f1-score  support\n",
      "Conspiracy Theory                    0.648209  0.400000  0.493001     45.0\n",
      "Education                            0.515686  0.492308  0.498029     13.0\n",
      "Election Campaign                    0.822538  0.812121  0.817249     33.0\n",
      "Environment                          0.775285  0.571429  0.644423     14.0\n",
      "Government/Public                    0.752474  0.791753  0.771358    291.0\n",
      "Health                               0.760028  0.621739  0.682546     46.0\n",
      "Immigration/Integration              0.800568  0.677778  0.732708     36.0\n",
      "Justice/Crime                        0.818810  0.827737  0.821392    137.0\n",
      "Labor/Employment                     0.671022  0.542857  0.598170     28.0\n",
      "Macroeconomics/Economic Regulation   0.772532  0.648387  0.699841     62.0\n",
      "Media/Journalism                     0.818535  0.658333  0.729310     48.0\n",
      "Others                               0.863196  0.780074  0.819341    271.0\n",
      "Religion                             0.508791  0.545455  0.518104     11.0\n",
      "Science/Technology                   0.716667  0.454545  0.549644     11.0\n",
      "War/Terror                           0.919851  0.905882  0.912687    255.0\n",
      "micro avg                            0.812512  0.765872  0.788360   1301.0\n",
      "macro avg                            0.744279  0.648693  0.685854   1301.0\n",
      "weighted avg                         0.812254  0.765872  0.784849   1301.0\n",
      "samples avg                          0.817100  0.798040  0.793512   1301.0\n",
      "../models/GRU_202012_epochs_200_train_size_full_fold_0\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/GRU_202012_epochs_200_train_size_full_fold_1\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/GRU_202012_epochs_200_train_size_full_fold_2\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/GRU_202012_epochs_200_train_size_full_fold_3\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/GRU_202012_epochs_200_train_size_full_fold_4\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Validation Classification Report In DataFrame Format:\n",
      "                                    precision    recall  f1-score  support\n",
      "Conspiracy Theory                    0.487619  0.185714  0.262914     27.6\n",
      "Education                            0.878788  0.325315  0.443636     13.4\n",
      "Election Campaign                    0.842761  0.718005  0.772715     32.4\n",
      "Environment                          0.803077  0.545641  0.635353     13.4\n",
      "Government/Public                    0.783813  0.831081  0.805855    285.0\n",
      "Health                               0.862912  0.676321  0.757540     41.0\n",
      "Immigration/Integration              0.761587  0.682992  0.716051     30.4\n",
      "Justice/Crime                        0.827295  0.829153  0.826934    133.8\n",
      "Labor/Employment                     0.738079  0.544762  0.620644     22.0\n",
      "Macroeconomics/Economic Regulation   0.755060  0.684091  0.711724     58.4\n",
      "Media/Journalism                     0.741435  0.732045  0.732626     40.4\n",
      "Others                               0.875174  0.767054  0.815872    256.4\n",
      "Religion                             0.657143  0.524762  0.559290     14.8\n",
      "Science/Technology                   0.600000  0.210476  0.236303     12.0\n",
      "War/Terror                           0.836146  0.720137  0.757009     60.2\n",
      "micro avg                            0.807796  0.737172  0.770222   1041.2\n",
      "macro avg                            0.763393  0.598503  0.643631   1041.2\n",
      "weighted avg                         0.806781  0.737172  0.759891   1041.2\n",
      "samples avg                          0.789700  0.763392  0.761526   1041.2\n",
      "\n",
      "Average Test Classification Report In DataFrame Format:\n",
      "                                    precision    recall  f1-score  support\n",
      "Conspiracy Theory                    0.575000  0.014907  0.028734    161.0\n",
      "Education                            0.920000  0.377778  0.528571      9.0\n",
      "Election Campaign                    0.100000  0.050000  0.066667      4.0\n",
      "Environment                          0.000000  0.000000  0.000000      5.0\n",
      "Government/Public                    0.481834  0.570175  0.518701    114.0\n",
      "Health                               0.866768  0.458182  0.596016     55.0\n",
      "Immigration/Integration              0.875846  0.555294  0.674715     85.0\n",
      "Justice/Crime                        0.517059  0.390000  0.438294     40.0\n",
      "Labor/Employment                     0.583333  0.093333  0.156132     15.0\n",
      "Macroeconomics/Economic Regulation   0.783000  0.640000  0.690278     20.0\n",
      "Media/Journalism                     0.876134  0.406667  0.553319     30.0\n",
      "Others                               0.731710  0.717391  0.723667     46.0\n",
      "Religion                             0.600000  0.325000  0.415584      8.0\n",
      "Science/Technology                   0.366667  0.120000  0.173810      5.0\n",
      "War/Terror                           0.979549  0.884211  0.928969    893.0\n",
      "micro avg                            0.881855  0.678658  0.766518   1490.0\n",
      "macro avg                            0.617127  0.373529  0.432897   1490.0\n",
      "weighted avg                         0.848798  0.678658  0.722312   1490.0\n",
      "samples avg                          0.896567  0.773213  0.807201   1490.0\n",
      "../models/IRA_202012_epochs_200_train_size_full_fold_0\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/IRA_202012_epochs_200_train_size_full_fold_1\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/IRA_202012_epochs_200_train_size_full_fold_2\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/IRA_202012_epochs_200_train_size_full_fold_3\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/IRA_202012_epochs_200_train_size_full_fold_4\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Validation Classification Report In DataFrame Format:\n",
      "                                    precision    recall  f1-score  support\n",
      "Conspiracy Theory                    0.734867  0.554409  0.629351     53.0\n",
      "Education                            0.655000  0.465812  0.542857     12.6\n",
      "Election Campaign                    0.813182  0.669468  0.732557     24.6\n",
      "Environment                          0.643810  0.463654  0.519456     11.2\n",
      "Government/Public                    0.801787  0.815791  0.807746    219.8\n",
      "Health                               0.852431  0.760095  0.801660     41.2\n",
      "Immigration/Integration              0.753346  0.741867  0.741841     39.8\n",
      "Justice/Crime                        0.851844  0.824038  0.836712    126.8\n",
      "Labor/Employment                     0.511420  0.364228  0.414439     15.2\n",
      "Macroeconomics/Economic Regulation   0.757594  0.529776  0.613402     25.0\n",
      "Media/Journalism                     0.880761  0.640455  0.728272     26.2\n",
      "Others                               0.899862  0.822787  0.859199    229.6\n",
      "Religion                             0.637879  0.481026  0.526626     13.4\n",
      "Science/Technology                   0.682222  0.368254  0.454553     11.2\n",
      "War/Terror                           0.946549  0.950667  0.948485    195.4\n",
      "micro avg                            0.846446  0.787011  0.815575   1045.0\n",
      "macro avg                            0.761503  0.630155  0.677144   1045.0\n",
      "weighted avg                         0.845307  0.787011  0.810136   1045.0\n",
      "samples avg                          0.843896  0.818392  0.817015   1045.0\n",
      "\n",
      "Average Test Classification Report In DataFrame Format:\n",
      "                                    precision    recall  f1-score  support\n",
      "Conspiracy Theory                    0.173540  0.470588  0.251073     34.0\n",
      "Education                            0.844841  0.476923  0.606922     13.0\n",
      "Election Campaign                    0.760736  0.669767  0.709043     43.0\n",
      "Environment                          0.731818  0.637500  0.672509     16.0\n",
      "Government/Public                    0.709168  0.691818  0.698224    440.0\n",
      "Health                               0.945769  0.596296  0.730300     54.0\n",
      "Immigration/Integration              0.634959  0.642105  0.636313     38.0\n",
      "Justice/Crime                        0.595978  0.434667  0.493911     75.0\n",
      "Labor/Employment                     0.728909  0.644898  0.675029     49.0\n",
      "Macroeconomics/Economic Regulation   0.892202  0.373262  0.510206    187.0\n",
      "Media/Journalism                     0.743807  0.267327  0.389787    101.0\n",
      "Others                               0.694717  0.537778  0.605669    180.0\n",
      "Religion                             0.825350  0.440000  0.558003     15.0\n",
      "Science/Technology                   0.054690  0.066667  0.059614      9.0\n",
      "War/Terror                           0.683605  0.688479  0.682364    217.0\n",
      "micro avg                            0.668927  0.568729  0.614103   1471.0\n",
      "macro avg                            0.668006  0.509205  0.551931   1471.0\n",
      "weighted avg                         0.718683  0.568729  0.611426   1471.0\n",
      "samples avg                          0.645600  0.602623  0.598185   1471.0\n",
      "../models/REA_0621_epochs_200_train_size_full_fold_0\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/REA_0621_epochs_200_train_size_full_fold_1\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/REA_0621_epochs_200_train_size_full_fold_2\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/REA_0621_epochs_200_train_size_full_fold_3\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/REA_0621_epochs_200_train_size_full_fold_4\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Validation Classification Report In DataFrame Format:\n",
      "                                    precision    recall  f1-score  support\n",
      "Conspiracy Theory                    0.709593  0.469116  0.550600     58.4\n",
      "Education                            0.783333  0.361319  0.479754     10.4\n",
      "Election Campaign                    0.860476  0.716264  0.777278     25.6\n",
      "Environment                          0.780952  0.430000  0.509762      7.8\n",
      "Government/Public                    0.779164  0.835641  0.806027    264.6\n",
      "Health                               0.823932  0.804324  0.811148     38.0\n",
      "Immigration/Integration              0.789080  0.730749  0.757779     45.0\n",
      "Justice/Crime                        0.712772  0.622174  0.655140     61.2\n",
      "Labor/Employment                     0.805848  0.555071  0.622296     19.0\n",
      "Macroeconomics/Economic Regulation   0.815025  0.641988  0.715637     52.6\n",
      "Media/Journalism                     0.808596  0.757246  0.780849     43.2\n",
      "Others                               0.892614  0.788628  0.837002    220.2\n",
      "Religion                             0.773810  0.536663  0.612830     15.0\n",
      "Science/Technology                   0.700000  0.200214  0.289137     10.2\n",
      "War/Terror                           0.920996  0.931962  0.926427    232.2\n",
      "micro avg                            0.829528  0.772841  0.800047   1103.4\n",
      "macro avg                            0.797080  0.625424  0.675444   1103.4\n",
      "weighted avg                         0.831306  0.772841  0.792697   1103.4\n",
      "samples avg                          0.827479  0.800829  0.798818   1103.4\n",
      "\n",
      "Average Test Classification Report In DataFrame Format:\n",
      "                                    precision    recall  f1-score  support\n",
      "Conspiracy Theory                    0.000000  0.000000  0.000000      7.0\n",
      "Education                            0.614081  0.641667  0.618312     24.0\n",
      "Election Campaign                    0.871601  0.878947  0.873373     38.0\n",
      "Environment                          0.703263  0.539394  0.584460     33.0\n",
      "Government/Public                    0.611007  0.829630  0.703626    216.0\n",
      "Health                               0.657408  0.851429  0.739187     70.0\n",
      "Immigration/Integration              0.578974  0.466667  0.501916     12.0\n",
      "Justice/Crime                        0.878393  0.894789  0.884552    403.0\n",
      "Labor/Employment                     0.606824  0.326667  0.399096     30.0\n",
      "Macroeconomics/Economic Regulation   0.581310  0.600000  0.584769     49.0\n",
      "Media/Journalism                     0.372264  0.500000  0.420865     16.0\n",
      "Others                               0.904548  0.339207  0.490720    227.0\n",
      "Religion                             0.512063  0.514286  0.506593      7.0\n",
      "Science/Technology                   0.392682  0.300000  0.320201     14.0\n",
      "War/Terror                           0.565551  0.606061  0.582708     33.0\n",
      "micro avg                            0.728005  0.698558  0.712684   1179.0\n",
      "macro avg                            0.589998  0.552583  0.547359   1179.0\n",
      "weighted avg                         0.759733  0.698558  0.695124   1179.0\n",
      "samples avg                          0.704800  0.714733  0.695400   1179.0\n",
      "../models/UGANDA_0621_epochs_200_train_size_full_fold_0\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/UGANDA_0621_epochs_200_train_size_full_fold_1\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/UGANDA_0621_epochs_200_train_size_full_fold_2\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/UGANDA_0621_epochs_200_train_size_full_fold_3\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/UGANDA_0621_epochs_200_train_size_full_fold_4\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Validation Classification Report In DataFrame Format:\n",
      "                                    precision    recall  f1-score  support\n",
      "Conspiracy Theory                    0.716206  0.415223  0.518381     57.6\n",
      "Education                            0.753788  0.443450  0.529921     12.2\n",
      "Election Campaign                    0.850889  0.769755  0.801767     26.6\n",
      "Environment                          0.821275  0.506161  0.607821     13.2\n",
      "Government/Public                    0.806748  0.814413  0.810417    276.0\n",
      "Health                               0.853277  0.785185  0.816062     42.4\n",
      "Immigration/Integration              0.800115  0.746656  0.771080     46.6\n",
      "Justice/Crime                        0.792340  0.816158  0.802951    137.2\n",
      "Labor/Employment                     0.736447  0.584028  0.648187     21.4\n",
      "Macroeconomics/Economic Regulation   0.816876  0.684122  0.741638     58.6\n",
      "Media/Journalism                     0.785145  0.713565  0.747036     44.4\n",
      "Others                               0.822014  0.675124  0.739855    123.0\n",
      "Religion                             0.729091  0.546653  0.599109     12.6\n",
      "Science/Technology                   0.714286  0.254870  0.355556      9.0\n",
      "War/Terror                           0.920215  0.929613  0.924713    237.0\n",
      "micro avg                            0.828234  0.768999  0.797185   1117.8\n",
      "macro avg                            0.794581  0.645665  0.694300   1117.8\n",
      "weighted avg                         0.826462  0.768999  0.790182   1117.8\n",
      "samples avg                          0.827500  0.798362  0.795197   1117.8\n",
      "\n",
      "Average Test Classification Report In DataFrame Format:\n",
      "                                    precision    recall  f1-score  support\n",
      "Conspiracy Theory                    0.400000  0.036364  0.066667     11.0\n",
      "Education                            0.596816  0.426667  0.484651     15.0\n",
      "Election Campaign                    0.789111  0.412121  0.539967     33.0\n",
      "Environment                          0.591209  0.600000  0.568340      6.0\n",
      "Government/Public                    0.636905  0.476730  0.545242    159.0\n",
      "Health                               0.748757  0.708333  0.723098     48.0\n",
      "Immigration/Integration              0.533333  0.550000  0.530794      4.0\n",
      "Justice/Crime                        0.348001  0.739130  0.470787     23.0\n",
      "Labor/Employment                     0.778077  0.344444  0.450918     18.0\n",
      "Macroeconomics/Economic Regulation   0.488354  0.547368  0.513264     19.0\n",
      "Media/Journalism                     0.190714  0.160000  0.172810     10.0\n",
      "Others                               0.913257  0.858906  0.885132    713.0\n",
      "Religion                             0.399194  0.389474  0.359491     19.0\n",
      "Science/Technology                   0.518333  0.120000  0.188393     20.0\n",
      "War/Terror                           0.322948  0.888889  0.470663      9.0\n",
      "micro avg                            0.793819  0.723939  0.757171   1107.0\n",
      "macro avg                            0.550334  0.483895  0.464681   1107.0\n",
      "weighted avg                         0.801705  0.723939  0.748975   1107.0\n",
      "samples avg                          0.770700  0.754300  0.755873   1107.0\n",
      "../models/VENEZUELA_201901_2_epochs_200_train_size_full_fold_0\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/VENEZUELA_201901_2_epochs_200_train_size_full_fold_1\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/VENEZUELA_201901_2_epochs_200_train_size_full_fold_2\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/VENEZUELA_201901_2_epochs_200_train_size_full_fold_3\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/VENEZUELA_201901_2_epochs_200_train_size_full_fold_4\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_9360\\1245734267.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(label, dtype=torch.float32)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Validation Classification Report In DataFrame Format:\n",
      "                                    precision    recall  f1-score  support\n",
      "Conspiracy Theory                    0.690635  0.550844  0.608461     42.6\n",
      "Education                            0.718462  0.546032  0.612222     12.2\n",
      "Election Campaign                    0.816014  0.737045  0.773338     23.6\n",
      "Environment                          0.801948  0.458095  0.572738     12.0\n",
      "Government/Public                    0.727636  0.735022  0.729572    185.8\n",
      "Health                               0.830982  0.770965  0.798733     45.4\n",
      "Immigration/Integration              0.781185  0.696334  0.729669     27.8\n",
      "Justice/Crime                        0.870626  0.817260  0.842467    108.2\n",
      "Labor/Employment                     0.743788  0.580308  0.647341     22.4\n",
      "Macroeconomics/Economic Regulation   0.768681  0.713211  0.735934     55.0\n",
      "Media/Journalism                     0.725274  0.658272  0.686856     31.4\n",
      "Others                               0.884908  0.806842  0.843781    233.2\n",
      "Religion                             0.643175  0.457749  0.512072      9.8\n",
      "Science/Technology                   0.380952  0.219872  0.278291      9.6\n",
      "War/Terror                           0.916176  0.940254  0.927870    230.4\n",
      "micro avg                            0.827170  0.777262  0.801305   1049.4\n",
      "macro avg                            0.753363  0.645874  0.686623   1049.4\n",
      "weighted avg                         0.824960  0.777262  0.796897   1049.4\n",
      "samples avg                          0.829817  0.807354  0.804215   1049.4\n",
      "\n",
      "Average Test Classification Report In DataFrame Format:\n",
      "                                    precision    recall  f1-score  support\n",
      "Conspiracy Theory                    0.474213  0.151163  0.226191     86.0\n",
      "Education                            0.586003  0.293333  0.377763     15.0\n",
      "Election Campaign                    0.724657  0.854167  0.778752     48.0\n",
      "Environment                          0.683333  0.283333  0.395000     12.0\n",
      "Government/Public                    0.897120  0.826885  0.860064    610.0\n",
      "Health                               0.801867  0.763636  0.779698     33.0\n",
      "Immigration/Integration              0.901902  0.561224  0.686576     98.0\n",
      "Justice/Crime                        0.738298  0.583333  0.646098    168.0\n",
      "Labor/Employment                     0.573911  0.646154  0.606634     13.0\n",
      "Macroeconomics/Economic Regulation   0.581499  0.767568  0.656458     37.0\n",
      "Media/Journalism                     0.726126  0.784000  0.751954     75.0\n",
      "Others                               0.747268  0.595062  0.661822    162.0\n",
      "Religion                             0.856667  0.466667  0.590910     33.0\n",
      "Science/Technology                   0.863095  0.423529  0.562598     17.0\n",
      "War/Terror                           0.603407  0.833333  0.698167     42.0\n",
      "micro avg                            0.795749  0.685990  0.736541   1449.0\n",
      "macro avg                            0.717291  0.588893  0.618579   1449.0\n",
      "weighted avg                         0.794658  0.685990  0.722989   1449.0\n",
      "samples avg                          0.771067  0.715350  0.720960   1449.0\n"
     ]
    }
   ],
   "source": [
    "def model_summary(model):\n",
    "    print(\"Model summary:\")\n",
    "    print(\"---------------------------\")\n",
    "    total_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        param_count = param.numel()\n",
    "        total_params += param_count\n",
    "    print(f\"Total parameters: {total_params}\")\n",
    "    \n",
    "\"\"\"def print_report(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, target_names=categories)\n",
    "    print(report)\n",
    "    sns.heatmap(cm, annot=True, xticklabels=categories, yticklabels=categories, fmt='g')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\"\"\"\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, x, y, mlb, tokenizer):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.mlb = mlb\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoded_tweets = self.preprocess_text(self.x)\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        return self.tokenizer(text, return_attention_mask=True, return_tensors='pt', padding=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.y[idx]\n",
    "        return {'input_ids': self.encoded_tweets['input_ids'][idx],\n",
    "                'attention_mask': self.encoded_tweets['attention_mask'][idx],\n",
    "                'label': torch.tensor(label, dtype=torch.float32)}\n",
    "        \n",
    "class MultiLabelDataCollator(DataCollatorWithPadding):\n",
    "    def __init__(self, tokenizer):\n",
    "        super().__init__(tokenizer)\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, torch.Tensor]]):\n",
    "        batch = super().__call__(features)\n",
    "        batch[\"labels\"] = torch.stack([feature[\"label\"] for feature in features])\n",
    "        return batch\n",
    "    \n",
    "def get_classification_report(data_loader, model, target_names, label_names):\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    for batch in data_loader:\n",
    "        batch_inputs = {'input_ids': batch['input_ids'].to(device),\n",
    "                        'attention_mask': batch['attention_mask'].to(device)}\n",
    "        with torch.no_grad():\n",
    "            logits = model(**batch_inputs).logits\n",
    "        batch_predictions = (logits > 0.5).detach().cpu().numpy().astype(int)\n",
    "        predictions.append(batch_predictions)\n",
    "        labels.append(batch['labels'].detach().cpu().numpy().astype(int))\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    labels = np.concatenate(labels, axis = 0)\n",
    "\n",
    "    #cm = multilabel_confusion_matrix(labels, predictions)\n",
    "    dict_report = classification_report(labels, predictions, target_names=target_names, labels=label_names, zero_division=0, output_dict=True)\n",
    "    report = classification_report(labels, predictions, target_names=target_names, labels=label_names, zero_division=0)\n",
    "    return dict_report, report\n",
    "    \n",
    "def calculate_average_report(reports):\n",
    "    avg_report = {}\n",
    "    for report in reports:\n",
    "        for key, scores in report.items():\n",
    "            if key not in avg_report:\n",
    "                avg_report[key] = {}\n",
    "                for score_key, score_value in scores.items():\n",
    "                    avg_report[key][score_key] = score_value\n",
    "            else:\n",
    "                for score_key, score_value in scores.items():\n",
    "                    avg_report[key][score_key] += score_value\n",
    "\n",
    "    num_reports = len(reports)\n",
    "    for key, scores in avg_report.items():\n",
    "        for score_key in scores:\n",
    "            avg_report[key][score_key] /= num_reports\n",
    "\n",
    "    return avg_report\n",
    "\n",
    "def average_report_to_dataframe(average_report):\n",
    "    data = {\n",
    "        \"precision\": [],\n",
    "        \"recall\": [],\n",
    "        \"f1-score\": [],\n",
    "        \"support\": []\n",
    "    }\n",
    "    index = []\n",
    "\n",
    "    for class_name, metrics in average_report.items():\n",
    "        if class_name == 'accuracy':\n",
    "            continue\n",
    "\n",
    "        index.append(class_name)\n",
    "        data[\"precision\"].append(metrics[\"precision\"])\n",
    "        data[\"recall\"].append(metrics[\"recall\"])\n",
    "        data[\"f1-score\"].append(metrics[\"f1-score\"])\n",
    "        data[\"support\"].append(metrics[\"support\"])\n",
    "\n",
    "    return pd.DataFrame(data, index=index)\n",
    "\n",
    "def calculate_metrics(task):\n",
    "    k = 5\n",
    "    \n",
    "    val_classification_reports = []\n",
    "    test_classification_reports = []\n",
    "\n",
    "    # Loop over each fold and load the corresponding model\n",
    "    for fold in range(k):\n",
    "        model_path = f\"../models/{task}_epochs_200_train_size_full_fold_{fold}\"\n",
    "        # find the latest checkpoint file\n",
    "        #checkpoint_files = [f for f in os.listdir(model_path) if f.startswith(\"checkpoint\")]\n",
    "        latest_checkpoint = os.path.join(model_path, \"\")  # use \"\" for models that were manually saved after training. use sorted(checkpoint_files)[0] for the first automatically saved checkpoint \n",
    "        print(latest_checkpoint)\n",
    "        \n",
    "        # Load the model and tokenizer\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(latest_checkpoint)\n",
    "        model.to(device)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-large\")\n",
    "\n",
    "        filename = f\"../data/labeled_data/{task}_test_{fold}.json\"\n",
    "        with open(filename) as f:\n",
    "            data = json.load(f)\n",
    "        train_df = pd.DataFrame(data[\"train\"])\n",
    "        val_df = pd.DataFrame(data[\"valid\"])\n",
    "        test_df = pd.DataFrame(data[\"test\"])\n",
    "        \n",
    "        train_annotations = train_df[\"annotations\"].tolist()\n",
    "        classes = set()\n",
    "        for annotation in train_annotations:\n",
    "            classes.update(annotation)\n",
    "        classes = sorted(list(classes))\n",
    "        \n",
    "        checkpoint = torch.load(os.path.join(model_path, \"pytorch_model.bin\"))\n",
    "        model.load_state_dict(checkpoint)\n",
    "        \n",
    "        mlb = MultiLabelBinarizer(classes=classes)\n",
    "        \n",
    "        train_labels = mlb.fit_transform(train_df[\"annotations\"])\n",
    "        val_labels = mlb.transform(val_df[\"annotations\"])\n",
    "        test_labels = mlb.transform(test_df[\"annotations\"])\n",
    "        \n",
    "        train_dataset = TweetDataset(train_df['text'].to_list(), torch.tensor(train_labels), mlb, tokenizer)\n",
    "        val_dataset = TweetDataset(val_df['text'].to_list(), torch.tensor(val_labels), mlb, tokenizer)\n",
    "        test_dataset = TweetDataset(test_df['text'].to_list(), torch.tensor(test_labels), mlb, tokenizer)\n",
    "        \n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            val_dataset, batch_size=4, shuffle=False, collate_fn=MultiLabelDataCollator(tokenizer)\n",
    "        )\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            test_dataset, batch_size=4, shuffle=False, collate_fn=MultiLabelDataCollator(tokenizer)\n",
    "        )\n",
    "        \n",
    "        model.eval()\n",
    "        val_report_dict, val_report = get_classification_report(val_loader, model, classes, range(len(classes)))\n",
    "        test_report_dict, test_report = get_classification_report(test_loader, model, classes, range(len(classes)))\n",
    "        val_classification_reports.append(val_report_dict)\n",
    "        test_classification_reports.append(test_report_dict)\n",
    "\n",
    "    val_average_report = calculate_average_report(val_classification_reports)\n",
    "    test_average_report = calculate_average_report(test_classification_reports)\n",
    "    val_average_report_df = average_report_to_dataframe(val_average_report)\n",
    "    test_average_report_df = average_report_to_dataframe(test_average_report)\n",
    "    print(\"\\nAverage Validation Classification Report In DataFrame Format:\")\n",
    "    print(val_average_report_df) \n",
    "    print(\"\\nAverage Test Classification Report In DataFrame Format:\")\n",
    "    print(test_average_report_df) \n",
    "    return val_average_report_df, test_average_report_df\n",
    "\n",
    "generic_val_average_report_df, generic_test_average_report_df = calculate_metrics(\"generic\")\n",
    "GRU_202012_val_average_report_df, GRU_202012_test_average_report_df = calculate_metrics(\"GRU_202012\")\n",
    "IRA_202012_val_average_report_df, IRA_202012_test_average_report_df = calculate_metrics(\"IRA_202012\")\n",
    "REA_0621_val_average_report_df, REA_0621_test_average_report_df = calculate_metrics(\"REA_0621\")\n",
    "UGANDA_0621_val_average_report_df, UGANDA_0621_test_average_report_df = calculate_metrics(\"UGANDA_0621\")\n",
    "VENEZUELA_201901_2_val_average_report_df, VENEZUELA_201901_2_test_average_report_df = calculate_metrics(\"VENEZUELA_201901_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataframes = {\n",
    "    \"generic_val_average_report\": generic_val_average_report_df,\n",
    "    \"generic_test_average_report\": generic_test_average_report_df,\n",
    "    \"GRU_202012_val_average_report\": GRU_202012_val_average_report_df,\n",
    "    \"GRU_202012_test_average_report\": GRU_202012_test_average_report_df,\n",
    "    \"IRA_202012_val_average_report\": IRA_202012_val_average_report_df,\n",
    "    \"IRA_202012_test_average_report\": IRA_202012_test_average_report_df,\n",
    "    \"REA_0621_val_average_report\": REA_0621_val_average_report_df,\n",
    "    \"REA_0621_test_average_report\": REA_0621_test_average_report_df,\n",
    "    \"UGANDA_0621_val_average_report\": UGANDA_0621_val_average_report_df,\n",
    "    \"UGANDA_0621_test_average_report\": UGANDA_0621_test_average_report_df,\n",
    "    \"VENEZUELA_201901_2_val_average_report\": VENEZUELA_201901_2_val_average_report_df,\n",
    "    \"VENEZUELA_201901_2_test_average_report\": VENEZUELA_201901_2_test_average_report_df,\n",
    "}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    csv_filename = f\"../reports/{name}.csv\"\n",
    "    df.to_csv(csv_filename, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macro Averages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Train Data  Test Data  Validation  Test\n",
      "0            generic    generic        0.71  0.69\n",
      "1        All but GRU        GRU        0.64  0.43\n",
      "2        All but IRA        IRA        0.68  0.55\n",
      "3        All but REA        REA        0.68  0.55\n",
      "4     All but UGANDA     UGANDA        0.69  0.46\n",
      "5  All but VENEZUELA  VENEZUELA        0.69  0.62\n"
     ]
    }
   ],
   "source": [
    "def extract_macro_avg_value(df):\n",
    "    return df[df.index == \"macro avg\"][\"f1-score\"].values[0]\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    micro_avg_value = round(extract_macro_avg_value(df), 2)\n",
    "    train_data = name.split(\"_\")[0]\n",
    "\n",
    "    if \"val\" in name:\n",
    "        validation_value = micro_avg_value\n",
    "        test_value = None\n",
    "    elif \"test\" in name:\n",
    "        validation_value = None\n",
    "        test_value = micro_avg_value\n",
    "\n",
    "    test_data = train_data\n",
    "    if train_data != \"generic\":\n",
    "        train_data = \"All but \" + train_data\n",
    "\n",
    "    summary_data.append({\n",
    "        \"Train Data\": train_data,\n",
    "        \"Test Data\": test_data,\n",
    "        \"Validation\": validation_value,\n",
    "        \"Test\": test_value,\n",
    "    })\n",
    "\n",
    "# Combine rows with the same \"Train Data\" and \"Test Data\" into one\n",
    "macro_summary_df = pd.DataFrame(summary_data)\n",
    "macro_summary_df = macro_summary_df.groupby([\"Train Data\", \"Test Data\"], as_index=False).first()\n",
    "\n",
    "# Reorder columns\n",
    "macro_summary_df = macro_summary_df[[\"Train Data\", \"Test Data\", \"Validation\", \"Test\"]]\n",
    "macro_summary_df = macro_summary_df.reindex([macro_summary_df.index[-1]] + list(macro_summary_df.index[:-1]))\n",
    "macro_summary_df = macro_summary_df.reset_index(drop=True)\n",
    "\n",
    "print(macro_summary_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Micro Averages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Train Data  Test Data  Validation  Test\n",
      "0            generic    generic        0.80  0.79\n",
      "1        All but GRU        GRU        0.77  0.77\n",
      "2        All but IRA        IRA        0.82  0.61\n",
      "3        All but REA        REA        0.80  0.71\n",
      "4     All but UGANDA     UGANDA        0.80  0.76\n",
      "5  All but VENEZUELA  VENEZUELA        0.80  0.74\n"
     ]
    }
   ],
   "source": [
    "def extract_micro_avg_value(df):\n",
    "    return df[df.index == \"micro avg\"][\"f1-score\"].values[0]\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    micro_avg_value = round(extract_micro_avg_value(df), 2)\n",
    "    train_data = name.split(\"_\")[0]\n",
    "\n",
    "    if \"val\" in name:\n",
    "        validation_value = micro_avg_value\n",
    "        test_value = None\n",
    "    elif \"test\" in name:\n",
    "        validation_value = None\n",
    "        test_value = micro_avg_value\n",
    "\n",
    "    test_data = train_data\n",
    "    if train_data != \"generic\":\n",
    "        train_data = \"All but \" + train_data\n",
    "\n",
    "    summary_data.append({\n",
    "        \"Train Data\": train_data,\n",
    "        \"Test Data\": test_data,\n",
    "        \"Validation\": validation_value,\n",
    "        \"Test\": test_value,\n",
    "    })\n",
    "\n",
    "# Combine rows with the same \"Train Data\" and \"Test Data\" into one\n",
    "micro_summary_df = pd.DataFrame(summary_data)\n",
    "micro_summary_df = micro_summary_df.groupby([\"Train Data\", \"Test Data\"], as_index=False).first()\n",
    "\n",
    "# Reorder columns\n",
    "micro_summary_df = micro_summary_df[[\"Train Data\", \"Test Data\", \"Validation\", \"Test\"]]\n",
    "micro_summary_df = micro_summary_df.reindex([micro_summary_df.index[-1]] + list(micro_summary_df.index[:-1]))\n",
    "micro_summary_df = micro_summary_df.reset_index(drop=True)\n",
    "\n",
    "print(macro_summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human-readable table:\n",
      "   Dataset           BERTweet Large     \n",
      "Train Data Test Data     Validation Test\n",
      "   Generic   Generic            0.8 0.79\n",
      "\n",
      "\n",
      "LaTeX table:\n",
      "\\begin{tabular}{|l|l|l|l|}\n",
      "\\hline \\hline{@{}c@{}}{|l|l|l|l|}\n",
      "\n",
      "   Dataset & \\multicolumn{2}{c}{BERTweet Large} \\\\ \\hline\n",
      "Train Data & Test Data &     Validation & Test \\\\ \\hline\n",
      "\n",
      "   Generic &   Generic &            0,8 & 0,79 \\\\ \\hline\n",
      "\n",
      "\\\\ \\hline \\hline\n",
      "\\end{tabular}{@{}c@{}}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_16532\\3602932945.py:23: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  latex_table = f1_scores_df.to_latex(index=False, bold_rows=True, multicolumn=True, multicolumn_format='c', decimal=',', column_format='|l|l|l|l|', header=True, escape=False)\n"
     ]
    }
   ],
   "source": [
    "def create_latex_table(val_average_report_df, test_average_report_df):\n",
    "    train_data = \"Generic\"\n",
    "    test_data = \"Generic\"\n",
    "    \n",
    "    val_micro_avg = round(val_average_report_df.loc[\"micro avg\", \"f1-score\"], 2)\n",
    "    test_micro_avg = round(test_average_report_df.loc[\"micro avg\", \"f1-score\"], 2)\n",
    "    \n",
    "    data = [[train_data, test_data, val_micro_avg, test_micro_avg]]\n",
    "    \n",
    "    columns = pd.MultiIndex.from_tuples([\n",
    "        (\"Dataset\", \"Train Data\"),\n",
    "        (\"Dataset\", \"Test Data\"),\n",
    "        (\"BERTweet Large\", \"Validation\"),\n",
    "        (\"BERTweet Large\", \"Test\")\n",
    "    ])\n",
    "    \n",
    "    f1_scores_df = pd.DataFrame(data, columns=columns)\n",
    "    \n",
    "    print(\"Human-readable table:\")\n",
    "    print(f1_scores_df.to_string(index=False))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    latex_table = f1_scores_df.to_latex(index=False, bold_rows=True, multicolumn=True, multicolumn_format='c', decimal=',', column_format='|l|l|l|l|', header=True, escape=False)\n",
    "\n",
    "    latex_table = latex_table.replace('\\\\toprule', '')\n",
    "    latex_table = latex_table.replace('\\\\midrule', '')\n",
    "    latex_table = latex_table.replace('\\\\bottomrule', '')\n",
    "\n",
    "    # Resize the header and center it\n",
    "    latex_table = latex_table.replace('{tabular}', '{tabular}{@{}c@{}}')\n",
    "    latex_table = latex_table.replace('Dataset & BERTweet Large', '\\\\large{Dataset} & \\\\large{BERTweet Large}')\n",
    "    \n",
    "    latex_table = latex_table.replace(\"\\\\begin{tabular}\", \"\\\\begin{tabular}{|l|l|l|l|}\\n\\\\hline \\\\hline\")\n",
    "    latex_table = latex_table.replace(\"\\\\end{tabular}\", \"\\\\\\\\ \\\\hline \\\\hline\\n\\\\end{tabular}\")\n",
    "\n",
    "    # Add borders between the rows\n",
    "    latex_table = latex_table.replace('\\\\\\\\\\n', '\\\\\\\\ \\\\hline\\n')\n",
    "\n",
    "    return latex_table\n",
    "\n",
    "latex_table = create_latex_table(val_average_report_df, test_average_report_df)\n",
    "print(\"LaTeX table:\")\n",
    "print(latex_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
