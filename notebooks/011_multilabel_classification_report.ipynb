{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../src\")\n",
    "import llm_utils\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "classes = [\"War/Terror\", \"Conspiracy Theory\", \"Education\", \"Election Campaign\", \"Environment\", \n",
    "              \"Government/Public\", \"Health\", \"Immigration/Integration\", \n",
    "              \"Justice/Crime\", \"Labor/Employment\", \n",
    "              \"Macroeconomics/Economic Regulation\", \"Media/Journalism\", \"Religion\", \"Science/Technology\", \"Others\"]\n",
    "# Transform the labels into binary format\n",
    "mlb = MultiLabelBinarizer(classes=classes)\n",
    "\n",
    "def get_report(base_path, test_name, extract_func, mlb, classes, verbose = False):\n",
    "    df = pd.read_csv(base_path+test_name+\"/test_generic_test_0.csv\")\n",
    "    if verbose:\n",
    "        print(\"------------------\")\n",
    "        print(\"Prompt:\")\n",
    "        print(df.prompt[0])\n",
    "        print(\"------------------\")\n",
    "        print()\n",
    "        print()\n",
    "        for i in range(0,3):\n",
    "            print(\"------------------\")\n",
    "            print(\"Pre-Extraction:\")\n",
    "            print(\"------------------\")\n",
    "            print(\"Normalized Tweet: \", df.iloc[i].normalized_tweet)\n",
    "            print(\"Response: \", df.iloc[i]['response'])\n",
    "        print()\n",
    "    df['annotations'] = df['annotations'].apply(lambda x: extract_func(x, classes))\n",
    "    df['response'] = df['response'].apply(lambda x: extract_func(x, classes))\n",
    "    if verbose:\n",
    "        for i in range(0,3):\n",
    "            print(\"------------------\")\n",
    "            print(\"Post-Extraction:\")\n",
    "            print(\"------------------\")\n",
    "            print(\"Normalized Tweet: \", df.iloc[i].normalized_tweet)\n",
    "            print(\"Response: \", df.iloc[i]['test'+test_name])\n",
    "        print()\n",
    "    y_true = mlb.fit_transform(df['annotations'])\n",
    "    y_pred = mlb.transform(df['response'])\n",
    "    report = classification_report(y_true, y_pred, output_dict=True, target_names=classes)\n",
    "    return pd.DataFrame(report).transpose()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------\n",
      "Multilabel v01 NO FINETUNING\n",
      "----------------\n",
      "                                    precision    recall  f1-score  support\n",
      "War/Terror                           0.332863  0.925490  0.489627    255.0\n",
      "Conspiracy Theory                    0.050467  0.600000  0.093103     45.0\n",
      "Education                            0.018717  0.538462  0.036176     13.0\n",
      "Election Campaign                    0.069136  0.848485  0.127854     33.0\n",
      "Environment                          0.023936  0.642857  0.046154     14.0\n",
      "Government/Public                    0.310850  0.728522  0.435766    291.0\n",
      "Health                               0.073350  0.652174  0.131868     46.0\n",
      "Immigration/Integration              0.050119  0.583333  0.092308     36.0\n",
      "Justice/Crime                        0.189807  0.788321  0.305949    137.0\n",
      "Labor/Employment                     0.043796  0.642857  0.082005     28.0\n",
      "Macroeconomics/Economic Regulation   0.084833  0.532258  0.146341     62.0\n",
      "Media/Journalism                     0.054656  0.562500  0.099631     48.0\n",
      "Religion                             0.017199  0.636364  0.033493     11.0\n",
      "Science/Technology                   0.019900  0.727273  0.038741     11.0\n",
      "Others                               0.394737  0.225564  0.287081    266.0\n",
      "micro avg                            0.123422  0.641204  0.207000   1296.0\n",
      "macro avg                            0.115624  0.642297  0.163073   1296.0\n",
      "weighted avg                         0.251673  0.641204  0.313119   1296.0\n",
      "samples avg                          0.278594  0.617233  0.321400   1296.0\n",
      "----------------\n",
      "\n",
      "----------------\n",
      "Multilabel v01 2 NO FINETUNING\n",
      "----------------\n",
      "                                    precision    recall  f1-score  support\n",
      "War/Terror                           0.411150  0.462745  0.435424    255.0\n",
      "Conspiracy Theory                    0.042056  0.200000  0.069498     45.0\n",
      "Education                            0.043103  0.384615  0.077519     13.0\n",
      "Election Campaign                    0.077519  0.303030  0.123457     33.0\n",
      "Environment                          0.027523  0.214286  0.048780     14.0\n",
      "Government/Public                    0.340000  0.350515  0.345178    291.0\n",
      "Health                               0.129771  0.369565  0.192090     46.0\n",
      "Immigration/Integration              0.072993  0.277778  0.115607     36.0\n",
      "Justice/Crime                        0.151020  0.270073  0.193717    137.0\n",
      "Labor/Employment                     0.095890  0.500000  0.160920     28.0\n",
      "Macroeconomics/Economic Regulation   0.136364  0.290323  0.185567     62.0\n",
      "Media/Journalism                     0.052632  0.250000  0.086957     48.0\n",
      "Religion                             0.035211  0.454545  0.065359     11.0\n",
      "Science/Technology                   0.025000  0.272727  0.045802     11.0\n",
      "Others                               0.291391  0.661654  0.404598    266.0\n",
      "micro avg                            0.177303  0.415895  0.248616   1296.0\n",
      "macro avg                            0.128775  0.350790  0.170032   1296.0\n",
      "weighted avg                         0.254865  0.415895  0.300108   1296.0\n",
      "samples avg                          0.279152  0.432333  0.301118   1296.0\n",
      "\n",
      "----------------\n",
      "Multilabel v01 NO FINETUNING Explanation First\n",
      "----------------\n",
      "                                    precision    recall  f1-score  support\n",
      "War/Terror                           0.653543  0.650980  0.652259    255.0\n",
      "Conspiracy Theory                    0.112903  0.311111  0.165680     45.0\n",
      "Education                            0.070175  0.615385  0.125984     13.0\n",
      "Election Campaign                    0.140000  0.424242  0.210526     33.0\n",
      "Environment                          0.068966  0.428571  0.118812     14.0\n",
      "Government/Public                    0.423529  0.494845  0.456418    291.0\n",
      "Health                               0.221239  0.543478  0.314465     46.0\n",
      "Immigration/Integration              0.168675  0.388889  0.235294     36.0\n",
      "Justice/Crime                        0.423529  0.525547  0.469055    137.0\n",
      "Labor/Employment                     0.153846  0.428571  0.226415     28.0\n",
      "Macroeconomics/Economic Regulation   0.228916  0.306452  0.262069     62.0\n",
      "Media/Journalism                     0.126050  0.312500  0.179641     48.0\n",
      "Religion                             0.080645  0.454545  0.136986     11.0\n",
      "Science/Technology                   0.081967  0.454545  0.138889     11.0\n",
      "Others                               0.468023  0.605263  0.527869    266.0\n",
      "micro avg                            0.318949  0.524691  0.396733   1296.0\n",
      "macro avg                            0.228134  0.462995  0.281358   1296.0\n",
      "weighted avg                         0.406316  0.524691  0.446530   1296.0\n",
      "samples avg                          0.500719  0.562033  0.498136   1296.0\n",
      "----------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'print(\"----------------\")\\nprint(\"Multilabel v01 NO FINETUNING Explanation First Only First Label Extracted\")\\nprint(\"----------------\")\\nprint(report_multilabel_no_fine_tune_explanation_first_v01_only_first_label_extracted)\\nprint(\"----------------\")'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path = \"../data/vicuna_4bit/\"\n",
    "\n",
    "report_multilabel_no_fine_tune_v01 = get_report(base_path, \"multilabel_no_fine_tune_v01\", llm_utils.extract_multilabel_list, mlb, classes)\n",
    "report_multilabel_no_fine_tune_v02 = get_report(base_path, \"multilabel_no_fine_tune_v02\", llm_utils.extract_multilabel_list, mlb, classes)\n",
    "#report_multilabel_no_fine_tune_v01_only_first_label_extracted = get_report(base_path, \"multi_label_no_fine_tune_v01\", llm_utils.extract_multilabel_list_only_first_class, mlb, classes)\n",
    "report_multilabel_no_fine_tune_explanation_first_v01 = get_report(base_path, \"multi_label_no_fine_tune_explanation_first_v01\", llm_utils.extract_multilabel_list_explanation_first, mlb, classes)\n",
    "report_multilabel_no_fine_tune_explanation_first_v01_only_first_label_extracted = get_report(base_path, \"multi_label_no_fine_tune_explanation_first_v01\", llm_utils.extract_multilabel_list_explanation_first_only_first_class, mlb, classes)\n",
    "\n",
    "print()\n",
    "print(\"----------------\")\n",
    "print(\"Multilabel v01 NO FINETUNING\")\n",
    "print(\"----------------\")\n",
    "print(report_multilabel_no_fine_tune_v01)\n",
    "print(\"----------------\")\n",
    "print()\n",
    "print(\"----------------\")\n",
    "print(\"Multilabel v02 2 NO FINETUNING\")\n",
    "print(\"----------------\")\n",
    "print(report_multilabel_no_fine_tune_v02)\n",
    "print()\n",
    "print(\"----------------\")\n",
    "print(\"Multilabel v01 NO FINETUNING Explanation First\")\n",
    "print(\"----------------\")\n",
    "print(report_multilabel_no_fine_tune_explanation_first_v01)\n",
    "print(\"----------------\")\n",
    "print()\n",
    "\"\"\"print(\"----------------\")\n",
    "print(\"Multilabel v01 NO FINETUNING Explanation First Only First Label Extracted\")\n",
    "print(\"----------------\")\n",
    "print(report_multilabel_no_fine_tune_explanation_first_v01_only_first_label_extracted)\n",
    "print(\"----------------\")\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilabel Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>War/Terror</th>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.760784</td>\n",
       "      <td>0.852747</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Conspiracy Theory</th>\n",
       "      <td>0.451613</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.523364</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Education</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Election Campaign</th>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Environment</th>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Government/Public</th>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.721649</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>291.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Health</th>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Immigration/Integration</th>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Justice/Crime</th>\n",
       "      <td>0.886792</td>\n",
       "      <td>0.686131</td>\n",
       "      <td>0.773663</td>\n",
       "      <td>137.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Labor/Employment</th>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Macroeconomics/Economic Regulation</th>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Media/Journalism</th>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.229167</td>\n",
       "      <td>0.338462</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Religion</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Science/Technology</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Others</th>\n",
       "      <td>0.830040</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.809249</td>\n",
       "      <td>266.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro avg</th>\n",
       "      <td>0.799631</td>\n",
       "      <td>0.668210</td>\n",
       "      <td>0.728037</td>\n",
       "      <td>1296.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.732766</td>\n",
       "      <td>0.545050</td>\n",
       "      <td>0.602023</td>\n",
       "      <td>1296.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.810913</td>\n",
       "      <td>0.668210</td>\n",
       "      <td>0.722263</td>\n",
       "      <td>1296.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>samples avg</th>\n",
       "      <td>0.807000</td>\n",
       "      <td>0.726733</td>\n",
       "      <td>0.749805</td>\n",
       "      <td>1296.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    precision    recall  f1-score  support\n",
       "War/Terror                           0.970000  0.760784  0.852747    255.0\n",
       "Conspiracy Theory                    0.451613  0.622222  0.523364     45.0\n",
       "Education                            0.428571  0.692308  0.529412     13.0\n",
       "Election Campaign                    0.851852  0.696970  0.766667     33.0\n",
       "Environment                          0.615385  0.571429  0.592593     14.0\n",
       "Government/Public                    0.760870  0.721649  0.740741    291.0\n",
       "Health                               0.785714  0.478261  0.594595     46.0\n",
       "Immigration/Integration              0.789474  0.416667  0.545455     36.0\n",
       "Justice/Crime                        0.886792  0.686131  0.773663    137.0\n",
       "Labor/Employment                     0.521739  0.428571  0.470588     28.0\n",
       "Macroeconomics/Economic Regulation   0.785714  0.354839  0.488889     62.0\n",
       "Media/Journalism                     0.647059  0.229167  0.338462     48.0\n",
       "Religion                             0.666667  0.363636  0.470588     11.0\n",
       "Science/Technology                   1.000000  0.363636  0.533333     11.0\n",
       "Others                               0.830040  0.789474  0.809249    266.0\n",
       "micro avg                            0.799631  0.668210  0.728037   1296.0\n",
       "macro avg                            0.732766  0.545050  0.602023   1296.0\n",
       "weighted avg                         0.810913  0.668210  0.722263   1296.0\n",
       "samples avg                          0.807000  0.726733  0.749805   1296.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vicuna_lora_multilabel_with_rules_v02_df = pd.read_csv(\"../data/vicuna_4bit/lora/multilabel_with_rules_v02/test_generic_test_0.csv\")\n",
    "vicuna_lora_multilabel_with_rules_v02_predictions_per_class, confusion_matrices, binary_classification_reports, multilabel_classification_reports = llm_utils.calculate_metrics_from_multilabel_list(vicuna_lora_multilabel_with_rules_v02_df, classes, llm_utils.extract_multilabel_list)\n",
    "vicuna_lora_multilabel_with_rules_v02 = {\"confusion_matrices\": confusion_matrices, \"binary_classification_reports\": binary_classification_reports, \"multilabel_classification_reports\": multilabel_classification_reports}\n",
    "vicuna_lora_multilabel_with_rules_v02[\"multilabel_classification_reports\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>support_macro</th>\n",
       "      <th>f1_score_class_0</th>\n",
       "      <th>support_class_0</th>\n",
       "      <th>f1_score_class_1</th>\n",
       "      <th>support_class_1</th>\n",
       "      <th>precision_class_0</th>\n",
       "      <th>recall_class_0</th>\n",
       "      <th>precision_class_1</th>\n",
       "      <th>recall_class_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>War/Terror</td>\n",
       "      <td>0.904691</td>\n",
       "      <td>0.946875</td>\n",
       "      <td>0.876365</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.956634</td>\n",
       "      <td>745</td>\n",
       "      <td>0.852747</td>\n",
       "      <td>255</td>\n",
       "      <td>0.923750</td>\n",
       "      <td>0.991946</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.760784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Conspiracy Theory</td>\n",
       "      <td>0.748212</td>\n",
       "      <td>0.716745</td>\n",
       "      <td>0.793310</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.973059</td>\n",
       "      <td>955</td>\n",
       "      <td>0.523364</td>\n",
       "      <td>45</td>\n",
       "      <td>0.981876</td>\n",
       "      <td>0.964398</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>0.622222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Education</td>\n",
       "      <td>0.760637</td>\n",
       "      <td>0.712243</td>\n",
       "      <td>0.840075</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.991862</td>\n",
       "      <td>987</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>13</td>\n",
       "      <td>0.995914</td>\n",
       "      <td>0.987842</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Election Campaign</td>\n",
       "      <td>0.879725</td>\n",
       "      <td>0.920787</td>\n",
       "      <td>0.846417</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.992784</td>\n",
       "      <td>967</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>33</td>\n",
       "      <td>0.989723</td>\n",
       "      <td>0.995863</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.696970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Environment</td>\n",
       "      <td>0.793509</td>\n",
       "      <td>0.804653</td>\n",
       "      <td>0.783179</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.994425</td>\n",
       "      <td>986</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>14</td>\n",
       "      <td>0.993921</td>\n",
       "      <td>0.994929</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Government/Public</td>\n",
       "      <td>0.819079</td>\n",
       "      <td>0.824496</td>\n",
       "      <td>0.814280</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.897418</td>\n",
       "      <td>709</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>291</td>\n",
       "      <td>0.888122</td>\n",
       "      <td>0.906911</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.721649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Health</td>\n",
       "      <td>0.789509</td>\n",
       "      <td>0.880511</td>\n",
       "      <td>0.735986</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.984424</td>\n",
       "      <td>954</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>46</td>\n",
       "      <td>0.975309</td>\n",
       "      <td>0.993711</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.478261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Immigration/Integration</td>\n",
       "      <td>0.766301</td>\n",
       "      <td>0.884033</td>\n",
       "      <td>0.706259</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.987147</td>\n",
       "      <td>964</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>36</td>\n",
       "      <td>0.978593</td>\n",
       "      <td>0.995851</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Justice/Crime</td>\n",
       "      <td>0.871180</td>\n",
       "      <td>0.919347</td>\n",
       "      <td>0.836113</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.968697</td>\n",
       "      <td>863</td>\n",
       "      <td>0.773663</td>\n",
       "      <td>137</td>\n",
       "      <td>0.951902</td>\n",
       "      <td>0.986095</td>\n",
       "      <td>0.886792</td>\n",
       "      <td>0.686131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Labor/Employment</td>\n",
       "      <td>0.728367</td>\n",
       "      <td>0.752681</td>\n",
       "      <td>0.708627</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.986147</td>\n",
       "      <td>972</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>28</td>\n",
       "      <td>0.983623</td>\n",
       "      <td>0.988683</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Macroeconomics/Economic Regulation</td>\n",
       "      <td>0.732403</td>\n",
       "      <td>0.872281</td>\n",
       "      <td>0.674221</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.975916</td>\n",
       "      <td>938</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>62</td>\n",
       "      <td>0.958848</td>\n",
       "      <td>0.993603</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.354839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Media/Journalism</td>\n",
       "      <td>0.658120</td>\n",
       "      <td>0.804709</td>\n",
       "      <td>0.611432</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>952</td>\n",
       "      <td>0.338462</td>\n",
       "      <td>48</td>\n",
       "      <td>0.962360</td>\n",
       "      <td>0.993697</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.229167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Religion</td>\n",
       "      <td>0.733025</td>\n",
       "      <td>0.829812</td>\n",
       "      <td>0.680807</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.995461</td>\n",
       "      <td>989</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>11</td>\n",
       "      <td>0.992958</td>\n",
       "      <td>0.997978</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Science/Technology</td>\n",
       "      <td>0.764903</td>\n",
       "      <td>0.996486</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.996474</td>\n",
       "      <td>989</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>11</td>\n",
       "      <td>0.992972</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Others</td>\n",
       "      <td>0.871201</td>\n",
       "      <td>0.877536</td>\n",
       "      <td>0.865445</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.933153</td>\n",
       "      <td>734</td>\n",
       "      <td>0.809249</td>\n",
       "      <td>266</td>\n",
       "      <td>0.925033</td>\n",
       "      <td>0.941417</td>\n",
       "      <td>0.830040</td>\n",
       "      <td>0.789474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 label  f1_score_macro  precision_macro  recall_macro support_macro  f1_score_class_0 support_class_0  f1_score_class_1 support_class_1  precision_class_0  recall_class_0  precision_class_1  recall_class_1\n",
       "0                           War/Terror        0.904691         0.946875      0.876365          1000          0.956634             745          0.852747             255           0.923750        0.991946           0.970000        0.760784\n",
       "1                    Conspiracy Theory        0.748212         0.716745      0.793310          1000          0.973059             955          0.523364              45           0.981876        0.964398           0.451613        0.622222\n",
       "2                            Education        0.760637         0.712243      0.840075          1000          0.991862             987          0.529412              13           0.995914        0.987842           0.428571        0.692308\n",
       "3                    Election Campaign        0.879725         0.920787      0.846417          1000          0.992784             967          0.766667              33           0.989723        0.995863           0.851852        0.696970\n",
       "4                          Environment        0.793509         0.804653      0.783179          1000          0.994425             986          0.592593              14           0.993921        0.994929           0.615385        0.571429\n",
       "5                    Government/Public        0.819079         0.824496      0.814280          1000          0.897418             709          0.740741             291           0.888122        0.906911           0.760870        0.721649\n",
       "6                               Health        0.789509         0.880511      0.735986          1000          0.984424             954          0.594595              46           0.975309        0.993711           0.785714        0.478261\n",
       "7              Immigration/Integration        0.766301         0.884033      0.706259          1000          0.987147             964          0.545455              36           0.978593        0.995851           0.789474        0.416667\n",
       "8                        Justice/Crime        0.871180         0.919347      0.836113          1000          0.968697             863          0.773663             137           0.951902        0.986095           0.886792        0.686131\n",
       "9                     Labor/Employment        0.728367         0.752681      0.708627          1000          0.986147             972          0.470588              28           0.983623        0.988683           0.521739        0.428571\n",
       "10  Macroeconomics/Economic Regulation        0.732403         0.872281      0.674221          1000          0.975916             938          0.488889              62           0.958848        0.993603           0.785714        0.354839\n",
       "11                    Media/Journalism        0.658120         0.804709      0.611432          1000          0.977778             952          0.338462              48           0.962360        0.993697           0.647059        0.229167\n",
       "12                            Religion        0.733025         0.829812      0.680807          1000          0.995461             989          0.470588              11           0.992958        0.997978           0.666667        0.363636\n",
       "13                  Science/Technology        0.764903         0.996486      0.681818          1000          0.996474             989          0.533333              11           0.992972        1.000000           1.000000        0.363636\n",
       "14                              Others        0.871201         0.877536      0.865445          1000          0.933153             734          0.809249             266           0.925033        0.941417           0.830040        0.789474"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vicuna_lora_multilabel_with_rules_v02[\"binary_classification_reports\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"../data/vicuna_4bit/lora/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All labels extracted from LLM response\n",
      "\n",
      "----------------\n",
      "Multilabel v01 128 LoRA rank retest\n",
      "----------------\n",
      "                                    precision    recall  f1-score  support\n",
      "War/Terror                           0.936893  0.756863  0.837310    255.0\n",
      "Conspiracy Theory                    0.508772  0.644444  0.568627     45.0\n",
      "Education                            0.444444  0.307692  0.363636     13.0\n",
      "Election Campaign                    0.933333  0.424242  0.583333     33.0\n",
      "Environment                          0.833333  0.357143  0.500000     14.0\n",
      "Government/Public                    0.861789  0.364261  0.512077    291.0\n",
      "Health                               0.769231  0.434783  0.555556     46.0\n",
      "Immigration/Integration              0.916667  0.611111  0.733333     36.0\n",
      "Justice/Crime                        0.825581  0.518248  0.636771    137.0\n",
      "Labor/Employment                     0.625000  0.357143  0.454545     28.0\n",
      "Macroeconomics/Economic Regulation   0.727273  0.129032  0.219178     62.0\n",
      "Media/Journalism                     0.583333  0.145833  0.233333     48.0\n",
      "Religion                             1.000000  0.090909  0.166667     11.0\n",
      "Science/Technology                   0.500000  0.090909  0.153846     11.0\n",
      "Others                               0.431703  0.962406  0.596042    266.0\n",
      "micro avg                            0.629318  0.576389  0.601692   1296.0\n",
      "macro avg                            0.726490  0.413001  0.474284   1296.0\n",
      "weighted avg                         0.744013  0.576389  0.584781   1296.0\n",
      "samples avg                          0.613033  0.612267  0.604907   1296.0\n",
      "----------------\n",
      "\n",
      "----------------\n",
      "Multilabel v01\n",
      "----------------\n",
      "                                    precision    recall  f1-score  support\n",
      "War/Terror                           0.888476  0.937255  0.912214    255.0\n",
      "Conspiracy Theory                    0.417722  0.733333  0.532258     45.0\n",
      "Education                            0.473684  0.692308  0.562500     13.0\n",
      "Election Campaign                    0.814815  0.666667  0.733333     33.0\n",
      "Environment                          0.615385  0.571429  0.592593     14.0\n",
      "Government/Public                    0.754325  0.749141  0.751724    291.0\n",
      "Health                               0.761905  0.695652  0.727273     46.0\n",
      "Immigration/Integration              0.806452  0.694444  0.746269     36.0\n",
      "Justice/Crime                        0.819549  0.795620  0.807407    137.0\n",
      "Labor/Employment                     0.727273  0.571429  0.640000     28.0\n",
      "Macroeconomics/Economic Regulation   0.756098  0.500000  0.601942     62.0\n",
      "Media/Journalism                     0.514286  0.375000  0.433735     48.0\n",
      "Religion                             0.600000  0.545455  0.571429     11.0\n",
      "Science/Technology                   0.500000  0.363636  0.421053     11.0\n",
      "Others                               0.840637  0.793233  0.816248    266.0\n",
      "micro avg                            0.773050  0.756944  0.764912   1296.0\n",
      "macro avg                            0.686040  0.645640  0.656665   1296.0\n",
      "weighted avg                         0.779726  0.756944  0.764013   1296.0\n",
      "samples avg                          0.805833  0.793717  0.783922   1296.0\n",
      "----------------\n",
      "\n",
      "----------------\n",
      "Multilabel v01 256 LoRA rank\n",
      "----------------\n",
      "                                    precision    recall  f1-score  support\n",
      "War/Terror                           0.874539  0.929412  0.901141    255.0\n",
      "Conspiracy Theory                    0.435897  0.755556  0.552846     45.0\n",
      "Education                            0.421053  0.615385  0.500000     13.0\n",
      "Election Campaign                    0.821429  0.696970  0.754098     33.0\n",
      "Environment                          0.583333  0.500000  0.538462     14.0\n",
      "Government/Public                    0.751656  0.780069  0.765599    291.0\n",
      "Health                               0.695652  0.695652  0.695652     46.0\n",
      "Immigration/Integration              0.833333  0.694444  0.757576     36.0\n",
      "Justice/Crime                        0.795775  0.824818  0.810036    137.0\n",
      "Labor/Employment                     0.666667  0.571429  0.615385     28.0\n",
      "Macroeconomics/Economic Regulation   0.813953  0.564516  0.666667     62.0\n",
      "Media/Journalism                     0.575758  0.395833  0.469136     48.0\n",
      "Religion                             0.636364  0.636364  0.636364     11.0\n",
      "Science/Technology                   0.545455  0.545455  0.545455     11.0\n",
      "Others                               0.854839  0.796992  0.824903    266.0\n",
      "micro avg                            0.771186  0.772377  0.771781   1296.0\n",
      "macro avg                            0.687047  0.666860  0.668888   1296.0\n",
      "weighted avg                         0.779536  0.772377  0.771710   1296.0\n",
      "samples avg                          0.808750  0.803883  0.791446   1296.0\n",
      "----------------\n",
      "\n",
      "----------------\n",
      "Multilabel v02\n",
      "----------------\n",
      "                                    precision    recall  f1-score  support\n",
      "War/Terror                           0.970000  0.760784  0.852747    255.0\n",
      "Conspiracy Theory                    0.451613  0.622222  0.523364     45.0\n",
      "Education                            0.428571  0.692308  0.529412     13.0\n",
      "Election Campaign                    0.851852  0.696970  0.766667     33.0\n",
      "Environment                          0.615385  0.571429  0.592593     14.0\n",
      "Government/Public                    0.760870  0.721649  0.740741    291.0\n",
      "Health                               0.785714  0.478261  0.594595     46.0\n",
      "Immigration/Integration              0.789474  0.416667  0.545455     36.0\n",
      "Justice/Crime                        0.886792  0.686131  0.773663    137.0\n",
      "Labor/Employment                     0.521739  0.428571  0.470588     28.0\n",
      "Macroeconomics/Economic Regulation   0.785714  0.354839  0.488889     62.0\n",
      "Media/Journalism                     0.647059  0.229167  0.338462     48.0\n",
      "Religion                             0.666667  0.363636  0.470588     11.0\n",
      "Science/Technology                   1.000000  0.363636  0.533333     11.0\n",
      "Others                               0.830040  0.789474  0.809249    266.0\n",
      "micro avg                            0.799631  0.668210  0.728037   1296.0\n",
      "macro avg                            0.732766  0.545050  0.602023   1296.0\n",
      "weighted avg                         0.810913  0.668210  0.722263   1296.0\n",
      "samples avg                          0.807000  0.726733  0.749805   1296.0\n",
      "----------------\n",
      "\n",
      "----------------\n",
      "Multilabel v02 256 LoRA rank\n",
      "----------------\n",
      "                                    precision    recall  f1-score  support\n",
      "War/Terror                           0.418033  1.000000  0.589595    255.0\n",
      "Conspiracy Theory                    0.343750  0.733333  0.468085     45.0\n",
      "Education                            0.300000  0.923077  0.452830     13.0\n",
      "Election Campaign                    0.609756  0.757576  0.675676     33.0\n",
      "Environment                          0.322581  0.714286  0.444444     14.0\n",
      "Government/Public                    0.594470  0.886598  0.711724    291.0\n",
      "Health                               0.285714  0.913043  0.435233     46.0\n",
      "Immigration/Integration              0.293578  0.888889  0.441379     36.0\n",
      "Justice/Crime                        0.354223  0.948905  0.515873    137.0\n",
      "Labor/Employment                     0.216981  0.821429  0.343284     28.0\n",
      "Macroeconomics/Economic Regulation   0.193443  0.951613  0.321526     62.0\n",
      "Media/Journalism                     0.099366  0.979167  0.180422     48.0\n",
      "Religion                             0.034700  1.000000  0.067073     11.0\n",
      "Science/Technology                   0.023529  0.727273  0.045584     11.0\n",
      "Others                               0.980952  0.387218  0.555256    266.0\n",
      "micro avg                            0.297643  0.808642  0.435126   1296.0\n",
      "macro avg                            0.338072  0.842160  0.416532   1296.0\n",
      "weighted avg                         0.524883  0.808642  0.545261   1296.0\n",
      "samples avg                          0.426626  0.789167  0.508291   1296.0\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "report_multilabel_v01_128_rank_retest = get_report(base_path, \"multilabel_without_context_v01_retest\", llm_utils.extract_multilabel_list, mlb, classes)\n",
    "report_multilabel_v01 = get_report(base_path, \"multilabel_without_context_v01\", llm_utils.extract_multilabel_list, mlb, classes)\n",
    "report_multilabel_v01_256_rank = get_report(base_path, \"multilabel_without_context_v01_256_rank\", llm_utils.extract_multilabel_list, mlb, classes)\n",
    "report_multilabel_v02 = get_report(base_path, \"multilabel_with_rules_v02\", llm_utils.extract_multilabel_list, mlb, classes)\n",
    "report_multilabel_v02_256_rank = get_report(base_path, \"multilabel_with_rules_v02_256_rank\", llm_utils.extract_multilabel_list, mlb, classes)\n",
    "\n",
    "print(\"All labels extracted from LLM response\")\n",
    "print()\n",
    "print(\"----------------\")\n",
    "print(\"Multilabel v01 128 LoRA rank retest\")\n",
    "print(\"----------------\")\n",
    "print(report_multilabel_v01_128_rank_retest)\n",
    "print(\"----------------\")\n",
    "print()\n",
    "print(\"----------------\")\n",
    "print(\"Multilabel v01\")\n",
    "print(\"----------------\")\n",
    "print(report_multilabel_v01)\n",
    "print(\"----------------\")\n",
    "print()\n",
    "print(\"----------------\")\n",
    "print(\"Multilabel v01 256 LoRA rank\")\n",
    "print(\"----------------\")\n",
    "print(report_multilabel_v01_256_rank)\n",
    "print(\"----------------\")\n",
    "print()\n",
    "print(\"----------------\")\n",
    "print(\"Multilabel v02\")\n",
    "print(\"----------------\")\n",
    "print(report_multilabel_v02)\n",
    "print(\"----------------\")\n",
    "print()\n",
    "print(\"----------------\")\n",
    "print(\"Multilabel v02 256 LoRA rank\")\n",
    "print(\"----------------\")\n",
    "print(report_multilabel_v02_256_rank)\n",
    "print(\"----------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics for multilabel but only the first class predicted is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only first label is extracted from LLM response\n",
      "\n",
      "----------------\n",
      "Multilabel v01 retest\n",
      "----------------\n",
      "                                    precision    recall  f1-score  support\n",
      "War/Terror                           0.888476  0.937255  0.912214    255.0\n",
      "Conspiracy Theory                    0.428571  0.631579  0.510638     19.0\n",
      "Education                            0.444444  0.727273  0.551724     11.0\n",
      "Election Campaign                    0.782609  0.620690  0.692308     29.0\n",
      "Environment                          0.666667  0.571429  0.615385     14.0\n",
      "Government/Public                    0.726027  0.716216  0.721088    222.0\n",
      "Health                               0.600000  0.600000  0.600000     20.0\n",
      "Immigration/Integration              0.500000  0.166667  0.250000      6.0\n",
      "Justice/Crime                        0.780000  0.821053  0.800000     95.0\n",
      "Labor/Employment                     0.625000  0.714286  0.666667     14.0\n",
      "Macroeconomics/Economic Regulation   0.619048  0.481481  0.541667     27.0\n",
      "Media/Journalism                     0.416667  0.384615  0.400000     13.0\n",
      "Religion                             0.750000  0.600000  0.666667      5.0\n",
      "Science/Technology                   0.600000  0.750000  0.666667      4.0\n",
      "Others                               0.840637  0.793233  0.816248    266.0\n",
      "micro avg                            0.780000  0.780000  0.780000   1000.0\n",
      "macro avg                            0.644543  0.634385  0.627418   1000.0\n",
      "weighted avg                         0.782541  0.780000  0.778940   1000.0\n",
      "samples avg                          0.780000  0.780000  0.780000   1000.0\n",
      "----------------\n",
      "\n",
      "----------------\n",
      "Multilabel v01\n",
      "----------------\n",
      "                                    precision    recall  f1-score  support\n",
      "War/Terror                           0.888476  0.937255  0.912214    255.0\n",
      "Conspiracy Theory                    0.428571  0.631579  0.510638     19.0\n",
      "Education                            0.444444  0.727273  0.551724     11.0\n",
      "Election Campaign                    0.782609  0.620690  0.692308     29.0\n",
      "Environment                          0.666667  0.571429  0.615385     14.0\n",
      "Government/Public                    0.726027  0.716216  0.721088    222.0\n",
      "Health                               0.600000  0.600000  0.600000     20.0\n",
      "Immigration/Integration              0.500000  0.166667  0.250000      6.0\n",
      "Justice/Crime                        0.780000  0.821053  0.800000     95.0\n",
      "Labor/Employment                     0.625000  0.714286  0.666667     14.0\n",
      "Macroeconomics/Economic Regulation   0.619048  0.481481  0.541667     27.0\n",
      "Media/Journalism                     0.416667  0.384615  0.400000     13.0\n",
      "Religion                             0.750000  0.600000  0.666667      5.0\n",
      "Science/Technology                   0.600000  0.750000  0.666667      4.0\n",
      "Others                               0.840637  0.793233  0.816248    266.0\n",
      "micro avg                            0.780000  0.780000  0.780000   1000.0\n",
      "macro avg                            0.644543  0.634385  0.627418   1000.0\n",
      "weighted avg                         0.782541  0.780000  0.778940   1000.0\n",
      "samples avg                          0.780000  0.780000  0.780000   1000.0\n",
      "----------------\n",
      "\n",
      "----------------\n",
      "Multilabel v01 256 LoRA rank\n",
      "----------------\n",
      "                                    precision    recall  f1-score  support\n",
      "War/Terror                           0.874539  0.929412  0.901141    255.0\n",
      "Conspiracy Theory                    0.440000  0.578947  0.500000     19.0\n",
      "Education                            0.411765  0.636364  0.500000     11.0\n",
      "Election Campaign                    0.791667  0.655172  0.716981     29.0\n",
      "Environment                          0.700000  0.500000  0.583333     14.0\n",
      "Government/Public                    0.725664  0.738739  0.732143    222.0\n",
      "Health                               0.650000  0.650000  0.650000     20.0\n",
      "Immigration/Integration              1.000000  0.166667  0.285714      6.0\n",
      "Justice/Crime                        0.802083  0.810526  0.806283     95.0\n",
      "Labor/Employment                     0.600000  0.642857  0.620690     14.0\n",
      "Macroeconomics/Economic Regulation   0.722222  0.481481  0.577778     27.0\n",
      "Media/Journalism                     0.428571  0.461538  0.444444     13.0\n",
      "Religion                             0.625000  1.000000  0.769231      5.0\n",
      "Science/Technology                   0.428571  0.750000  0.545455      4.0\n",
      "Others                               0.854839  0.796992  0.824903    266.0\n",
      "micro avg                            0.784000  0.784000  0.784000   1000.0\n",
      "macro avg                            0.670328  0.653246  0.630540   1000.0\n",
      "weighted avg                         0.790648  0.784000  0.783116   1000.0\n",
      "samples avg                          0.784000  0.784000  0.784000   1000.0\n",
      "----------------\n",
      "\n",
      "----------------\n",
      "Multilabel v02\n",
      "----------------\n",
      "                                    precision    recall  f1-score  support\n",
      "War/Terror                           0.970000  0.760784  0.852747    255.0\n",
      "Conspiracy Theory                    0.294118  0.526316  0.377358     19.0\n",
      "Education                            0.400000  0.727273  0.516129     11.0\n",
      "Election Campaign                    0.791667  0.655172  0.716981     29.0\n",
      "Environment                          0.666667  0.571429  0.615385     14.0\n",
      "Government/Public                    0.664122  0.783784  0.719008    222.0\n",
      "Health                               0.600000  0.600000  0.600000     20.0\n",
      "Immigration/Integration              0.166667  0.166667  0.166667      6.0\n",
      "Justice/Crime                        0.750000  0.821053  0.783920     95.0\n",
      "Labor/Employment                     0.409091  0.642857  0.500000     14.0\n",
      "Macroeconomics/Economic Regulation   0.625000  0.555556  0.588235     27.0\n",
      "Media/Journalism                     0.222222  0.153846  0.181818     13.0\n",
      "Religion                             0.666667  0.800000  0.727273      5.0\n",
      "Science/Technology                   0.750000  0.750000  0.750000      4.0\n",
      "Others                               0.830040  0.789474  0.809249    266.0\n",
      "micro avg                            0.747000  0.747000  0.747000   1000.0\n",
      "macro avg                            0.587084  0.620281  0.593651   1000.0\n",
      "weighted avg                         0.773930  0.747000  0.753940   1000.0\n",
      "samples avg                          0.747000  0.747000  0.747000   1000.0\n",
      "----------------\n",
      "\n",
      "----------------\n",
      "Multilabel v02 256 LoRA rank\n",
      "----------------\n",
      "                                    precision    recall  f1-score  support\n",
      "War/Terror                           0.418033  1.000000  0.589595    255.0\n",
      "Conspiracy Theory                    0.200000  0.052632  0.083333     19.0\n",
      "Education                            0.428571  0.818182  0.562500     11.0\n",
      "Election Campaign                    0.523810  0.379310  0.440000     29.0\n",
      "Environment                          0.500000  0.285714  0.363636     14.0\n",
      "Government/Public                    0.575758  0.256757  0.355140    222.0\n",
      "Health                               0.500000  0.600000  0.545455     20.0\n",
      "Immigration/Integration              1.000000  0.333333  0.500000      6.0\n",
      "Justice/Crime                        0.666667  0.315789  0.428571     95.0\n",
      "Labor/Employment                     0.428571  0.214286  0.285714     14.0\n",
      "Macroeconomics/Economic Regulation   0.636364  0.259259  0.368421     27.0\n",
      "Media/Journalism                     0.071429  0.153846  0.097561     13.0\n",
      "Religion                             0.285714  0.400000  0.333333      5.0\n",
      "Science/Technology                   0.571429  1.000000  0.727273      4.0\n",
      "Others                               0.980952  0.387218  0.555256    266.0\n",
      "micro avg                            0.502000  0.502000  0.502000   1000.0\n",
      "macro avg                            0.519153  0.430422  0.415719   1000.0\n",
      "weighted avg                         0.633213  0.502000  0.476923   1000.0\n",
      "samples avg                          0.502000  0.502000  0.502000   1000.0\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/bruno/0d2f61d2-2b9c-4043-9a46-8e4dfe74fc95/bruno/anaconda3/envs/my_env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "report_multilabel_v01_128_rank_retest = get_report(base_path, \"multilabel_without_context_v01_retest\", llm_utils.extract_multilabel_list_only_first_class, \n",
    "                                                   mlb, classes)\n",
    "\n",
    "report_multilabel_v01 = get_report(base_path, \"multilabel_without_context_v01\", llm_utils.extract_multilabel_list_only_first_class,\n",
    "                                   mlb, classes)\n",
    "\n",
    "report_multilabel_v01_256_rank = get_report(base_path, \"multilabel_without_context_v01_256_rank\", llm_utils.extract_multilabel_list_only_first_class,\n",
    "                                   mlb, classes)\n",
    "\n",
    "report_multilabel_v02 = get_report(base_path, \"multilabel_with_rules_v02\", llm_utils.extract_multilabel_list_only_first_class,\n",
    "                                   mlb, classes)\n",
    "\n",
    "report_multilabel_v02_256_rank = get_report(base_path, \"multilabel_with_rules_v02_256_rank\", llm_utils.extract_multilabel_list_only_first_class,\n",
    "                                   mlb, classes)\n",
    "\n",
    "print(\"Only first label is extracted from LLM response\")\n",
    "print()\n",
    "print(\"----------------\")\n",
    "print(\"Multilabel v01 retest\")\n",
    "print(\"----------------\")\n",
    "print(report_multilabel_v01)\n",
    "print(\"----------------\")\n",
    "print()\n",
    "print(\"----------------\")\n",
    "print(\"Multilabel v01\")\n",
    "print(\"----------------\")\n",
    "print(report_multilabel_v01)\n",
    "print(\"----------------\")\n",
    "print()\n",
    "print(\"----------------\")\n",
    "print(\"Multilabel v01 256 LoRA rank\")\n",
    "print(\"----------------\")\n",
    "print(report_multilabel_v01_256_rank)\n",
    "print(\"----------------\")\n",
    "print()\n",
    "print(\"----------------\")\n",
    "print(\"Multilabel v02\")\n",
    "print(\"----------------\")\n",
    "print(report_multilabel_v02)\n",
    "print(\"----------------\")\n",
    "print()\n",
    "print(\"----------------\")\n",
    "print(\"Multilabel v02 256 LoRA rank\")\n",
    "print(\"----------------\")\n",
    "print(report_multilabel_v02_256_rank)\n",
    "print(\"----------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
